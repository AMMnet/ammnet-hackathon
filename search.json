[
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html",
    "href": "posts/mapping-r-part2/mapping_rasters.html",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "",
    "text": "Take the survey!\n\n\n\nIf you are present for the live session on Wednesday May 7th 2025, please click here to take the survey."
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#overview",
    "href": "posts/mapping-r-part2/mapping_rasters.html#overview",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Overview",
    "text": "Overview\nWelcome back! We hope you enjoyed the first part of our Mapping in R series, which focused on vector data. In this follow-up session, we’ll dive into working with raster data in R. We’ll begin with a brief introduction to raster concepts and attributes, followed by a hands-on walkthrough using real-world examples.\nBy the end of the session, you’ll be able to confidently load and export raster data, crop and reproject rasters, classify and calculate with raster layers, extract summary statistics, and produce publication-ready visualizations in R. We’ll primarily use the terra, tidyterra and tidyverse packages to support our work."
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#learning-objectives",
    "href": "posts/mapping-r-part2/mapping_rasters.html#learning-objectives",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the structure and characteristics of raster data\nLoad, export and explore raster datasets in R\nReproject raster data to different coordinate systems\nPlot raster data using both base and ggplot2 approaches\nCrop and mask rasters based on vector boundaries\nAggregate or resample rasters for different resolutions\nPerform raster calculations and extract summary values\nConvert vector data to raster format (rasterizing)"
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#what-is-raster-data",
    "href": "posts/mapping-r-part2/mapping_rasters.html#what-is-raster-data",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "What is Raster data?",
    "text": "What is Raster data?\nRaster data is a type of spatial data represented as a grid of cells or pixels, where each cell has a specific value representing information about the area it covers. It’s commonly used in Geographic Information Systems (GIS) and remote sensing to represent continuous phenomena such as:\n\nElevation (Digital Elevation Models - DEMs)\nTemperature\nRainfall\nLand cover\nOther Satellite imagery\n\nCharacteristics of Raster Data: Raster data is made up of rows and columns forming a matrix or grid.\nCell or Pixel Value: Each cell in the grid has a value representing a certain attribute (e.g., temperature, vegetation index).\nSpatial Resolution: The size of each cell determines the resolution – smaller cells provide more detail (higher resolution), while larger cells provide less detail (lower resolution).\nCoordinate System: Raster data is often georeferenced, meaning it is tied to specific locations on the Earth’s surface using a coordinate system (e.g., latitude/longitude, UTM).\nFile Formats: Common formats include GeoTIFF, NetCDF, ASCII Grid, and IMG.\n\n\n\n\n\n\n\nAdvantages\nDiadvantages\n\n\n\n\n\nGood for representing continuous data (e.g., elevation, pollution levels).\n\n\nCan require large storage space, especially at high resolution.\n\n\n\n\nEasier to manipulate and analyze with mathematical functions (e.g., averaging, summing).\n\n\nMay lose detail when resampled or re-projected.\n\n\n\n\nCompatible with remote sensing data which is inherently raster-based.\n\n\nLess suitable for representing discrete objects (e.g., buildings, roads).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaster data in R using terra\nThere are two major packages available to handle rasters in R: terra and raster. The terra package is the newer, faster, and more efficient alternative to the older raster package in R for handling raster data. Both packages are commonly used for spatial data analysis, but there are some important differences. Namely, it is optimized for performance, particularly when working with large raster datasets. New features are regularly added, with better compatibility with the sf package and finally functions are cleaner, more consistent, and easier to remember.\nfor this tutorial we’ll be using the terra package to read, manipulate, and writing raster data.\n\n\nSpatRaster\nA SpatRaster represents multi-layer (multi-variable) raster data. A SpatRaster always stores a number of fundamental parameters decribing its geometry. These include the number of columns and rows, the spatial extent, and the Coordinate Reference System. In addition, a SpatRaster can store information about the file in which the raster cell values are stored. Or, if there is no such a file, a SpatRaster can hold the cell values in memory.\n\n\n\n\n\n\n\n\n\nwe’ll start off by loading in some key packages we’ll be using during this tutorial section\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(tidyverse)\nlibrary(malariaAtlas)\nlibrary(RColorBrewer)"
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#loading-raster-data",
    "href": "posts/mapping-r-part2/mapping_rasters.html#loading-raster-data",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Loading raster data",
    "text": "Loading raster data\nFor this tutorial we’re going to stick to using Tanzania as an example. In your drive you’ll find in the data/rasters/ folder where we have some pre-downloaded population rasters from WorldPop.\n\n#lets load a population raster in R\npopulation &lt;- rast(\"data/rasters/tza_pop_2022_constrained.tif\")\npopulation\n\nclass       : SpatRaster \ndimensions  : 12916, 13342, 1  (nrow, ncol, nlyr)\nresolution  : 0.0008333333, 0.0008333333  (x, y)\nextent      : 29.32708, 40.44542, -11.74542, -0.9820831  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : tza_pop_2022_constrained.tif \nname        : tza_ppp_2020_constrained \nmin value   :                    0.000 \nmax value   :                 2387.388 \n\n\nWhen you load the raster you’ll see some metadata that highlights the spatial extent (i.e. the bounding box around the raster); the coordinate reference system and the resolution (i.e. size of the pixel). This raster is what you call a single layer raster.\nNote that population is a SpatRaster of with a single band. The nice thing about terra package is you can also deal with multiple rasters, creating mutliple bands or already multi-band satellite imageries. layers (“bands”). It also work with other raster file formats, including GeoTiff, NetCDF, Imagine, and ESRI Grid formats.\ntypically at this stage you can use the terra::plot() function to simply plot the population raster you have loaded\n\nplot(population)\n\n\n\n\n\n\n\n\nThis is a nice quick way to look at the image you’ve brought in. Besides loading rasters into R from a download. There are some rasters that are available in packages that could be of interest to malaria modelers. We’ll download a Plasmodium falciparum parasite rate surface from the malariaAtlas package as an example:\n\n#first we'll load the tanzania shapefile from the package\n\nStart tag expected, '&lt;' not found\nStart tag expected, '&lt;' not found\n\n#next we'll load the dataset of PfPR for the year 2022\npfpr_2022 &lt;- getRaster(dataset_id = \"Malaria__202406_Global_Pf_Parasite_Rate\", year = 2022, shp = tz_districts)\n\n&lt;GMLEnvelope&gt;\n....|-- lowerCorner: -11.7612 29.3414 \"2000-01-01T00:00:00\"\n....|-- upperCorner: -0.9844 40.4432 \"2022-01-01T00:00:00\"Start tag expected, '&lt;' not found\n\n\nHere is another way to download rasters. We can also quickly plot this data using the malariaAtlas package or using terra\n\nautoplot(pfpr_2022)\n\n\n\n\n\n\n\nplot(pfpr_2022, main = \"PfPR 2-10 in 2022\")\n\n\n\n\n\n\n\n\ncoming back to the actual raster you’ll notice that the there are two layers in the data. This is known as a multiband rasters. this second layer in the prevalence surface is a population mask which seems to be empty so we can also just drop this band and treat it like a single band raster.\n\npfpr_2022\n\nclass       : SpatRaster \ndimensions  : 258, 266, 2  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : 29.33333, 40.41667, -11.75, -0.9999987  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nvarname     : Malaria__202406_Global_Pf_Parasite_Rate_2022-01-01T00_00_00_-11.7612,29.3414,-0.9844,40.4432 \nnames       : Proportion of C~ 2000-2022-2022, Mask: Sparsely Populated \nmin values  :                       0.0000000,                      NaN \nmax values  :                       0.3834594,                      NaN \n\npfpr_2022 &lt;- pfpr_2022[[1]]\n\n#the name of the raster is super long, so we'll fix that\nnames(pfpr_2022) &lt;- \"pfpr_2022\""
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#projectingreprojecting",
    "href": "posts/mapping-r-part2/mapping_rasters.html#projectingreprojecting",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Projecting/Reprojecting",
    "text": "Projecting/Reprojecting\nRaster data, like vector data, has a coordinate reference system (CRS). Sometimes we need to reproject a raster to match the CRS of other spatial layers.\nHowever, unlike vectors, raster data is made up of a fixed grid of cells. When projecting, this grid must be resampled, meaning new cell values are estimated based on the original ones. This can alter the data, so it’s best to avoid reprojecting rasters unless necessary. Common resampling methods including “nearest neighbor” for categorical data (e.g., land cover) and “bilinear” for continuous data (e.g., elevation, temperature).\nBecause projection of rasters affects the cell values, in most cases you will want to avoid projecting raster data and rather project vector data which will have no distortion effect. But here is how you can project raster data.\n\nr &lt;- rast(xmin=-110, xmax=-90, ymin=40, ymax=60, ncols=40, nrows=40)\nvalues(r) &lt;- 1:ncell(r)\nr\n\nclass       : SpatRaster \ndimensions  : 40, 40, 1  (nrow, ncol, nlyr)\nresolution  : 0.5, 0.5  (x, y)\nextent      : -110, -90, 40, 60  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nname        : lyr.1 \nmin value   :     1 \nmax value   :  1600 \n\nplot(r)\n\n\n\n\n\n\n\nnewcrs &lt;- \"+proj=robin +datum=WGS84\"\npr1 &lt;- terra::project(r, newcrs)\ncrs(pr1)\n\n[1] \"PROJCRS[\\\"unknown\\\",\\n    BASEGEOGCRS[\\\"unknown\\\",\\n        DATUM[\\\"World Geodetic System 1984\\\",\\n            ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n                LENGTHUNIT[\\\"metre\\\",1]],\\n            ID[\\\"EPSG\\\",6326]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8901]]],\\n    CONVERSION[\\\"unknown\\\",\\n        METHOD[\\\"Robinson\\\"],\\n        PARAMETER[\\\"Longitude of natural origin\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"False easting\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"(E)\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"(N)\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\nplot(pr1)\n\n\n\n\n\n\n\n\n\n# Define target CRS\n#we're going to use the Universal Mercator Projection (which makes the world flat)\ntarget_crs &lt;- \"EPSG:3857\"\n\n# Reproject raster\nprojected_population &lt;- project(population, target_crs, method = \"bilinear\") #bilinear because we assume population is continuous\n\nplot(projected_population)"
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#plotting-raster-data",
    "href": "posts/mapping-r-part2/mapping_rasters.html#plotting-raster-data",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Plotting Raster Data",
    "text": "Plotting Raster Data\nso far we’ve seen we can plot raster data using the basic terra functions of plot(). But perhaps you might want to change things about how the raster looks (e.g. colors) or bin the information, we might want to plot the data in ggplot() instead. We’re going to do this with our parasite prevalence surface\n\nggplot()+\n  geom_raster(data = pfpr_2022, mapping = aes(x=x, y =y, fill = pfpr_2022))+\n  coord_equal()\n\n\n\n\n\n\n\n\ngeom_raster() or geom_tile() are the two main functions you can use ggplot to plot raster/gridded data. Typically in both you would need to give it the x,y (coordinates) and the value to fill in the cell for it to plot correctly. You would also need to include the extra layer on how to deal with coordinates called coord_equal(). When you are adding in sf shapefiles you might switch to coord_sf() for it to know how to treat the coordinates appropriately. Try and see what the image looks like when you leave it out.\nAlternatively, geom_spatraster() comes from the package tidyterra and is the fastest and easiest way to make a plot of a raster in ggplot, we’ll mostly use geom_spatraster from here onwards as then we won’t need to include additional information on x,y and the coordinates information. From the above plot you can see the default is not particularly pretty, so let’s make this look prettier\n\nggplot(tz_districts)+\n  geom_sf()+\n  geom_spatraster(data = pfpr_2022, mapping = aes(fill = pfpr_2022))+\n  geom_sf(fill = NA)+\n  scale_fill_distiller(palette = \"RdYlGn\", na.value = 'transparent')+\n  theme_void()+\n  labs(title = \"Plasmodium falciparum 2-10 for 2022\", fill = \"PfPR\")\n\n\n\n\n\n\n\n\n\n# Define color palette (5 bins = 5 colors)\npfpr_pal &lt;- brewer.pal(n = 5, name = \"RdYlGn\")\npfpr_pal &lt;- rev(pfpr_pal) #reverse it to make low green and high red\n\n# Define break points\npfpr_breaks &lt;- c(0, 0.05, 0.1, 0.2, 0.3, 1)\n\nggplot() +\n  geom_spatraster(data = pfpr_2022, aes(fill = pfpr_2022)) +\n  geom_sf(data = tz_districts, fill = NA) +\n  scale_fill_stepsn(colours = pfpr_pal, breaks = pfpr_breaks, na.value = 0) +\n  theme_void() +\n  labs(title = \"Plasmodium falciparum 2-10 for 2022\", fill = \"PfPR\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1: Make a plot of the population\n\n\n\n\nTry make the same map as above but using the population raster instead\nWhat color palette would help make this more representative?\nCan you perhaps change the scale of the values to log10?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(tz_districts)+\n  geom_sf()+\n  geom_spatraster(data = population, mapping = aes(fill = tza_ppp_2020_constrained))+\n  geom_sf(fill = NA)+\n  scale_fill_viridis_c(option = \"D\", na.value = \"transparent\", trans = \"log10\", direction = -1)+\n  theme_void()+\n  labs(title = \"Population count for 2022\", fill = \"All age\")\n\n&lt;SpatRaster&gt; resampled to 500424 cells.\n\n\nWarning in scale_fill_viridis_c(option = \"D\", na.value = \"transparent\", :\nlog-10 transformation introduced infinite values."
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#manipulating-rasters",
    "href": "posts/mapping-r-part2/mapping_rasters.html#manipulating-rasters",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Manipulating rasters",
    "text": "Manipulating rasters\nWe can find out the spatial extent of a raster by using the ext() function, and easily crop the raster to other extents using crop(). We can specify the coordinates we wish to crop the raster to, or a take the extent from a spatial object and crop the raster to that.\n\next(pfpr_2022)\n\nSpatExtent : 29.3333333333333, 40.4166666666666, -11.7499987284343, -0.999998728434268 (xmin, xmax, ymin, ymax)\n\npfpr1 &lt;- crop(pfpr_2022, c(-8,35,-6,25))   # c(xmin, xmax, ymin, ymax)\nplot(pfpr1)\n\n\n\n\n\n\n\n# crop malaria prevalence to just kilimanjaro\nmtwara &lt;- filter(tz_districts, name_1 == 'Mtwara')\nmtwara_pfpr &lt;- crop(pfpr_2022, mtwara)\nplot(mtwara_pfpr)\n\n\n\n\n\n\n\n\nWe may then want to change all of the raster cells which lay outside of the polygon for Mtwara region to be NA. This can be done using mask().\n\n# mask malaria prevalence to just kilimanjaro\nmtwara &lt;- filter(tz_districts, name_1 == 'Mtwara')\nmtwara_pfpr &lt;- crop(pfpr_2022, mtwara) %&gt;% mask(mtwara) #reccomend to crop to set new extents\nplot(mtwara_pfpr)\n\n\n\n\n\n\n\n\nwhen masking you’d want to consider that mask does not help with setting extents so its best to first crop and then mask"
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#aggregatingresampling",
    "href": "posts/mapping-r-part2/mapping_rasters.html#aggregatingresampling",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Aggregating/Resampling",
    "text": "Aggregating/Resampling\nAs you noticed in the population raster you have a very high resolution on 100m, when visualising it can be challenging so we may want to aggregate the rasters up.\n\npopulation_1km &lt;- aggregate(population, fact = 10, fun = \"sum\", na.rm=TRUE)\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\nplot(population_1km)\n\n\n\n\n\n\n\n\nYou can disaggregate using the function disagg but you would need to be careful as you would need to use the methods “near” or “bilinear” to interpolate into smaller cells. This could distort the information in the raster unknowningly."
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#raster-maths",
    "href": "posts/mapping-r-part2/mapping_rasters.html#raster-maths",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Raster maths",
    "text": "Raster maths\nwith multiple rasters you can also do some simple calculations. One that might be useful is if you wanted to calculated the population at risk of malaria. A few things to remember is that the rasters must be of the same extent. We can use the resample function to align them\n\n#check extent matches\next(population_1km) == ext(pfpr_2022)\n\n[1] FALSE\n\n#if they don't match use resample to get them to match\npopulation_1km_resamp &lt;- resample(population_1km, pfpr_2022)\n\n#Now we can multiply the population and prevalence information to get population at risk\npop_at_risk &lt;- population_1km_resamp * pfpr_2022\nnames(pop_at_risk) = \"population_at_risk\"\n\nggplot()+\n  geom_raster(pop_at_risk, mapping = aes(x = x, y = y, fill = population_at_risk))+\n  geom_sf(tz_districts, mapping = aes(geometry = geometry), fill=NA)+\n  scale_fill_viridis_c(option = \"B\", trans = \"log10\", na.value = \"transparent\")+\n  theme_void()+\n  coord_sf()+\n  labs(title = \"Population at risk in 2022\", fill = \"Population\")\n\nWarning in scale_fill_viridis_c(option = \"B\", trans = \"log10\", na.value =\n\"transparent\"): log-10 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2: Modifying prevalence into a percentage\n\n\n\n\nCan you try to manipulate the pfpr_2022 raster to be in percentage form?\nMake a plot of the new percentage form pfpr_2022 raster\nCan you try to categorise it.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Define color palette (5 bins = 5 colors)\npfpr_pal &lt;- brewer.pal(n = 5, name = \"RdYlGn\")\npfpr_pal &lt;- rev(pfpr_pal) #reverse it to make low green and high red\n\n# Define break points\npfpr_breaks &lt;- c(0, 5, 10, 20, 30,100)\n\nggplot(tz_districts)+\n  geom_sf()+\n  geom_spatraster(data = pfpr_2022*100, mapping = aes(fill = pfpr_2022))+\n  geom_sf(fill = NA)+\n  scale_fill_stepsn(colours = pfpr_pal, breaks = pfpr_breaks, na.value = 0) +\n  theme_void()+\n  labs(title = \"Plasmodium falciparum 2-10 for 2022\", fill = \"PfPR\")"
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#extracting-information",
    "href": "posts/mapping-r-part2/mapping_rasters.html#extracting-information",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Extracting information",
    "text": "Extracting information\nWe might want to summarise the rasters to the district level. We can do that using the extract function\n\npop_risk &lt;- terra::extract(pop_at_risk, vect(tz_districts), sum, na.rm=TRUE, ID = FALSE)\n\ntz_districts &lt;- bind_cols(tz_districts, pop_risk)\n\nggplot(tz_districts)+\n  geom_sf(mapping = aes(fill = population_at_risk))+\n  scale_fill_distiller(palette = \"Reds\", direction = 1, trans = 'log10', na.value = \"lightblue\")+\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3: Extracting more information\n\n\n\n\nCan you extract the pfpr_2022 only values and the population separately?\nDo you get a different value if you use population at 100m vs population_1km at 1km?\nCan you explain why?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npfpr &lt;- terra::extract(pfpr_2022, vect(tz_districts), mean, na.rm=TRUE, ID = FALSE)\npop &lt;- terra::extract(population, vect(tz_districts), sum, na.rm=TRUE, ID = FALSE)\n\ntz_districts &lt;- bind_cols(tz_districts, pfpr, pop)\n\nggplot(tz_districts)+\n  geom_sf(mapping = aes(fill = pfpr_2022))+\n  scale_fill_distiller(palette = \"RdYlGn\", na.value = \"lightblue\")+\n  theme_void()"
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#mutliband-data",
    "href": "posts/mapping-r-part2/mapping_rasters.html#mutliband-data",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Mutliband data",
    "text": "Mutliband data\ni’ve mentioned previously that terra in R has the ability to deal with multiple bands/layers of rasters. So we can load several rasters all in at the same time and perform calculations on them in the same way which is great! the rasters do need to be of the same spatial extent and projection for them to be loaded in correctly. Here we’re going to load in data pulled from CHIRPS about monthly rainfall in 2022.\n\n#first we create a list of all the rasters for chirps\nrainfall_rasters &lt;- list.files(path = \"data/rasters/\", pattern = \"chirps\", full.names = TRUE)\n\n#then we'll load it into R the same way we do a single band\nrainfall_2022 &lt;- rast(rainfall_rasters)\nrainfall_2022\n\nclass       : SpatRaster \ndimensions  : 258, 267, 12  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : 29.33333, 40.45833, -11.75, -1  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsources     : chirps-v2-0.2022.01.sum.5km.NN.tif  \n              chirps-v2-0.2022.02.sum.5km.NN.tif  \n              chirps-v2-0.2022.03.sum.5km.NN.tif  \n              ... and 9 more sources\nnames       : chirp~km.NN, chirp~km.NN, chirp~km.NN, chirp~km.NN,  chirp~km.NN,  chirp~km.NN, ... \nmin values  :    20.05125,    10.48068,    9.421162,    4.000703,   0.04140074, 2.701764e-10, ... \nmax values  :   944.62292,   671.34375,  610.169739,  804.803101, 403.89791870, 2.246316e+02, ... \n\n\nyou’ll find the meta data shows you have 12 nlyrs (layers) that have been loaded in and each layer is a month of rainfall. We can plot this to see what it looks like\n\nplot(rainfall_2022)\n\n\n\n\n\n\n\n\nThe nice thing is it plots all 12 at the same time, but uses free scales. We can also use ggplot to do the same\n\n#let's maybe first clean up the names, turn them into dates\nnames(rainfall_2022) &lt;- seq(ym(\"2022-01\"), ym(\"2022-12\"), by = \"months\") %&gt;% format(\"%b %Y\")\n\nggplot()+\n  geom_spatraster(data = rainfall_2022)+\nfacet_wrap(~lyr, ncol = 4)+\n  scale_fill_distiller(palette = \"Blues\", direction = 1, na.value = \"transparent\", trans = 'sqrt')+\n  theme_void()+\n  labs(fill = \"mm\", title = \"Rainfall\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4: Handling rainfall\n\n\n\n\nCan you extract the sum of rainfall in every district in Tanzania for each month?\nMake a plot of the rainfall patterns in regions by month\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nrainfall &lt;- terra::extract(rainfall_2022, tz_districts, sum, na.rm=TRUE, ID = FALSE)\n\ntz_rainfall &lt;- tz_districts %&gt;% \n  bind_cols(rainfall)\n\ntz_rainfall %&gt;% \n  pivot_longer(cols = `Jan 2022`:`Dec 2022`, names_to = \"date\", values_to =\"rain\") %&gt;% \nggplot()+\n  geom_sf(mapping = aes(fill = rain/1000))+\n  facet_wrap(~date)+\n  scale_fill_distiller(palette = \"Blues\", direction = 1, na.value = \"transparent\", trans = 'sqrt')+\n  theme_void()+\n  labs(fill = \"m\", title = \"Rainfall\")"
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#rasterizing",
    "href": "posts/mapping-r-part2/mapping_rasters.html#rasterizing",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Rasterizing",
    "text": "Rasterizing\nYou might want to sometimes convert vector data into raster. This process is called rasterizing. For this function to work you need a template raster you want to use to provide it the resolution, crs and extents.\n\nrasterise_pop_risk &lt;- rasterize(tz_districts, field = \"population_at_risk\", pop_at_risk)\nplot(rasterise_pop_risk)"
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#exporting-raster-data",
    "href": "posts/mapping-r-part2/mapping_rasters.html#exporting-raster-data",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Exporting raster data",
    "text": "Exporting raster data\nUse writeRaster to write raster data. You must provide a SpatRaster and a filename. The file format will be guessed from the filename extension. If that does not work you can provide an argument like format=GTiff. Note the argument overwrite=TRUE and see ?writeRaster for more arguments, such as datatype= to set the a specific datatype (e.g., integer).\n\nwriteRaster(pop_at_risk, \"data/rasters/population_risk_2022.tif\")"
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#sources-for-raster-data-for-modelling",
    "href": "posts/mapping-r-part2/mapping_rasters.html#sources-for-raster-data-for-modelling",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Sources for Raster data for modelling",
    "text": "Sources for Raster data for modelling\nIn this tutorial I’ve only shown you two sources of information from WorldPop and MAP but there can be mainly other useful sources to get others types of rasters like environmental covariates\nWorldClim a great resource for bio climatic raster pre-made. They include historical (1970 - 2000) and future (upto 2100) and are processed for Global climate models with some of the SSPs scenarios. The data also comes at different resolutions depending on your analysis.\nGoogleEarthEngine: got alot of pre-processes raster data like MODIS NASA (temperature, EVI, landcover) as well as some of the MAP products. It includes some really nice ones too like Google Build footprints\nVectorAtlas: a great place to find some pulled together vector information and some of the published vector maps. They are coming out with some new suitability maps very soon so a great place to view for all the mathematical modelers looking for vector information.\nIHME burden estimates: some rasters for different diseases and mortality get published here and might be a good resource outside of malaria but also for mortality trends"
  },
  {
    "objectID": "posts/mapping-r-part2/mapping_rasters.html#additional-resources",
    "href": "posts/mapping-r-part2/mapping_rasters.html#additional-resources",
    "title": "Live Session 4: Mapping in R part 2",
    "section": "Additional Resources",
    "text": "Additional Resources\nCarpentries Introduction to Geospatial Raster and Vector data in R is a great tutorial that inspired much of the material in this hackathon. Try it out!\nMAP Training is the foundational material used to develop these notes and has much more information beyond just rasters\nThe terra package highlights even more cooler things you can do with rasters in R beyond what we can cover here so worth checking out for the enthusiasts!\n\n\n\n\n\n\nExtra data challenge! Tidytuesday\n\n\n\nFor the entusiasts whom might want to try making cooler rasters here is a tidytuesday challenge. to map out Global Holidays and Travel courtesy of WorldPop\nFor more fun challenges and getting practise in general in R i highly reccomend trying out tidytuesday"
  },
  {
    "objectID": "posts/data-wrangle-french/index.html",
    "href": "posts/data-wrangle-french/index.html",
    "title": "Live Session 4: Introduction à la manipulation de données dans R",
    "section": "",
    "text": "Take the survey!\n\n\n\nSi vous êtes présent à la session en direct le lundi 21 octobre, veuillez cliquer ici pour participer à l’enquête."
  },
  {
    "objectID": "posts/data-wrangle-french/index.html#introduction",
    "href": "posts/data-wrangle-french/index.html#introduction",
    "title": "Live Session 4: Introduction à la manipulation de données dans R",
    "section": "Introduction",
    "text": "Introduction\n\nQu’entendons-nous par “data wrangling” (traitement des données)?\nLe dictionnaire de Cambridge donne plusieurs significations au verbe wrangle:\n\nto argue with someone about something, especially for a long time (se disputer avec quelqu’un à propos de quelque chose, en particulier pendant une longue période)\nto take care of, control, or move animals, especially large animals such as cows or horses (mainly American English) (prendre soin, contrôler ou déplacer des animaux, en particulier les gros animaux comme les vaches ou les chevaux)\nto move a person or thing somewhere, usually with difficulty or using force (déplacer une personne ou une chose quelque part, généralement avec difficulté ou en utilisant la force)\nto take care of or deal with something, usually when this is difficult (prendre soin de ou gérer quelque chose, généralement lorsque cela est difficile)\n\n\n\nData Wrangling (traitement des données)\nPar “traitement des données”, nous entendons ici le processus de vérification et de correction de la qualité et de l’intégrité des données pertinentes pour la modélisation du paludisme, avant toute analyse ultérieure. Ce processus est également connu sous le nom de validation des données.\nLa validation de données consiste à vérifier divers aspects de votre ensemble de données, tels que les valeurs manquantes, les types de données, les valeurs aberrantes et le respect de règles ou de contraintes spécifiques.\nValider nos données contribue à maintenir leur qualité et leur intégrité, garantissant que toutes les analyses ou décisions prises sur la base des données sont robustes et fiables.\n\n\nPourquoi valider les données?\nGarantir l’intégrité des données : la validation des données permet d’identifier et de corriger les erreurs, garantissant ainsi l’intégrité de l’ensemble de données.\nAméliorer l’exactitude des analyses : des données propres et validées permettent d’obtenir des résultats d’analyse et de modélisation plus précis.\nConformité et normes : la validation des données garantit que les données sont conformes aux règles, normes ou exigences réglementaires prédéfinies.\nPrévention des erreurs : la détection précoce des erreurs permet d’éviter les problèmes en aval et de faire gagner du temps lors de leur résolution."
  },
  {
    "objectID": "posts/data-wrangle-french/index.html#getting-started",
    "href": "posts/data-wrangle-french/index.html#getting-started",
    "title": "Live Session 4: Introduction à la manipulation de données dans R",
    "section": "Getting Started",
    "text": "Getting Started\nBefore you begin, you might want to create a new project in RStudio. This can be done by clicking on the “New Project” button in the upper right corner of the RStudio window. You can then name the project and choose a directory to save it in.\nNext, we will load the tidyverse package. This package provides a set of useful functions for data manipulation and visualization. We will use the ggplot2 package to create plots in the later section of this tutorial.\nAvant de commencer, il est recommendé de créer un nouveau projet dans RStudio. Pour cela, cliquez sur le bouton “Nouveau projet” dans le coin supérieur droit de la fenêtre RStudio. Vous pouvez ensuite nommer le projet et choisir un répertoire dans lequel l’enregistrer.\nEnsuite, nous allons charger le package tidyverse. Ce package fournit un ensemble de fonctions utiles pour la manipulation et la visualisation des données. Nous utiliserons le package ggplot2 pour créer des graphiques dans la dernière section de ce tutoriel.\n\n# load packages\nlibrary(tidyverse)\n\nEnsuite, nous allons télécharger les deux exemples de jeux de données que nous utiliserons dans ce tutoriel. Ceux-ci sont disponibles dans le répertoire GitHub AMMnet Hackathon.\nJe suggère de créer un dossier data dans votre projet R, puis nous pourrons télécharger les deux exemples d’ensembles de données afin qu’ils soient enregistrés sur votre ordinateur.\n\n# Create a data folder\ndir.create(\"data\")\n\n# Download example data\nurl &lt;- \"https://raw.githubusercontent.com/AMMnet/AMMnet-Hackathon/main/02_data-wrangle/data/\"\n\ndownload.file(paste0(url, \"mockdata_cases1.csv\"), destfile = \"data/mockdata_cases1.csv\")\ndownload.file(paste0(url, \"mosq_mock1.csv\"), destfile = \"data/mosq_mock1.csv\")\n\n# Load example data\ndata_cases   &lt;- read_csv(\"data/mockdata_cases1.csv\")\nmosq_data  &lt;- read_csv(\"data/mosq_mock1.csv\")\n\nLes deux ensembles de données que nous utiliserons sont mockdata_cases1.csv et mosq_mock1.csv, qui sont des exemples d’ensembles de données fictifs qui devraient être similaires aux données de surveillance des cas de paludisme et de collecte de moustiques sur le terrain, respectivement. Dans les sections suivantes, nous utiliserons mockdata_cases1.csv et mosq_mock1.csv pour introduire les concepts de nettoyage et de caractérisation des données dans R."
  },
  {
    "objectID": "posts/data-wrangle-french/index.html#vérifier-les-données-pour-détecter-déventuelles-erreurs",
    "href": "posts/data-wrangle-french/index.html#vérifier-les-données-pour-détecter-déventuelles-erreurs",
    "title": "Live Session 4: Introduction à la manipulation de données dans R",
    "section": "1. Vérifier les données pour détecter d’éventuelles erreurs",
    "text": "1. Vérifier les données pour détecter d’éventuelles erreurs\n\nLa prévalence est une fraction définie dans [0,1]\nNote: Une prévalence de 0 ou 1, bien que non statistiquement erronée, doit être vérifiée pour en garantir l’exactitude.\nQuelles observations comportent des erreurs ?\n\n# Erroneous values for prevalence\ndata_cases%&gt;%\n   dplyr::filter(prev &lt;= 0 | prev &gt;= 1)\n\n# A tibble: 3 × 10\n  location month  year ages  total positive xcoord ycoord    prev time_order_loc\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1 mordor       4  2018 15_a…    91       23  -20.0   30.5 25.3                 4\n2 neverwh…     2  2019 15_a…    22       -1  -20.8   29.6 -0.0455             14\n3 neverwh…     3  2018 unde…    25        0  -19.8   30.2  0                   3\n\n\nCommentaire: Nous avons deux observations avec des valeurs non-logiques de prev : 25.3and -0.455, et une ligne avec prev égale zéro pour un certain mois.\n\n\nProgrammation défensive\nRemarque: L’utilisation de “::” nous permet d’appeler une fonction à partir d’un package spécifique de R. Il arrive que si le package de base “stats” est appelé en premier, la fonction de filter, si elle n’est pas spécifiée avec le package R, échoue.\n\n# Erroneous values for prevalence\ndata_cases%&gt;%\n    stats::filter(prev &lt; 0 | prev &gt; 1) \n\n\n\nNous corrigeons les deux prévalences en recalculant\nUne bonne pratique est de laisser les données d’origine intactes (avantage de R sur Stata):\n\n# Update erroneous values for prevalence\ndata_prev &lt;- data_cases%&gt;%\n                       dplyr::mutate(prev_updated=positive/total)\n\nNous avons une observation avec une valeur négative par erreur.\nQuelles sont vos options?\n\nNe jamais supprimer les données\nInterroger et demander à l’équipe de gestion des données de procéder aux investigations nécessaires et de procéder à une correction.\n\n\ndata_prev%&gt;%\n    dplyr::filter(prev_updated &lt;= 0 | prev_updated &gt;= 1)\n\n# A tibble: 2 × 11\n  location month  year ages  total positive xcoord ycoord    prev time_order_loc\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1 neverwh…     2  2019 15_a…    22       -1  -20.8   29.6 -0.0455             14\n2 neverwh…     3  2018 unde…    25        0  -19.8   30.2  0                   3\n# ℹ 1 more variable: prev_updated &lt;dbl&gt;\n\n\nPour l’instant (afin de poursuivre cette démonstration), nous abandonnons les observations problématiques.\nPourquoi cela ne fonctionne-t-il pas ?\n\n# Filter erroneous values for prevalence, wrong way\ndata_use &lt;- data_prev%&gt;%\n              dplyr::filter (prev_updated &gt;= 0 | prev_updated &lt;= 1)\n\nPourquoi cela ne fonctionne-t-il pas ?\n\n# Filter erroneous values for prevalence\ndata_use &lt;- data_prev%&gt;%\n             dplyr::filter (prev_updated &gt;= 0 )%&gt;%\n              dplyr::filter (prev_updated &lt;= 1)\n\ndata_use%&gt;%\n       dplyr::filter(prev_updated &lt;= 0 | prev_updated &gt;= 1)\n\n# A tibble: 1 × 11\n  location   month  year ages  total positive xcoord ycoord  prev time_order_loc\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1 neverwhere     3  2018 unde…    25        0  -19.8   30.2     0              3\n# ℹ 1 more variable: prev_updated &lt;dbl&gt;\n\n\n\n\nSchemas\nPour éviter que des erreurs n’apparaissent dans vos données, vous devez définir un schéma qui accompagne vos données enregistrées. Un schéma est un document qui énonce les règles relatives aux types de données et aux valeurs ou plages attendues dans une colonne particulière de votre bloc de données.\nPar exemple, pour la prévalence, nous savons qu’il doit s’agir d’un nombre réel compris entre zéro et un.\nLe package R validate peut être utilisé pour créer un schéma pour votre trame de données:\n\n# Filter erroneous values for prevalence\nlibrary(validate)\nschema &lt;- validate::validator(prev &gt;= 0,\n                   prev &lt;= 1,\n                   positive &gt;= 0)\n\nout   &lt;- validate::confront(data_cases, schema)\nsummary(out)\n\n  name items passes fails nNA error warning             expression\n1   V1   514    513     1   0 FALSE   FALSE     prev - 0 &gt;= -1e-08\n2   V2   514    513     1   0 FALSE   FALSE      prev - 1 &lt;= 1e-08\n3   V3   514    513     1   0 FALSE   FALSE positive - 0 &gt;= -1e-08\n\n\nEn utilisant le schéma des colonnes prev et positive, nous aurions pu facilement détecter les trois entrées problématiques. Pour plus de détails, vous pouvez consulter la vignette du package validate.\nRemarque: La prochaine fois que vous recevrez des données de vos collaborateurs, n’oubliez pas de leur demander le fichier de schéma associé (par exemple, au format YAML). Bonne chance !"
  },
  {
    "objectID": "posts/data-wrangle-french/index.html#consultez-les-statistiques-récapitulatives",
    "href": "posts/data-wrangle-french/index.html#consultez-les-statistiques-récapitulatives",
    "title": "Live Session 4: Introduction à la manipulation de données dans R",
    "section": "2. Consultez les statistiques récapitulatives",
    "text": "2. Consultez les statistiques récapitulatives\n\nStatistiques agrégées par lieu de collecte (pour toutes les dates)\n\n# Summary statistics \n\ndata_use%&gt;%\n   dplyr::group_by(location)%&gt;%\n     dplyr::summarise(nobs=n(),\n                      mean_prev=mean(prev_updated),\n                      min_prev=min(prev_updated),\n                      max_prev=max(prev_updated))\n\n# A tibble: 5 × 5\n  location    nobs mean_prev min_prev max_prev\n  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mordor       105     0.314   0.158     0.488\n2 narnia       104     0.326   0.08      0.488\n3 neverwhere    95     0.301   0         0.486\n4 oz           104     0.255   0.0714    0.459\n5 wonderland   105     0.382   0.194     0.535\n\n\n\n\nStatistiques agrégées par lieu de collecte et par année\nLe tableau s’allonge. Il pourrait être trop compliqué d’ajouter des contrôles par mois et par groupe d’âge.\nRemarque: Pourquoi n’y a-t-il que 3 mesures en 2020?\n\n# Summary statistics by location\ndata_use%&gt;%\n  dplyr::group_by(location, year)%&gt;%\n  dplyr::summarise(nobs=n(),\n                   mean_prev=mean(prev_updated),\n                   min_prev=min(prev_updated),\n                   max_prev=max(prev_updated))\n\n# A tibble: 15 × 6\n# Groups:   location [5]\n   location    year  nobs mean_prev min_prev max_prev\n   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 mordor      2018    36     0.318   0.206     0.473\n 2 mordor      2019    36     0.313   0.170     0.451\n 3 mordor      2020    33     0.312   0.158     0.488\n 4 narnia      2018    36     0.340   0.138     0.449\n 5 narnia      2019    36     0.361   0.216     0.488\n 6 narnia      2020    32     0.270   0.08      0.483\n 7 neverwhere  2018    36     0.304   0         0.45 \n 8 neverwhere  2019    56     0.298   0.0370    0.486\n 9 neverwhere  2020     3     0.307   0.04      0.473\n10 oz          2018    35     0.252   0.0714    0.459\n11 oz          2019    36     0.254   0.0861    0.446\n12 oz          2020    33     0.260   0.112     0.405\n13 wonderland  2018    36     0.365   0.255     0.454\n14 wonderland  2019    36     0.388   0.194     0.535\n15 wonderland  2020    33     0.393   0.276     0.476\n\n\n\n\n\n\n\n\nDéfi 1 : Explorer les ensembles de données data_prev et data_use\n\n\n\n\nCréez un tableau indiquant le nombre d’entrées de données par groupe d’âge et par lieu de collecte pour chacun d’eux!\nDans quel groupe d’âge et dans quel localité les observations ont-elles été supprimées?\n\n\n\nUn peu plus avancé. Utilisation de listes (ce n’est pas le sujet du cours mais c’est un point important).\n\n# Summary statistics by location\ndata_use_list &lt;- data_use%&gt;%\n                  dplyr::group_split(location)\n\nOu utilisez la librairie purrr :\n\n# Summary statistics by location, map summary function\nlibrary(purrr)\n\ndata_use_age_summary &lt;- purrr::map(.x=seq(length(data_use_list)),\n                                   .f=function(x){\n                                     data_use_list[[x]]%&gt;%\n                                       dplyr::group_by(location,year,ages)%&gt;%\n                                       dplyr::summarise(nobs=n(),\n                                                        mean_prev=mean(prev_updated),\n                                                        min_prev=min(prev_updated),\n                                                        max_prev=max(prev_updated)) \n                                     \n                                   })\n\n\n\nConcentrons-nous maintenant sur le premier objet de la liste (mordor)\nNous savons que les femmes enceintes et les enfants de moins de 5 ans sont les plus vulnérables.\nOutput (ages) n’est pas ordonné comme nous le souhaiterions (chronologiquement).\n\n# Summary statistics by location\n\ndata_mordor &lt;- data_use_age_summary[[1]]\n\ndata_mordor\n\n# A tibble: 9 × 7\n# Groups:   location, year [3]\n  location  year ages      nobs mean_prev min_prev max_prev\n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mordor    2018 15_above    12     0.270    0.206    0.369\n2 mordor    2018 5_to_14     12     0.335    0.219    0.427\n3 mordor    2018 under_5     12     0.348    0.259    0.473\n4 mordor    2019 15_above    12     0.266    0.170    0.377\n5 mordor    2019 5_to_14     12     0.278    0.176    0.390\n6 mordor    2019 under_5     12     0.394    0.315    0.451\n7 mordor    2020 15_above    11     0.255    0.158    0.333\n8 mordor    2020 5_to_14     11     0.352    0.258    0.488\n9 mordor    2020 under_5     11     0.330    0.190    0.422\n\n\n\n\nComment procéder ?\n\n# Summary statistics with age groups\nage_order &lt;- c(\"under_5\",\"5_to_14\",\"15_above\")\n\ndata_use_ordered &lt;- data_use\n\ndata_use_ordered$age_group &lt;- factor(data_use$ages, levels =age_order)\n\ndata_mordor_reordered &lt;- data_use_ordered%&gt;%\n                           dplyr::group_by(location, year,age_group)%&gt;%\n                            dplyr::summarise(nobs=n(),\n                                             mean_prev=mean(prev_updated),\n                                             min_prev=min(prev_updated),\n                                             max_prev=max(prev_updated))%&gt;%\n                                 dplyr::filter(location==\"mordor\")\n\nComparons les deux\n\n# Compare for Mordor\n\ndata_mordor\n\n# A tibble: 9 × 7\n# Groups:   location, year [3]\n  location  year ages      nobs mean_prev min_prev max_prev\n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mordor    2018 15_above    12     0.270    0.206    0.369\n2 mordor    2018 5_to_14     12     0.335    0.219    0.427\n3 mordor    2018 under_5     12     0.348    0.259    0.473\n4 mordor    2019 15_above    12     0.266    0.170    0.377\n5 mordor    2019 5_to_14     12     0.278    0.176    0.390\n6 mordor    2019 under_5     12     0.394    0.315    0.451\n7 mordor    2020 15_above    11     0.255    0.158    0.333\n8 mordor    2020 5_to_14     11     0.352    0.258    0.488\n9 mordor    2020 under_5     11     0.330    0.190    0.422\n\ndata_mordor_reordered\n\n# A tibble: 9 × 7\n# Groups:   location, year [3]\n  location  year age_group  nobs mean_prev min_prev max_prev\n  &lt;chr&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mordor    2018 under_5      12     0.348    0.259    0.473\n2 mordor    2018 5_to_14      12     0.335    0.219    0.427\n3 mordor    2018 15_above     12     0.270    0.206    0.369\n4 mordor    2019 under_5      12     0.394    0.315    0.451\n5 mordor    2019 5_to_14      12     0.278    0.176    0.390\n6 mordor    2019 15_above     12     0.266    0.170    0.377\n7 mordor    2020 under_5      11     0.330    0.190    0.422\n8 mordor    2020 5_to_14      11     0.352    0.258    0.488\n9 mordor    2020 15_above     11     0.255    0.158    0.333"
  },
  {
    "objectID": "posts/data-wrangle-french/index.html#utilisation-des-graphiques",
    "href": "posts/data-wrangle-french/index.html#utilisation-des-graphiques",
    "title": "Live Session 4: Introduction à la manipulation de données dans R",
    "section": "3. Utilisation des graphiques",
    "text": "3. Utilisation des graphiques\n\nNous devons évaluer l’évolution de la prévalence pour toutes les régions par mois\n\n#Plotting evolution over time\nevolution_plot &lt;- ggplot2::ggplot(data=data_use_ordered,\n                                  mapping=aes(x=month,\n                                              y=prev_updated,\n                                              group=location,\n                                              colour=location))+\n                        ggplot2::geom_line(lwd=1.1)+\n                           ggplot2::facet_wrap(~year)+ \n                            ggplot2::theme_bw()+\n                             ggplot2::xlab(\"Month of the Year\")+\n                               ggplot2::ylab(\"Prevalence\")+\n                                ggplot2::scale_x_discrete(limits=factor(1:12),\n                                                          labels=c(\"J\",\"F\",\"M\",\n                                                                   \"A\",\"M\",\"J\",\n                                                                   \"J\",\"A\",\"S\",\n                                                                   \"O\",\"N\",\"D\"))+\n                                   ggplot2::scale_y_continuous(breaks=seq(from=0,\n                                                                          to=0.7,\n                                                                          by=0.1))\n\nevolution_plot\n\n\n\n\n\n\n\n\nObservation: Graphique de prévalence avec des lignes verticales par mois et par an: nous avons plusieurs sous-groupes pour les données de prévalence et nous traçons des facettes pour les différentes catégories de age_group.\n\n#Plotting evolution over time, fix 1\nevolution_plot_ages &lt;- ggplot2::ggplot(data=data_use_ordered,\n                                  mapping=aes(x=month,\n                                              y=prev_updated,\n                                              group=location,\n                                              colour=location))+\n  ggplot2::geom_line(lwd=1.1)+\n  ggplot2::facet_wrap(age_group~year)+ \n  ggplot2::theme_bw()+\n  ggplot2::xlab(\"Month of the Year\")+\n  ggplot2::ylab(\"Prevalence\")+\n  ggplot2::scale_x_discrete(limits=factor(1:12),\n                            labels=c(\"J\",\"F\",\"M\",\n                                     \"A\",\"M\",\"J\",\n                                     \"J\",\"A\",\"S\",\n                                     \"O\",\"N\",\"D\"))+\n  ggplot2::scale_y_continuous(breaks=seq(from=0,\n                                         to=0.7,\n                                         by=0.1))\n\nevolution_plot_ages\n\n\n\n\n\n\n\n\nObservation : QIl y a quelques améliorations, mais nous avons toujours des lignes verticales, peut-être avons-nous d’autres variables de groupe. Regardons uniquement les lignes qui ont plus d’une entrée par emplacement, mois, année, groupe d’âge.\n\n#Plotting evolution over time, fix 2\n\ndata_use_ordered%&gt;%\n  group_by(location,month,year,age_group)%&gt;%\n  tally()%&gt;%\n  filter(n&gt;1)%&gt;%\n  left_join(data_use_ordered)\n\n# A tibble: 48 × 13\n# Groups:   location, month, year [8]\n   location   month  year age_group     n ages     total positive xcoord ycoord\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 neverwhere     6  2019 under_5       2 under_5     24        4  -20.6   30.7\n 2 neverwhere     6  2019 under_5       2 under_5     26        1  -20.5   30.7\n 3 neverwhere     6  2019 5_to_14       2 5_to_14     27        5  -19.7   30.0\n 4 neverwhere     6  2019 5_to_14       2 5_to_14     27        8  -19.3   30.2\n 5 neverwhere     6  2019 15_above      2 15_above    70       31  -19.4   29.4\n 6 neverwhere     6  2019 15_above      2 15_above    74       27  -19.2   29.2\n 7 neverwhere     7  2019 under_5       2 under_5     25        5  -20.0   29.1\n 8 neverwhere     7  2019 under_5       2 under_5     26        4  -20.7   28.6\n 9 neverwhere     7  2019 5_to_14       2 5_to_14     27        7  -18.8   29.3\n10 neverwhere     7  2019 5_to_14       2 5_to_14     23        6  -20.4   29.8\n# ℹ 38 more rows\n# ℹ 3 more variables: prev &lt;dbl&gt;, time_order_loc &lt;dbl&gt;, prev_updated &lt;dbl&gt;\n\n\nObservation: OK, nous voyons qu’au sein d’une même localité, il existe plusieurs points de données de prévalence même si les coordonnées xcoord et ycoord diffèrent. Afin d’avoir un seul graphique par lieu, nous pourrions faire la moyenne de xcoord et ycoord dans chaque localité. Il peut aussi s’agir d’enregistrements dupliqués, puisque xcoord et ycoord sont très proches?\n\n#Plotting evolution over time, fix 3\n\ndata_use_ordered%&gt;%\n  group_by(location,month,year,age_group)%&gt;%\n  summarize(prev_updated_mean=mean(prev_updated),\n            prev_updated_min=min(prev_updated),\n            prev_updated_max=max(prev_updated))%&gt;%\n  ggplot2::ggplot(mapping=aes(x=month,\n                              y=prev_updated_mean,\n                              file=location,\n                              group=location,\n                              colour=location))+\n  ggplot2::geom_line(lwd=1.1)+\n  ggplot2::facet_wrap(age_group~year)+ \n  ggplot2::theme_bw()+\n  ggplot2::xlab(\"Month of the Year\")+\n  ggplot2::ylab(\"Prevalence\")+\n  ggplot2::scale_x_discrete(limits=factor(1:12),\n                            labels=c(\"J\",\"F\",\"M\",\n                                     \"A\",\"M\",\"J\",\n                                     \"J\",\"A\",\"S\",\n                                     \"O\",\"N\",\"D\"))+\n  ggplot2::scale_y_continuous(breaks=seq(from=0,\n                                         to=0.7,\n                                         by=0.1))\n\n\n\n\n\n\n\n\nObservation: La prévalence est très variable tout au long de l’année selon les endroits en moyenne; Wonderland étant affecté par une prévalence élevée tandis qu’Oz a la prévalence la plus faible."
  },
  {
    "objectID": "posts/data-wrangle-french/index.html#lensemble-de-données-sur-les-moustiques",
    "href": "posts/data-wrangle-french/index.html#lensemble-de-données-sur-les-moustiques",
    "title": "Live Session 4: Introduction à la manipulation de données dans R",
    "section": "L’ensemble de données sur les moustiques",
    "text": "L’ensemble de données sur les moustiques\nJetons un oeil à l’ensemble de données mosq_data.\nNous vérifions la cohérence de cet ensemble de données en affichant un tableau des valeurs enregistrées par colonne:\n\nmosq_data %&gt;%\n  map( function(x) table(x) )\n\n$session\nx\n 1  2 \n52 52 \n\n$Village\nx\nnaernia  narnia \n      2     102 \n\n$Compound.ID\nx\n 1  2  3  4 \n26 26 26 26 \n\n$Method\nx\nALC HLC \n  1 103 \n\n$Location\nx\n Indoor Outdoor \n     52      52 \n\n$hour\nx\n01h-02h 02h-03h 03h-04h 04h-05h 05h-06h 06h-07h 07h-08h 19h-20h 20h-21h 21h-22h \n      8       8       8       8       8       8       8       8       8       8 \n22h-23h 23h-24h 24h-01h \n      8       8       8 \n\n$ag.Male\nx\n 0  3  4  5  6  7 14 16 20 22 27 35 \n93  1  1  1  1  1  1  1  1  1  1  1 \n\n$Ag.unfed\nx\n 0  1  2  3  4  5  6  7  8 10 20 \n57 13  7  8  4  4  2  4  2  1  2 \n\n$Ag.halffed\nx\n 0  3  4  5  8  9 \n92  3  3  3  1  2 \n\n$Ag.fed\nx\n 0  1  3  5 \n88  7  3  6 \n\n$Ag.grsgr\nx\n 0  1  2  3  4  6  8 12 17 20 23 27 35 37 \n70 13  6  1  2  1  3  2  1  1  1  1  1  1 \n\n$tot.gamb\nx\n 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 23 \n40 10 10  9  8  6  3  2  1  1  5  2  1  2  2  2 \n\n$Culex.male\nx\n  0 \n104 \n\n$Culex.female\nx\n 0  1  2 \n94  9  1 \n\n$Mansonia.male\nx\n  0   1 \n103   1 \n\n$Mansonia.female\nx\n 0  1  2 \n90 11  3 \n\n$Aedes.male\nx\n  0 \n104 \n\n$Aedes.female\nx\n 0  1  2 \n98  5  1 \n\n\nIl semblerait que nous ayons quelques fautes de frappe dans les noms de Method and Village.\n\n\n\n\n\n\nDéfi 2 : Utilisation de schémas pour l’ensemble de données sur les moustiques\n\n\n\n\nCréez un schéma qui fournit des règles pour les strings (c’est-à-dire les chaînes de caractères) attendues dans les colonnes Method et Village.\nUtiliser la syntaxe fournie ici\n\n\n\n\nschema &lt;- validate::validator(Method%in%c(\"HLC\"),\n                              Village%in%c(\"narnia\"))\n\nout   &lt;- validate::confront(mosq_data, schema)\nsummary(out)\n\n  name items passes fails nNA error warning                expression\n1   V1   104    103     1   0 FALSE   FALSE     Method %vin% c(\"HLC\")\n2   V2   104    102     2   0 FALSE   FALSE Village %vin% c(\"narnia\")\n\n\nLes colonnes Village et Method semblent avoir des erreurs de saisie de données. Nous devons corriger cela.\n\nmosq_data&lt;-mosq_data%&gt;%\n  mutate(Method=ifelse(Method==\"ALC\",\"HLC\",Method),\n         Village=ifelse(Village==\"naernia\",\"narnia\",Village))\n\nIl semble que les différentes colonnes concernent les tailles de population d’Anopheles Gambiae. Modifions les noms des colonnes en utilisant rename du package tidyverse.\n\nmosq_data%&gt;%\n  rename(\"AnophelesGambiae.male\"=\"ag.Male\",\n         \"AnophelesGambiae.unfed\"=\"Ag.unfed\",\n         \"AnophelesGambiae.halffed\"=\"Ag.halffed\",\n         \"AnophelesGambiae.fed\"=\"Ag.fed\",\n         \"AnophelesGambiae.gravid\"=\"Ag.grsgr\")-&gt;mosq_data\n\nIl semble que tot.gamb devrait compter le nombre total d’Anopheles Gambiae. Vérifions:\n\nmosq_data%&gt;%\n  mutate(AnophelesGambiae_total=AnophelesGambiae.male+AnophelesGambiae.unfed+AnophelesGambiae.halffed+AnophelesGambiae.fed+AnophelesGambiae.gravid)-&gt;mosq_data\n\nmosq_data%&gt;%\n  filter(AnophelesGambiae_total!=tot.gamb)%&gt;%select(AnophelesGambiae_total,tot.gamb)\n\n# A tibble: 11 × 2\n   AnophelesGambiae_total tot.gamb\n                    &lt;dbl&gt;    &lt;dbl&gt;\n 1                     12        0\n 2                     16        2\n 3                      0        6\n 4                     24        8\n 5                     24        1\n 6                     74       12\n 7                     54        3\n 8                     70        1\n 9                     34        2\n10                     40        2\n11                     46        0\n\n\nDonc 11 lignes sur 104 présentent cette divergence. Gardons plutôt Anopheles.total, puisqu’il a été calculé à partir des données.\nComme le statut des Anopheles est mutuellement exclusif dans les données HLC, nous pouvons dessiner un graphique à barres empilées, avec la couleur des barres définie par le statut. Pour produire un tel graphique efficacement dans ggplot2, nous devons faire pivoter le tableau.\nIci en particulier, nous voulons passer d’un format large à un format long afin d’obtenir une colonne décrivant le statut des moustiques Anopheles. Nous utiliserons notamment l’argument names_sep de la fonction pivot_longer pour séparer par exemple le nom de colonne AnophelesGambiae.male et utiliser male comme niveau dans une nouvelle colonne appelée status. Il en va de même pour les autres noms de colonnes.\nIntégrer les variables session, Village, Compound.ID, Method, Location, hour, AnophelesGambiae_total dans la définition de la variable de regroupement aidera à conserver ces variables dans le tableau au format long.\n\nmosq_data%&gt;%\n  group_by(session,Village,Compound.ID,Method,Location,hour,AnophelesGambiae_total)%&gt;%\n  select(contains(\"AnophelesGambiae.\"))%&gt;%\n  pivot_longer(cols=contains(\"AnophelesGambiae.\"),names_sep=\"AnophelesGambiae.\",names_to=c(NA,\"status\"),values_to = \"AnophelesGambiae\")-&gt;mosq_data_gamb_wide\n\nmosq_data_gamb_wide%&gt;%\n  ggplot()+\n  geom_bar(aes(x=hour,y=AnophelesGambiae,fill=status),position=\"stack\",stat=\"identity\")+\n  scale_x_discrete(guide = guide_axis(angle = 60))\n\n\n\n\n\n\n\n\nObservation: Nous avons plusieurs valeurs pour Compound.ID. La géométrie geom_bar les additionne automatiquement dans le graphique. Nous pouvons utiliser facet_wrap pour voir ces strates:\n\nmosq_data_gamb_wide%&gt;%\n  ggplot()+\n  geom_bar(aes(x=hour,y=AnophelesGambiae,fill=status),position=\"stack\",stat=\"identity\")+\n  scale_x_discrete(guide = guide_axis(angle = 60))+\n  facet_wrap(~Compound.ID)\n\n\n\n\n\n\n\n\nNous pouvons aussi utiliser notre variable Anopheles_total et la représenter comme un grapique ligne au dessus du grapique en barres:\n\nmosq_data_gamb_wide%&gt;%\n  mutate(grouping=paste0(Compound.ID,Location,session))%&gt;%\n  ggplot()+\n  geom_bar(aes(x=hour,y=AnophelesGambiae,fill=status),position=\"stack\",stat=\"identity\")+\n  geom_line(aes(x=hour,y=AnophelesGambiae_total,group=grouping))+\n  scale_x_discrete(guide = guide_axis(angle = 60))+\n  facet_wrap(~Compound.ID+session+Location)"
  },
  {
    "objectID": "posts/data-vis/index.html",
    "href": "posts/data-vis/index.html",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "",
    "text": "This tutorial introduces you to data visualization in R. We will learn how to develop an understanding of our data before visualization, making quick exploratory visualizations using base R functions, and creating various plots using the ggplot2 package. You’ll learn how to customize and enhance your visualizations for clear data communication. By the end, you’ll have the skills to create plots to effectively present your data insights."
  },
  {
    "objectID": "posts/data-vis/index.html#getting-started",
    "href": "posts/data-vis/index.html#getting-started",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "Getting Started",
    "text": "Getting Started\nBefore you begin, you might want to create a new project in RStudio. This can be done by clicking on the “New Project” button in the upper right corner of the RStudio window. You can then name the project and choose a directory to save it in.\nNext, we will load the tidyverse package. This package provides a set of useful functions for data manipulation and visualization. We will use the ggplot2 package to create plots in the later section of this tutorial.\n\n# load packages\nlibrary(tidyverse)\n\nNext, let’s download the two example datasets we will use in this tutorial. These are avialable in the AMMnet Hackathon GitHub repository.\nI suggest creating a data folder inside your R project, then we can download the two example datasets so that they are saved to your computer.\n\n# Create a data folder\ndir.create(\"data\")\n\n# Download example data\nurl &lt;- \"https://raw.githubusercontent.com/AMMnet/AMMnet-Hackathon/main/01_data-vis/data/\"\n\ndownload.file(paste0(url, \"mockdata_cases.csv\"), destfile = \"data/mockdata_cases.csv\")\ndownload.file(paste0(url, \"mosq_mock.csv\"), destfile = \"data/mosq_mock.csv\")\n\n# Load example data\nmalaria_data   &lt;- read_csv(\"data/mockdata_cases.csv\")\nmosquito_data  &lt;- read_csv(\"data/mosq_mock.csv\")\n\nThe two datasets we will use are mockdata_cases.csv and mosq_mock.csv, which are mock example datasets that should be similar to malaria case surviellance and mosquito field collection data, respectively. In the following sections we will use the mockdata_cases.csv to introduce concepts of data visualization in R. The mosq_mock.csv dataset is used in the challenge sections."
  },
  {
    "objectID": "posts/data-vis/index.html#characterizing-our-data",
    "href": "posts/data-vis/index.html#characterizing-our-data",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "Characterizing our data",
    "text": "Characterizing our data\nBefore we start visualizing our data, we need to understand the characteristics of our data. The goal is to get an idea of the data structure and to understand the relationships between variables.\nHere are some functions that can help us understand the structure of our data:\n\n# Explore the structure and summary of the datasets\ndim(malaria_data)  \nhead(malaria_data)\nsummary(malaria_data)\n\nWe should also explore individual columns/variables\n\nmalaria_data$location          # values for a single column\nunique(malaria_data$location)  # unique values for a single column\ntable(malaria_data$location)   # frequencies for a single column\ntable(malaria_data$location, malaria_data$ages)  # frequencies for multiple columns\n\nFinally, we should check for missing values in each column, as these can affect our visualizations.\n\nsum(is.na(malaria_data))\n\n[1] 0\n\n\n\n\n\n\n\n\nChallenge 1: Explore the structure and summary of the mosquito_data dataset\n\n\n\n\nWhat are the dimensions of the dataset?\nWhat are the column names?\nWhat are the column types?\nWhat are some key variables or relationships that we can explore?"
  },
  {
    "objectID": "posts/data-vis/index.html#exploratory-visualizations-using-base-r-functions",
    "href": "posts/data-vis/index.html#exploratory-visualizations-using-base-r-functions",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "Exploratory Visualizations Using Base R Functions",
    "text": "Exploratory Visualizations Using Base R Functions\nFirst, we will look at some exploratory data visualization techniques using base R functions. The purpose of these plots is to help us understand the relationships between variables and characteristics of our data. They are useful for quickly exploring the data and understanding the relationships, but they are not are not great for sharing in scientific publications/presentations.\n\nSingle variable comparison\nFor one variable comparison, we can use hist() function to create a histogram.\n\nhist(malaria_data$prev)\n\n\n\n\n\n\n\nhist(malaria_data$prev, \n    breaks = 10, \n    main = \"Distribution of Malaria Prevalence\",\n    xlab = \"Malaria Prevalence\",\n    ylab = \"Frequency\",\n    col = \"lightblue\",\n    border = \"black\")\n\n\n\n\n\n\n\n\nAnnother useful function for single variable comparisons is barplot(). In this case, we will use the table() function to count the number of observations in each category, then use barplot() to create a barplot.\n\nbarplot(table(malaria_data$ages))\n\n\n\n\n\n\n\nbarplot(table(malaria_data$location))\n\n\n\n\n\n\n\nbarplot(table(malaria_data$year))\n\n\n\n\n\n\n\n\n\n\nMultiple variables\nFor multiple variables, we can use plot() function to create a scatterplot. In this case, we will use the S operator to pull out an individual column from the dataset. Then we will use plot() to create a scatterplot. The first argument in plot() is the x variable, and the second argument is the y variable.\n\nplot(malaria_data$total, malaria_data$positive)\n\n\n\n\n\n\n\nplot(malaria_data$month, malaria_data$prev)\n\n\n\n\n\n\n\n\nWe can also create boxplots by using boxplot() function. In this function we use the ~ operator, which tells R to use the values on the lefthand side of the ~ as the x variable and the righthand side of the ~ as the y variable. I think of ~ as “in terms of”, and for boxplots this means that your numerical variable will be on the x axis and the categorical variable will be on the y axis.\n\nboxplot(malaria_data$prev ~ malaria_data$month) \n\n\n\n\n\n\n\nboxplot(malaria_data$prev ~ malaria_data$location) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2: Explore the structure and summary of the mosquito_data dataset\n\n\n\n\nAre their any interesting patterns in individual variables/columns?\nAre there any relationships between variables/columns?"
  },
  {
    "objectID": "posts/data-vis/index.html#data-visualization-with-ggplot2",
    "href": "posts/data-vis/index.html#data-visualization-with-ggplot2",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "Data Visualization with ggplot2",
    "text": "Data Visualization with ggplot2\nBase R functions like hist() and barplot() are great for quickly exploring our data, but we may want to use more powerful visualization techniques when preparing outputs for scientific reports, presentations, and publications.\nThe ggplot2 package is a popular visualization package for R. It provides an easy-to-use interface for creating data visualizations. The ggplot2 package is based on the “grammar of graphics” and is a powerful way to create complex visualizations that are useful for creating scientific and publication-quality figures.\nThe “grammar of graphics” used in ggplot2 is a set of rules that are used to develop data visualizations using a layering approach. Layers are added using the ‘+’ operator.\n\nComponents of a ggplot\nThere are three main components of a ggplot: 1. The data: the dataset we want to visualize 2. The aesthetics: the visual properties from the data used in the plot 3. The geometries: the visual representations of the data (e.g., points, lines, bars)\n\nThe data\nAll ggplot2 plots require a data frame as input. Just running this line will produce a blank plot because we have stated which elements from the data we want to visualize or how we want to visualize them.\n\nggplot(data = malaria_data) \n\n\n\n\n\n\n\n\n\n\nThe aesthetics\nNext, we need to specify the visual properties of the plot that are determined by the data. The aesthetics are specified using the aes() function. The output should now produce a blank plot but with determined visual properties (e.g., axes labels).\n\nggplot(data = malaria_data, aes(x = total, y = positive)) \n\n\n\n\n\n\n\n\n\n\nThe geometries\nFinally, we need to specify the visual representation of the data. The geometries are specified using the geom_* function. There are many different types of geometries that can be used in ggplot2. We will use geom_point() in this example and we will append it to the previous plot using the + operator. The output should now produce a plot with the specified visual representation of the data.\n\nggplot(data = malaria_data, aes(x = total, y = positive)) + geom_point()\n\n\n\n\n\n\n\n\nHere are some examples of different geom functions:\n\nggplot(data = malaria_data, aes(x = prev)) +\n  geom_histogram(bins = 20)  # the \"bins\" argument specifies the number of bars\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = year)) +\n  geom_bar(fill = \"tomato\")  # the \"fill\" argument specifies the color of the bars\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = location, y = prev)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2)  # geom_jitter adds jittered points to the plot, and \n\n\n\n\n\n\n\n                            # the \"alpha\" argument specifies the transparency\n\nggplot(data = malaria_data, aes(x = location, y = prev)) +\n  geom_violin() +          # Violin plot are similar to boxplots, but illustrate \n  geom_jitter(alpha = 0.2) # the distribution of the data\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = total, y = positive)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")  # The smooth geom add a smoothed line to the plot, \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n                              # using the \"lm\" or other methods\n\n\n\n\nExtending the aesthetics\nAdditional visual properties, such as color, size, and shape, can be defined from our input data using the aes() function. Here is an example of adding color to a previous plot using the color aesthetic.\n\nggplot(data = malaria_data, aes(x = total, y = positive, color = location)) +\n  geom_point()\n\n\n\n\n\n\n\n\nNote that this is different then defining a color directly within the geom_point(), which would only apply a single color to all points.\n\nggplot(data = malaria_data, aes(x = total, y = positive)) +\n  geom_point(color = \"tomato\")\n\n\n\n\n\n\n\n\nWhen using the aes() function, the visual properties will be determined by a variable in the dataset. This allows us to visualize relationships between multiple variables at the same time.\n\nggplot(data = malaria_data, aes(x = prev, fill = ages)) +\n  geom_histogram(color = \"black\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2)\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = total, y = positive, color = location), alpha = 0.5) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = xcoord, y = ycoord, color = location)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3: Create ggplot2 visualizations of the ‘mosquito_data’ dataset\n\n\n\n\nAre their any interesting patterns in individual variables/columns?\nHow can we use the aes() function to view multiple variables in a single plot?\nAre there any additional geometries that may be useful for visualizing this dataset?"
  },
  {
    "objectID": "posts/data-vis/index.html#customizing-ggplot-graphics-for-presentation-and-communication",
    "href": "posts/data-vis/index.html#customizing-ggplot-graphics-for-presentation-and-communication",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "Customizing ggplot Graphics for Presentation and Communication",
    "text": "Customizing ggplot Graphics for Presentation and Communication\nIn this section, we will using additional features of ggplot2 to customize and develop high-quality plots that can used in scientific publications and presentations.\n\nThemes\nThere are many different themes that can be used in ggplot2. The “theme” function is used to specify the theme of the plot. There are many preset theme functions, and further custom themes can be created using the generic theme() function.\nTypically you will want to set the theme at the end of your plot.\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  theme_classic()\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  theme_bw()\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = ages)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nLabels\nLabels can be added to various components of a plot using the labs() function.\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = ages)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  labs(title = \"Malaria prevalence by location and age group\",\n       subtitle = \"Data from 2018 - 2020\",\n       x = \"Location\",\n       y = \"Prevalence\",\n       fill = \"Age group\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n### Custom color palettes\nThere are many different color palettes that can be used in ggplot2. The “scale_color” function is used to specify the color of the plot. There are many preset color palettes, and further custom color palettes can be created using the generic scale_color() function.\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nWe can also set our own colors.\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  scale_fill_manual(values = c(\"#C6E0FF\", \"#136F63\", \"#E0CA3C\", \"#F34213\", \"#3E2F5B\"))\n\n\n\n\n\n\n\n\nThe examples above show how to use colors for categorical variables, but we can also use custom color palettes for continuous variables.\n\nggplot(data = malaria_data, aes(x = total, y = positive, color = prev)) +\n  geom_point() +\n  scale_color_gradient(low = \"blue\", high = \"red\")\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = total, y = positive, color = prev)) +\n  geom_point() +\n  # use viridis package to create custom color palettes\n  scale_color_viridis_c(option = \"magma\")  \n\n\n\n\n\n\n\n\n\n\nFacets\nFacets are a powerful feature of ggplot2 that allow us to create multiple plots based on a single variable. This “small multiple” approach is another effective way to visualize relationships between mutliple variables.\nFacets also make use of the ~ operator.\n\nggplot(data = malaria_data, aes(x = total, y = positive, color = prev)) +\n  geom_point() +\n  scale_color_viridis_c(option = \"magma\") +\n  facet_wrap(~ location)\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  facet_wrap(~ ages) +\n  coord_flip() +  # flips the x and y axes\n  scale_fill_manual(\n    values = c(\"#C6E0FF\", \"#136F63\", \"#E0CA3C\", \"#F34213\", \"#3E2F5B\")) +\n  labs(title = \"Malaria prevalence by location and age group\",\n       subtitle = \"Data from 2018 - 2020\",\n       x = \"Location\",\n       y = \"Prevalence\",\n       fill = \"Age group\") +\ntheme_classic()\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = prev, fill = ages)) +\n  geom_histogram(bins = 10) +\n  scale_fill_viridis_d() +\n  facet_grid(year ~ .)\n\n\n\n\n\n\n\n\n\n\nExporting plots\nWe can export plots to a variety of formats using the ggsave() function. We can specify which plot to export by saving in an object and then calling the object in the ggsave() function, otherwise ggsave() will save the current/last plot. The width and height of the output image using the width and height can be set using the width and height arguments, and the resolution of the image using the dpi argument.\nThe file type can be set using the format argument, or by using a specific file extension. I recommend using informative names for the output file so that it is easily identifiable.\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  facet_wrap(~ ages) +\n  coord_flip() +  # flips the x and y axes\n  scale_fill_manual(values = c(\"#C6E0FF\", \"#136F63\", \"#E0CA3C\", \"#F34213\", \"#3E2F5B\")) +\n  labs(title = \"Malaria prevalence by location and age group\",\n       subtitle = \"Data from 2018 - 2020\",\n       x = \"Location\",\n       y = \"Prevalence\",\n       fill = \"Age group\") +\ntheme_classic()\n\nggsave(\"malaria-prevalence-age-boxplot.png\", width = 10, height = 6, dpi = 300)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4: Develop customized ggplot figures for the ‘mosquito_data’ dataset\n\n\n\n\nTest customs themes on your previous plots, consider looking for new packages with more themes\nApply custom color palettes to your plots, explore additional color palettes and packages\nUse facets to visualize relationships between multiple variables"
  },
  {
    "objectID": "posts/data-vis/index.html#final-challenges",
    "href": "posts/data-vis/index.html#final-challenges",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "Final Challenges",
    "text": "Final Challenges\nCHALLENGE 1: Create a figure showing how the Anopheles gambiae total counts vary each day and by location.\nCHALLENGE 2: Create a figure showing the hourly Anopheles gambiae total counts each hour."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A new virtual hackathon series for continual learning was developed by a collaborative effort between the Best Practices and Learning Committees.\nAims:\n\nTo create a continual support space that anyone can access\nTo learn how to code and how to code better\nTo upskill people from a distance, ultimately improving all our abilities in:\n\ndata handling, presentation and analysis\nexperimental design\nquantitative understanding\ncoding and problem solving\ntransmission modelling\n\nTo collaborate better and more equitably\n\nStructure:\n\nA theme is selected by the committee\nExperts sign up to support specific themed areas\nA participant submits a coding related problem for consideration within a specific theme\nExperts select 2 – 3 problems per session on the chosen theme, and prepare some solutions to present\n\nExperts will work with the Hackathon Committee for a half a day to consider the problems submitted and draft solutions. Once solutions are ready for a hackathon audience, they will present these during a 2-hour training session. Once the 2-hour training session concludes, experts will summarize the session, adapt solutions, and upload them to the shared space. A recording of the training will also be provided.\nIf you are an expert in the following topic areas, we need your help!\n\nData handling\nVisualisation \nDescriptive statistics\nPower calculations\nRegression statistics\nMechanistic modelling\nBayesian statistics\n\nWhy volunteer as an expert? The best way to learn is to teach. You’ll also have an opportunity to network and gain professional visibility within the AMMnet community.\nTo volunteer as an expert, please complete the form at the link here."
  },
  {
    "objectID": "about.html#a-virtual-hackathon-series-for-continual-learning",
    "href": "about.html#a-virtual-hackathon-series-for-continual-learning",
    "title": "About",
    "section": "",
    "text": "A new virtual hackathon series for continual learning was developed by a collaborative effort between the Best Practices and Learning Committees.\nAims:\n\nTo create a continual support space that anyone can access\nTo learn how to code and how to code better\nTo upskill people from a distance, ultimately improving all our abilities in:\n\ndata handling, presentation and analysis\nexperimental design\nquantitative understanding\ncoding and problem solving\ntransmission modelling\n\nTo collaborate better and more equitably\n\nStructure:\n\nA theme is selected by the committee\nExperts sign up to support specific themed areas\nA participant submits a coding related problem for consideration within a specific theme\nExperts select 2 – 3 problems per session on the chosen theme, and prepare some solutions to present\n\nExperts will work with the Hackathon Committee for a half a day to consider the problems submitted and draft solutions. Once solutions are ready for a hackathon audience, they will present these during a 2-hour training session. Once the 2-hour training session concludes, experts will summarize the session, adapt solutions, and upload them to the shared space. A recording of the training will also be provided.\nIf you are an expert in the following topic areas, we need your help!\n\nData handling\nVisualisation \nDescriptive statistics\nPower calculations\nRegression statistics\nMechanistic modelling\nBayesian statistics\n\nWhy volunteer as an expert? The best way to learn is to teach. You’ll also have an opportunity to network and gain professional visibility within the AMMnet community.\nTo volunteer as an expert, please complete the form at the link here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to AMMNet Hackathon Blog!",
    "section": "",
    "text": "Live Session 4: Mapping in R part 2\n\n\nRasters\n\n\n\nR\n\n\nSpatial data\n\n\nData cleaning\n\n\nData visualization\n\n\nGIS\n\n\nLive session\n\n\n\n\n\n\n\n\n\nApr 17, 2025\n\n\nPunam Amratia, Herieth Mboya, Jailos Lubinda, Adam Saddler, Paulina Dzianach, Ellie Sherrard-Smith, Justin Millar, Naomi Tedto\n\n\n\n\n\n\n\n\n\n\n\n\nLive Session 4: Introduction à la manipulation de données dans R\n\n\n\n\n\n\nR\n\n\nData cleaning\n\n\nData validation\n\n\nFrançais\n\n\nLive session\n\n\n\n\n\n\n\n\n\nApr 9, 2025\n\n\nCharlène Naomie Tedto Mfangnia, Didier Adjakidje, Lazaro Mwandigha, Christian Selinger, Ellie Sherrard-Smith, Justin Millar\n\n\n\n\n\n\n\n\n\n\n\n\nLive Session 3: Introduction to Mapping in R\n\n\n\n\n\n\nR\n\n\nSpatial data\n\n\nData cleaning\n\n\nData visualization\n\n\nGIS\n\n\nLive session\n\n\n\n\n\n\n\n\n\nFeb 20, 2025\n\n\nPunam Amratia, Jailos Lubinda, Adam Saddler, Paulina Dzianach, Justin Millar, Naomie Tedto\n\n\n\n\n\n\n\n\n\n\n\n\nLive Session 2: Introduction to Data Wrangling in R\n\n\n\n\n\n\nR\n\n\nData cleaning\n\n\nData validation\n\n\nLive session\n\n\n\n\n\n\n\n\n\nOct 14, 2024\n\n\nLazaro Mwandigha, Christian Selinger, Ellie Sherrard-Smith, Justin Millar\n\n\n\n\n\n\n\n\n\n\n\n\nLive Session 1: Introduction to Data Visualization in R\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\nggplot2\n\n\nLive session\n\n\n\n\n\n\n\n\n\nJul 19, 2024\n\n\nJustin Millar, Ellie Sherrard-Smith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/data-wrangle/index.html",
    "href": "posts/data-wrangle/index.html",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "",
    "text": "Take the survey!\n\n\n\nIf you are present for the live session on Monday October 21st, please click here to take the survey."
  },
  {
    "objectID": "posts/data-wrangle/index.html#introduction",
    "href": "posts/data-wrangle/index.html#introduction",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "Introduction",
    "text": "Introduction\n\nWhat do we mean with data wrangling?\nThe Cambridge dictionary gives several meanings to the verb wrangle:\n\nto argue with someone about something, especially for a long time\nto take care of, control, or move animals, especially large animals such as cows or horses (mainly American English)\nto move a person or thing somewhere, usually with difficulty or using force\nto take care of or deal with something, usually when this is difficult\n\n\n\nData Wrangling\nBy data wrangling, we mean here the process of checking and correcting quality and integrity of data relevant to malaria modeling, prior to any further analysis. This is also known as data validation.\nData validation involves checking various aspects of your dataset, such as missing values, data types, outliers, and adherence to specific rules or constraints.\nValidating our data helps maintain its quality and integrity, ensuring that any analyses or decisions made based on the data are robust and reliable.\n\n\nWhy Validate Data?\nEnsure Data Integrity: Validating data helps identify and rectify errors, ensuring the integrity of the dataset.\nImprove Analysis Accuracy: Clean and validated data leads to more accurate analysis and modeling results.\nCompliance and Standards: Data validation ensures that the data conforms to predefined rules, standards, or regulatory requirements.\nError Prevention: Early detection of errors can prevent downstream issues and save time in troubleshooting."
  },
  {
    "objectID": "posts/data-wrangle/index.html#getting-started",
    "href": "posts/data-wrangle/index.html#getting-started",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "Getting Started",
    "text": "Getting Started\nBefore you begin, you might want to create a new project in RStudio. This can be done by clicking on the “New Project” button in the upper right corner of the RStudio window. You can then name the project and choose a directory to save it in.\nNext, we will load the tidyverse package. This package provides a set of useful functions for data manipulation and visualization. We will use the ggplot2 package to create plots in the later section of this tutorial.\n\n# load packages\nlibrary(tidyverse)\n\nNext, let’s download the two example datasets we will use in this tutorial. These are available in the AMMnet Hackathon GitHub repository.\nI suggest creating a data folder inside your R project, then we can download the two example datasets so that they are saved to your computer.\n\n# Create a data folder\ndir.create(\"data\")\n\n# Download example data\nurl &lt;- \"https://raw.githubusercontent.com/AMMnet/AMMnet-Hackathon/main/02_data-wrangle/data/\"\n\ndownload.file(paste0(url, \"mockdata_cases1.csv\"), destfile = \"data/mockdata_cases1.csv\")\ndownload.file(paste0(url, \"mosq_mock1.csv\"), destfile = \"data/mosq_mock1.csv\")\n\n# Load example data\ndata_cases   &lt;- read_csv(\"data/mockdata_cases1.csv\")\nmosq_data  &lt;- read_csv(\"data/mosq_mock1.csv\")\n\nThe two datasets we will use are mockdata_cases1.csv and mosq_mock1.csv, which are mock example datasets that should be similar to malaria case surveillance and mosquito field collection data, respectively. In the following sections we will use the mockdata_cases1.csv and mosq_mock1.csv to introduce concepts of data cleaning and characterization in R."
  },
  {
    "objectID": "posts/data-wrangle/index.html#check-the-data-for-potential-errors",
    "href": "posts/data-wrangle/index.html#check-the-data-for-potential-errors",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "1. Check the data for potential errors",
    "text": "1. Check the data for potential errors\n\nPrevalence is a fraction defined in [0,1]\nNote: Prevalence of 0 or 1 while not statistically erroneous, need checking for accuracy.\nWhat observations have errors?\n\n# Erroneous values for prevalence\ndata_cases%&gt;%\n   dplyr::filter(prev &lt;= 0 | prev &gt;= 1)\n\n# A tibble: 3 × 10\n  location month  year ages  total positive xcoord ycoord    prev time_order_loc\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1 mordor       4  2018 15_a…    91       23  -20.0   30.5 25.3                 4\n2 neverwh…     2  2019 15_a…    22       -1  -20.8   29.6 -0.0455             14\n3 neverwh…     3  2018 unde…    25        0  -19.8   30.2  0                   3\n\n\nComment: We have two rows with nonsensical prev data 25.3and -0.455, and one row with zero prev at a given month.\n\n\nDefensive programming\nNote: The use of “::” enables us to call a function from a specific R package I have had instances where if “stats” base R package was called first, the filter function if not specified with the R package fails.\n\n# Erroneous values for prevalence\ndata_cases%&gt;%\n    stats::filter(prev &lt; 0 | prev &gt; 1) \n\n\n\nWe correct the two prevalence by re-calculating\nGood practice to leave the original data intact (advantage of R over Stata)\n\n# Update erroneous values for prevalence\ndata_prev &lt;- data_cases%&gt;%\n                       dplyr::mutate(prev_updated=positive/total)\n\nWe have a case erroneously reported with a negative value.\nWhat are your options?\n\nNever delete data\nQuery and have data management team make the necessary investigations and make a correction\n\n\ndata_prev%&gt;%\n    dplyr::filter(prev_updated &lt;= 0 | prev_updated &gt;= 1)\n\n# A tibble: 2 × 11\n  location month  year ages  total positive xcoord ycoord    prev time_order_loc\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1 neverwh…     2  2019 15_a…    22       -1  -20.8   29.6 -0.0455             14\n2 neverwh…     3  2018 unde…    25        0  -19.8   30.2  0                   3\n# ℹ 1 more variable: prev_updated &lt;dbl&gt;\n\n\nFor now (in order to proceed with this demo), we drop the problematic observation.\nWhy is this not working?\n\n# Filter erroneous values for prevalence, wrong way\ndata_use &lt;- data_prev%&gt;%\n              dplyr::filter (prev_updated &gt;= 0 | prev_updated &lt;= 1)\n\nWhy is this working?\n\n# Filter erroneous values for prevalence\ndata_use &lt;- data_prev%&gt;%\n             dplyr::filter (prev_updated &gt;= 0 )%&gt;%\n              dplyr::filter (prev_updated &lt;= 1)\n\ndata_use%&gt;%\n       dplyr::filter(prev_updated &lt;= 0 | prev_updated &gt;= 1)\n\n# A tibble: 1 × 11\n  location   month  year ages  total positive xcoord ycoord  prev time_order_loc\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1 neverwhere     3  2018 unde…    25        0  -19.8   30.2     0              3\n# ℹ 1 more variable: prev_updated &lt;dbl&gt;\n\n\n\n\nSchemas\nTo prevent nonsensical data appearing in your data, you should define a schema that comes along with your recorded data. A schema is a document that states rules for data types and values or ranges to be expected in a particular column of your data frame.\nE.g. for prevalence, we know that this should be a real number between zero and one.\nThe R package validate can be used to create a schema for your data frame:\n\n# Filter erroneous values for prevalence\nlibrary(validate)\nschema &lt;- validate::validator(prev &gt;= 0,\n                   prev &lt;= 1,\n                   positive &gt;= 0)\n\nout   &lt;- validate::confront(data_cases, schema)\nsummary(out)\n\n  name items passes fails nNA error warning             expression\n1   V1   514    513     1   0 FALSE   FALSE     prev - 0 &gt;= -1e-08\n2   V2   514    513     1   0 FALSE   FALSE      prev - 1 &lt;= 1e-08\n3   V3   514    513     1   0 FALSE   FALSE positive - 0 &gt;= -1e-08\n\n\nUsing the schema for the columns prev and positive, we could have readily detected the three problematic entries. For more details, you can have a look into the vignette of the validate package.\nNote: Next time when you receive data from your collaborators, you might want to ask them for the associated schema file (e.g. YAML format). Good luck!"
  },
  {
    "objectID": "posts/data-wrangle/index.html#look-at-summary-statistics",
    "href": "posts/data-wrangle/index.html#look-at-summary-statistics",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "2. Look at summary statistics",
    "text": "2. Look at summary statistics\n\nSummary stats by location (across all time points)\n\n# Summary statistics \n\ndata_use%&gt;%\n   dplyr::group_by(location)%&gt;%\n     dplyr::summarise(nobs=n(),\n                      mean_prev=mean(prev_updated),\n                      min_prev=min(prev_updated),\n                      max_prev=max(prev_updated))\n\n# A tibble: 5 × 5\n  location    nobs mean_prev min_prev max_prev\n  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mordor       105     0.314   0.158     0.488\n2 narnia       104     0.326   0.08      0.488\n3 neverwhere    95     0.301   0         0.486\n4 oz           104     0.255   0.0714    0.459\n5 wonderland   105     0.382   0.194     0.535\n\n\n\n\nSummary stats by location and year (across all time points)\nTable getting longer. Might be too cumbersome to add checks by month and age group Note: point of query - why just had 3 measurements in 2020?\n\n# Summary statistics by location\ndata_use%&gt;%\n  dplyr::group_by(location, year)%&gt;%\n  dplyr::summarise(nobs=n(),\n                   mean_prev=mean(prev_updated),\n                   min_prev=min(prev_updated),\n                   max_prev=max(prev_updated))\n\n# A tibble: 15 × 6\n# Groups:   location [5]\n   location    year  nobs mean_prev min_prev max_prev\n   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 mordor      2018    36     0.318   0.206     0.473\n 2 mordor      2019    36     0.313   0.170     0.451\n 3 mordor      2020    33     0.312   0.158     0.488\n 4 narnia      2018    36     0.340   0.138     0.449\n 5 narnia      2019    36     0.361   0.216     0.488\n 6 narnia      2020    32     0.270   0.08      0.483\n 7 neverwhere  2018    36     0.304   0         0.45 \n 8 neverwhere  2019    56     0.298   0.0370    0.486\n 9 neverwhere  2020     3     0.307   0.04      0.473\n10 oz          2018    35     0.252   0.0714    0.459\n11 oz          2019    36     0.254   0.0861    0.446\n12 oz          2020    33     0.260   0.112     0.405\n13 wonderland  2018    36     0.365   0.255     0.454\n14 wonderland  2019    36     0.388   0.194     0.535\n15 wonderland  2020    33     0.393   0.276     0.476\n\n\n\n\n\n\n\n\nChallenge 1: Explore the data_prev and data_use datasets\n\n\n\n\nCreate a table showing the number of data entries per age group and location for each of them!\nWhich age group and location have observations removed?\n\n\n\nSlightly more advanced. Use of lists (not scope of the course but there is a point here).\n\n# Summary statistics by location\ndata_use_list &lt;- data_use%&gt;%\n                  dplyr::group_split(location)\n\nOr use the purrr library:\n\n# Summary statistics by location, map summary function\nlibrary(purrr)\n\ndata_use_age_summary &lt;- purrr::map(.x=seq(length(data_use_list)),\n                                   .f=function(x){\n                                     data_use_list[[x]]%&gt;%\n                                       dplyr::group_by(location,year,ages)%&gt;%\n                                       dplyr::summarise(nobs=n(),\n                                                        mean_prev=mean(prev_updated),\n                                                        min_prev=min(prev_updated),\n                                                        max_prev=max(prev_updated)) \n                                     \n                                   })\n\n\n\nNow let’s focus on the first list object (mordor)\nWe know pregnant mothers, children &lt;5 are most vulnerable.\nOutput (ages) isn’t ordered as we would want (chronologically).\n\n# Summary statistics by location\n\ndata_mordor &lt;- data_use_age_summary[[1]]\n\ndata_mordor\n\n# A tibble: 9 × 7\n# Groups:   location, year [3]\n  location  year ages      nobs mean_prev min_prev max_prev\n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mordor    2018 15_above    12     0.270    0.206    0.369\n2 mordor    2018 5_to_14     12     0.335    0.219    0.427\n3 mordor    2018 under_5     12     0.348    0.259    0.473\n4 mordor    2019 15_above    12     0.266    0.170    0.377\n5 mordor    2019 5_to_14     12     0.278    0.176    0.390\n6 mordor    2019 under_5     12     0.394    0.315    0.451\n7 mordor    2020 15_above    11     0.255    0.158    0.333\n8 mordor    2020 5_to_14     11     0.352    0.258    0.488\n9 mordor    2020 under_5     11     0.330    0.190    0.422\n\n\n\n\nHow to proceed?\n\n# Summary statistics with age groups\nage_order &lt;- c(\"under_5\",\"5_to_14\",\"15_above\")\n\ndata_use_ordered &lt;- data_use\n\ndata_use_ordered$age_group &lt;- factor(data_use$ages, levels =age_order)\n\ndata_mordor_reordered &lt;- data_use_ordered%&gt;%\n                           dplyr::group_by(location, year,age_group)%&gt;%\n                            dplyr::summarise(nobs=n(),\n                                             mean_prev=mean(prev_updated),\n                                             min_prev=min(prev_updated),\n                                             max_prev=max(prev_updated))%&gt;%\n                                 dplyr::filter(location==\"mordor\")\n\nLet’s compare the two\n\n# Compare for Mordor\n\ndata_mordor\n\n# A tibble: 9 × 7\n# Groups:   location, year [3]\n  location  year ages      nobs mean_prev min_prev max_prev\n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mordor    2018 15_above    12     0.270    0.206    0.369\n2 mordor    2018 5_to_14     12     0.335    0.219    0.427\n3 mordor    2018 under_5     12     0.348    0.259    0.473\n4 mordor    2019 15_above    12     0.266    0.170    0.377\n5 mordor    2019 5_to_14     12     0.278    0.176    0.390\n6 mordor    2019 under_5     12     0.394    0.315    0.451\n7 mordor    2020 15_above    11     0.255    0.158    0.333\n8 mordor    2020 5_to_14     11     0.352    0.258    0.488\n9 mordor    2020 under_5     11     0.330    0.190    0.422\n\ndata_mordor_reordered\n\n# A tibble: 9 × 7\n# Groups:   location, year [3]\n  location  year age_group  nobs mean_prev min_prev max_prev\n  &lt;chr&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mordor    2018 under_5      12     0.348    0.259    0.473\n2 mordor    2018 5_to_14      12     0.335    0.219    0.427\n3 mordor    2018 15_above     12     0.270    0.206    0.369\n4 mordor    2019 under_5      12     0.394    0.315    0.451\n5 mordor    2019 5_to_14      12     0.278    0.176    0.390\n6 mordor    2019 15_above     12     0.266    0.170    0.377\n7 mordor    2020 under_5      11     0.330    0.190    0.422\n8 mordor    2020 5_to_14      11     0.352    0.258    0.488\n9 mordor    2020 15_above     11     0.255    0.158    0.333"
  },
  {
    "objectID": "posts/data-wrangle/index.html#use-of-graphs",
    "href": "posts/data-wrangle/index.html#use-of-graphs",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "3. Use of graphs",
    "text": "3. Use of graphs\n\nWe need to assess the evolution of prevalence for all regions by month\n\n#Plotting evolution over time\nevolution_plot &lt;- ggplot2::ggplot(data=data_use_ordered,\n                                  mapping=aes(x=month,\n                                              y=prev_updated,\n                                              group=location,\n                                              colour=location))+\n                        ggplot2::geom_line(lwd=1.1)+\n                           ggplot2::facet_wrap(~year)+ \n                            ggplot2::theme_bw()+\n                             ggplot2::xlab(\"Month of the Year\")+\n                               ggplot2::ylab(\"Prevalence\")+\n                                ggplot2::scale_x_discrete(limits=factor(1:12),\n                                                          labels=c(\"J\",\"F\",\"M\",\n                                                                   \"A\",\"M\",\"J\",\n                                                                   \"J\",\"A\",\"S\",\n                                                                   \"O\",\"N\",\"D\"))+\n                                   ggplot2::scale_y_continuous(breaks=seq(from=0,\n                                                                          to=0.7,\n                                                                          by=0.1))\n\nevolution_plot\n\n\n\n\n\n\n\n\nObservation: Prevalence graph with vertical lines per month and year, means we have several subgroups for prevalence data, we plot facets for levels of age_group\n\n#Plotting evolution over time, fix 1\nevolution_plot_ages &lt;- ggplot2::ggplot(data=data_use_ordered,\n                                  mapping=aes(x=month,\n                                              y=prev_updated,\n                                              group=location,\n                                              colour=location))+\n  ggplot2::geom_line(lwd=1.1)+\n  ggplot2::facet_wrap(age_group~year)+ \n  ggplot2::theme_bw()+\n  ggplot2::xlab(\"Month of the Year\")+\n  ggplot2::ylab(\"Prevalence\")+\n  ggplot2::scale_x_discrete(limits=factor(1:12),\n                            labels=c(\"J\",\"F\",\"M\",\n                                     \"A\",\"M\",\"J\",\n                                     \"J\",\"A\",\"S\",\n                                     \"O\",\"N\",\"D\"))+\n  ggplot2::scale_y_continuous(breaks=seq(from=0,\n                                         to=0.7,\n                                         by=0.1))\n\nevolution_plot_ages\n\n\n\n\n\n\n\n\nObservation: Some improvements, but we still have vertical lines, maybe we have other group variables. Let’s only look at those rows that have more than one entry per location, month, year, age_group\n\n#Plotting evolution over time, fix 2\n\ndata_use_ordered%&gt;%\n  group_by(location,month,year,age_group)%&gt;%\n  tally()%&gt;%\n  filter(n&gt;1)%&gt;%\n  left_join(data_use_ordered)\n\n# A tibble: 48 × 13\n# Groups:   location, month, year [8]\n   location   month  year age_group     n ages     total positive xcoord ycoord\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 neverwhere     6  2019 under_5       2 under_5     24        4  -20.6   30.7\n 2 neverwhere     6  2019 under_5       2 under_5     26        1  -20.5   30.7\n 3 neverwhere     6  2019 5_to_14       2 5_to_14     27        5  -19.7   30.0\n 4 neverwhere     6  2019 5_to_14       2 5_to_14     27        8  -19.3   30.2\n 5 neverwhere     6  2019 15_above      2 15_above    70       31  -19.4   29.4\n 6 neverwhere     6  2019 15_above      2 15_above    74       27  -19.2   29.2\n 7 neverwhere     7  2019 under_5       2 under_5     25        5  -20.0   29.1\n 8 neverwhere     7  2019 under_5       2 under_5     26        4  -20.7   28.6\n 9 neverwhere     7  2019 5_to_14       2 5_to_14     27        7  -18.8   29.3\n10 neverwhere     7  2019 5_to_14       2 5_to_14     23        6  -20.4   29.8\n# ℹ 38 more rows\n# ℹ 3 more variables: prev &lt;dbl&gt;, time_order_loc &lt;dbl&gt;, prev_updated &lt;dbl&gt;\n\n\nObservation: OK, we see that within one location there are several prevalence data points, they differ by the xcoord and ycoord. In order to plot by location, we could average across xcoord and ycoord witin each location; maybe those are duplicated recordings, since xcoord and ycoord are very close?\n\n#Plotting evolution over time, fix 3\n\ndata_use_ordered%&gt;%\n  group_by(location,month,year,age_group)%&gt;%\n  summarize(prev_updated_mean=mean(prev_updated),\n            prev_updated_min=min(prev_updated),\n            prev_updated_max=max(prev_updated))%&gt;%\n  ggplot2::ggplot(mapping=aes(x=month,\n                              y=prev_updated_mean,\n                              file=location,\n                              group=location,\n                              colour=location))+\n  ggplot2::geom_line(lwd=1.1)+\n  ggplot2::facet_wrap(age_group~year)+ \n  ggplot2::theme_bw()+\n  ggplot2::xlab(\"Month of the Year\")+\n  ggplot2::ylab(\"Prevalence\")+\n  ggplot2::scale_x_discrete(limits=factor(1:12),\n                            labels=c(\"J\",\"F\",\"M\",\n                                     \"A\",\"M\",\"J\",\n                                     \"J\",\"A\",\"S\",\n                                     \"O\",\"N\",\"D\"))+\n  ggplot2::scale_y_continuous(breaks=seq(from=0,\n                                         to=0.7,\n                                         by=0.1))\n\n\n\n\n\n\n\n\nObservation: Prevalence widely variable throughout they year across the locations on average, wonderland affected by high prevalence while oz has the lowest prevalence"
  },
  {
    "objectID": "posts/data-wrangle/index.html#the-mosquito-data-set",
    "href": "posts/data-wrangle/index.html#the-mosquito-data-set",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "The mosquito data set",
    "text": "The mosquito data set\nLet’s take a look at the mosq_datadataset.\nWe check the sanity of this data set by displaying a table of recorded values per column:\n\nmosq_data %&gt;%\n  map( function(x) table(x) )\n\n$session\nx\n 1  2 \n52 52 \n\n$Village\nx\nnaernia  narnia \n      2     102 \n\n$Compound.ID\nx\n 1  2  3  4 \n26 26 26 26 \n\n$Method\nx\nALC HLC \n  1 103 \n\n$Location\nx\n Indoor Outdoor \n     52      52 \n\n$hour\nx\n01h-02h 02h-03h 03h-04h 04h-05h 05h-06h 06h-07h 07h-08h 19h-20h 20h-21h 21h-22h \n      8       8       8       8       8       8       8       8       8       8 \n22h-23h 23h-24h 24h-01h \n      8       8       8 \n\n$ag.Male\nx\n 0  3  4  5  6  7 14 16 20 22 27 35 \n93  1  1  1  1  1  1  1  1  1  1  1 \n\n$Ag.unfed\nx\n 0  1  2  3  4  5  6  7  8 10 20 \n57 13  7  8  4  4  2  4  2  1  2 \n\n$Ag.halffed\nx\n 0  3  4  5  8  9 \n92  3  3  3  1  2 \n\n$Ag.fed\nx\n 0  1  3  5 \n88  7  3  6 \n\n$Ag.grsgr\nx\n 0  1  2  3  4  6  8 12 17 20 23 27 35 37 \n70 13  6  1  2  1  3  2  1  1  1  1  1  1 \n\n$tot.gamb\nx\n 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 23 \n40 10 10  9  8  6  3  2  1  1  5  2  1  2  2  2 \n\n$Culex.male\nx\n  0 \n104 \n\n$Culex.female\nx\n 0  1  2 \n94  9  1 \n\n$Mansonia.male\nx\n  0   1 \n103   1 \n\n$Mansonia.female\nx\n 0  1  2 \n90 11  3 \n\n$Aedes.male\nx\n  0 \n104 \n\n$Aedes.female\nx\n 0  1  2 \n98  5  1 \n\n\nLooks like we have some typos in the names for Method and Village.\n\n\n\n\n\n\nChallenge 2: Using schemas for the mosquito data set\n\n\n\n\nCreate a schema that provides rules for the strings (i.e. words) to be expected in the columns Method and Village.\nUse the syntax from here\n\n\n\n\nschema &lt;- validate::validator(Method%in%c(\"HLC\"),\n                              Village%in%c(\"narnia\"))\n\nout   &lt;- validate::confront(mosq_data, schema)\nsummary(out)\n\n  name items passes fails nNA error warning                expression\n1   V1   104    103     1   0 FALSE   FALSE     Method %vin% c(\"HLC\")\n2   V2   104    102     2   0 FALSE   FALSE Village %vin% c(\"narnia\")\n\n\nThe columns Village and Method seem to have some data entry errors. We need to correct for that.\n\nmosq_data&lt;-mosq_data%&gt;%\n  mutate(Method=ifelse(Method==\"ALC\",\"HLC\",Method),\n         Village=ifelse(Village==\"naernia\",\"narnia\",Village))\n\nIt looks like the several columns concern Anopheles Gambiae population sizes. Let’s change the column names using rename from the tidyverse package.\n\nmosq_data%&gt;%\n  rename(\"AnophelesGambiae.male\"=\"ag.Male\",\n         \"AnophelesGambiae.unfed\"=\"Ag.unfed\",\n         \"AnophelesGambiae.halffed\"=\"Ag.halffed\",\n         \"AnophelesGambiae.fed\"=\"Ag.fed\",\n         \"AnophelesGambiae.gravid\"=\"Ag.grsgr\")-&gt;mosq_data\n\nSeems like the tot.gamb should count the the total number of Anopheles Gambiae populations. Let’s check:\n\nmosq_data%&gt;%\n  mutate(AnophelesGambiae_total=AnophelesGambiae.male+AnophelesGambiae.unfed+AnophelesGambiae.halffed+AnophelesGambiae.fed+AnophelesGambiae.gravid)-&gt;mosq_data\n\nmosq_data%&gt;%\n  filter(AnophelesGambiae_total!=tot.gamb)%&gt;%select(AnophelesGambiae_total,tot.gamb)\n\n# A tibble: 11 × 2\n   AnophelesGambiae_total tot.gamb\n                    &lt;dbl&gt;    &lt;dbl&gt;\n 1                     12        0\n 2                     16        2\n 3                      0        6\n 4                     24        8\n 5                     24        1\n 6                     74       12\n 7                     54        3\n 8                     70        1\n 9                     34        2\n10                     40        2\n11                     46        0\n\n\nOK, so 11 out of 104 rows have this discrepancy. Let’s keep rather Anopheles.total, since it was calculated from the data.\nSince the status of the Anopheles is mutually exclusive in the HLC data, we can draw a stacked bar chart, with the bar color defined by the status. To produce such a graph efficiently in ggplot2, we need to pivot the table.\nHere in particular we want to switch from a wide format to a long format table in order to obtain a column describing the status of the Anopheles mosquitoes. We will use in particular the names_separgument of the pivot_longer function to separate e.g. the column name AnophelesGambiae.male and use maleas level in a new column called status. The same goes for other column names.\nSetting the grouping variable to session, Village, Compound.ID, Method, Location, hour, AnophelesGambiae_total will help to keep those variables in the long format table.\n\nmosq_data%&gt;%\n  group_by(session,Village,Compound.ID,Method,Location,hour,AnophelesGambiae_total)%&gt;%\n  select(contains(\"AnophelesGambiae.\"))%&gt;%\n  pivot_longer(cols=contains(\"AnophelesGambiae.\"),names_sep=\"AnophelesGambiae.\",names_to=c(NA,\"status\"),values_to = \"AnophelesGambiae\")-&gt;mosq_data_gamb_wide\n\nmosq_data_gamb_wide%&gt;%\n  ggplot()+\n  geom_bar(aes(x=hour,y=AnophelesGambiae,fill=status),position=\"stack\",stat=\"identity\")+\n  scale_x_discrete(guide = guide_axis(angle = 60))\n\n\n\n\n\n\n\n\nObservation: We had several values for Compound.ID. The geom_bar geometry is automatically adding them up in the graph. We can use facet_wrapto see those strata:\n\nmosq_data_gamb_wide%&gt;%\n  ggplot()+\n  geom_bar(aes(x=hour,y=AnophelesGambiae,fill=status),position=\"stack\",stat=\"identity\")+\n  scale_x_discrete(guide = guide_axis(angle = 60))+\n  facet_wrap(~Compound.ID)\n\n\n\n\n\n\n\n\nOn we can also use our variable Anopheles_total and plot is as a line graph on top of the bar graph:\n\nmosq_data_gamb_wide%&gt;%\n  mutate(grouping=paste0(Compound.ID,Location,session))%&gt;%\n  ggplot()+\n  geom_bar(aes(x=hour,y=AnophelesGambiae,fill=status),position=\"stack\",stat=\"identity\")+\n  geom_line(aes(x=hour,y=AnophelesGambiae_total,group=grouping))+\n  scale_x_discrete(guide = guide_axis(angle = 60))+\n  facet_wrap(~Compound.ID+session+Location)"
  },
  {
    "objectID": "posts/mapping-r/index.html",
    "href": "posts/mapping-r/index.html",
    "title": "Live Session 3: Introduction to Mapping in R",
    "section": "",
    "text": "Take the survey!\n\n\n\nIf you are present for the live session on Thursday March 6th 2025, please click here to take the survey."
  },
  {
    "objectID": "posts/mapping-r/index.html#overview",
    "href": "posts/mapping-r/index.html#overview",
    "title": "Live Session 3: Introduction to Mapping in R",
    "section": "Overview",
    "text": "Overview\nThis live session aims to introduce participants to mapping in R, with a focus on working with vector data. We will start with a brief introduction to key GIS concepts before diving into hands-on mapping techniques using R. Participants will learn how to read, manipulate, and visualize spatial data using the sf, tidyverse, and tmap packages. By the end of the session, attendees will be able to load vector data, perform basic spatial operations, and create effective maps in R.\n\nObjectives of tutorial\n\nIntroduction to GIS concepts\n\nTypes of Spatial Data – Overview of vector (point, line, polygon) and raster data.\nCoordinate reference systems\n\nVector data\n\nIntroduction to sp and sf packages\nImporting shapefiles into R (Spatial points and polygons)\nJoining data to shapefiles\nMaking buffes\nWriting shapefiles out in R\n\nCreating publication quality maps\n\nUsing ggplot2\nUsing tmap\n\nInteractive maps using tmap"
  },
  {
    "objectID": "posts/mapping-r/index.html#gis-concepts",
    "href": "posts/mapping-r/index.html#gis-concepts",
    "title": "Live Session 3: Introduction to Mapping in R",
    "section": "GIS concepts",
    "text": "GIS concepts\n\nBasic Definitions\nSpatial data is a term we use to describe any data which includes information on the locations and shapes of geographic features and the relationships between them, usually stored as coordinates and topology. Spatial data can be used to describe, for example:\n\nLocations of points of interest (e.g. water bodies, households, health facilities)\nDiscrete events and their locations (e.g. malaria cases, crime spots, floods)\nContinuous surfaces (e.g. elevation, temperature, rainfall)\nAreas with counts or rates aggregated from individual-level data (e.g. disease prevalence rates, population census, etc.)\n\nSpatial data can often be categorised into vector data or raster data.\nVector data is a representation of the world using points, lines, and polygons. This data is created by digitizing the base data, and it stores information in x, y coordinates. Vector data is one of two primary types of spatial data in geographic information systems (GIS) – the other being raster data. Example: sea, lake, land parcels, countries and their administrative regions, etc.\n\n\n\n\n\n\n\n\n\nsource:Colin Williams (NEON); captured from Earth Data Science course\n\nPoints: Each individual point is defined by a single x, y coordinate. There can be many points in a vector point file. Examples of point data include: Health facility locations, household clusters.\nLines: Lines are composed of many (at least 2) vertices, or points, that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each “bend” in the road or stream represents a vertex that has defined x, y location.\nPolygons: A polygon consists of 3 or more vertices that are connected and “closed”. Thus the outlines of plot boundaries, lakes, oceans, and states or countries are often represented by polygons. Occasionally, a polygon can have a hole in the middle of it (like a doughnut), this is something to be aware of but not an issue you will deal with in this tutorial.\n\n\n\n\n\n\nTip\n\n\n\nOccasionally, a polygon can have a hole in the middle of it (like a doughnut), or might gave gaps between each other, this is something to be aware of but not an issue you will deal with in this tutorial. As a tip, we would first recommend reaching out to the owner of the shapefile to clean prior to fixing it yourself.\n\n\n\nRaster data consists of a matrix of cells (or pixels) organized into rows and columns (or a grid) where each cell contains a value representing information, such as temperature. Rasters are digital aerial photographs, imagery from satellites, digital pictures, or even scanned maps.\n\n\n\n\n\n\n\n\n\n(source: National Ecological Observatory Network (NEON))\nBoth types of data structures can either hold discrete or continuous information and can be layered upon each other and interact. An example of discrete data would be data defining the spatial extent of water bodies, built up areas, forests, or locations of health facilities. In contrast, continuous spatial data would be used to record quantities, such as elevation, temperature, population, etc.\n\n\n\n\n\n\n\n\n\nsource:https://link.springer.com/chapter/10.1007/978-3-030-01680-7_1\nAnalysis of spatial data supports explanation of certain phenomena and solving problems which are influenced by components which vary in space, or both space and time. Two important properties of geospatial data are, for example, that:\n\nObservations in geographic space tend to be dependent on other geographic factors. For example, variations in disease rates within a country may be due to variations in population density, age, socioeconomic factors, disease vector densities, temperature or a combination of these and other factors.\nObservations that are geographically close to each other tend to be more alike than those which are further apart.\nIts important to have layers of spatial data all be on the same coordinate reference system.\n\n\n\nCoordinate Reference System\n\n\n\n\n\n\n\n\n\nsource: http://ayresriverblog.com\nLet’s address the elephant in the room and agree that the Earth is round! ;)\nWhen dealing with spatial data we often are dealing with topology which allows us to describe the information geographically. A coordinate reference system (CRS) refers to the way in which this spatial data that represent the earth’s surface (which is round / 3 dimensional) are flattened so that you can “Draw” them on a 2-dimensional surface (e.g. your computer screens or a piece of paper). There are many ways which we can ‘flatten’ the earth, each using a different (sometimes) mathematical approach to performing the flattening resulting in different coordinate system grids: most commonly the Geographic Coordinate System (GCS) and Projected Coordinate System (PCS). These approaches to flattening the data are specifically designed to optimize the accuracy of the data in terms of length and area and normally are stored in the CRS part of the shapefile in R. This is especially important when dealing with countries that are further away from the equator. Here is a fun little video to showcase how coordinate reference systems can really change/distort the reality of the actual geography!\n\n\n\n\nGeographic Coordinate System\nA geographic coordinate system is a reference system used to pin point locations onto the surface of the Earth. The locations in this system are defined by two values: latitude and longitude. These values are measured as angular distance from two planes which cross the center of the Earth - the plane defined by the Equator (for measuring the latitude coordinate), and the plane defined by the Prime Meridian (for measuring the longitude coordinate).\nA Geographic Coordinate System (GCS) is defined by an ellipsoid, geoid and datum.\nThe use of the ellipsoid in GCS comes from the fact that the Earth is not a perfect sphere. In particular, the Earth’s equatorial axis is around 21 km longer than the prime meridian axis. The geoid represents the heterogeneity of the Earth’s surface resulting from the variations in the gravitational pull. The datum represents the choice of alignment of ellipsoid (or sphere) and the geoid, to complete the ensemble of the GCS.\nIt is important to know which GCS is associated with a given spatial file, because changing the choice of the GCS (i.e. the choice of ellipsoid, geoid or datum) can change the values of the coordinates measured for the same locations. Therefore, if our GCS is not properly defined, it could lead to misplacement of point locations on the map.\n\n\nProjected Coordinate System\nThe Projected Coordinate System (PCS) is used to represent the Earth’s coordinates on a flat (map) surface. The PCS is represented by a grid, with locations defined by the Cartesian coordinates (with x and y axis). In order to transform coordinates from the GCS to PCS, mathematical transformations need to be applied, hereby referred to as projections.\n\n\n\n\n\n\n\n\n\nsource: CA Furuti"
  },
  {
    "objectID": "posts/mapping-r/index.html#vector-data-in-r-using-sf",
    "href": "posts/mapping-r/index.html#vector-data-in-r-using-sf",
    "title": "Live Session 3: Introduction to Mapping in R",
    "section": "Vector data in R using sf",
    "text": "Vector data in R using sf\nVector data are composed of discrete geometric locations (x,y values) known as vertices that define the “shape” of the spatial object. The organization of the vertices determines the type of vector that you are working with: point, line or polygon. Typically vector data is stored in shapefiles and within R can be called in using several different packages, most common being sp and sf. For the purpose of this material we will focus of teaching you the package sf as it is intended to succeed and replace R packages sp, rgeos and the vector parts of rgdal packages. It also connects nicely to tidyverse learnt in previous hackathons :)\nwe’ll start off by loading in some key packages we’ll be using during this tutorial section\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(ggspatial)\nlibrary(ggrepel)\nlibrary(tidyverse)\nlibrary(malariaAtlas)\n\n\n\n\n\n\n\nTransition from sp to sf packages\n\n\n\nThe sp package (spatial) provides classes and methods for spatial (vector) data; the classes document where the spatial location information resides, for 2D or 3D data. Utility functions are provided, e.g. for plotting data as maps, spatial selection, as well as methods for retrieving coordinates, for subsetting, print, summary, etc.\nThe sf package (simple features = points, lines, polygons and their respective ‘multi’ versions) is the new kid on the block with further functions to work with simple features, a standardized way to encode spatial vector data. It binds to the packages ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations.\nFor the time being, it is best to know and use both the sp and the sf packages, as discussed in this post. However, we focus on the sf package. for the following reasons:\n\nsf ensures fast reading and writing of data\nsf provides enhanced plotting performance\nsf objects can be treated as data frames in most operations\nsf functions can be combined using %&gt;% operator and works well with the tidyverse collection of R packages.\nsf function names are relatively consistent and intuitive (all begin with st_) However, in some cases we need to transform sf objects to sp objects or vice versa. In that case, a simple transformation to the desired class is necessary:\n\nTo sp object &lt;- as(object, Class = \"Spatial\")\nTo sf object_sf = st_as_sf(object_sp, \"sf\")\nA word of advice: be flexible in the usage of sf and sp. Sometimes it may be hard to explain why functions work for one data type and do not for the other. But since transformation is quite easy, time is better spent on analyzing your data than on wondering why operations do not work.\n\n\n\nShapefiles: Points, Lines, and Polygons\nGeospatial data in vector format are often stored in a shapefile format. Because the structure of points, lines, and polygons are different, each individual shapefile can only contain one vector type (all points, all lines or all polygons). You will not find a mixture of point, line and polygon objects in a single shapefile.\nObjects stored in a shapefile often have a set of associated attributes that describe the data. For example, a line shapefile that contains the locations of streams, might contain the associated stream name, stream “order” and other information about each stream line object.\n\n\n\n\n\n\n\n\n\nShapefiles often have many file types associated to it. Each of these files continains valuable information. The most important file is the .shp file which is the main file that stores the feature geometries. .shx is an index file to connect the feature geometry and .dbf is a dBASE file that stores all the attribute information (like a table of information). These three files are required for plotting vector data. Often times you might also get additional useful files such as the .prj which stores the coordination system information.\n\n\nImporting shapefiles into R\nShapefiles can be called in to R using the function st_read(). Similarly to read_csv() we include a filepath to a shapefile. In this instance we would load the part of the shapefile that ends with .shp\n\nPolygons\n\ntz_admin1 &lt;- st_read(\"data/shapefiles/TZ_admin1.shp\")\n\nReading layer `TZ_admin1' from data source \n  `C:\\Users\\jmillar\\OneDrive - PATH\\Documents\\github_new\\ammnet-hackathon\\posts\\mapping-r\\data\\shapefiles\\TZ_admin1.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 36 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29.3414 ymin: -11.7612 xmax: 40.4432 ymax: -0.9844\nGeodetic CRS:  WGS 84\n\n\nYou’ll notice that when you load the shapefile in, there will be a set of information explaining the features of the shapefile. The first sentence shows that you have loaded a ESRI shapefile, it contains 36 features (which are polygons in this case) and 17 columns of information stored as a data tabll. It mentions also there is a spatial extent (called bounding box) and the coordinate reference system (CRS).\nYou can also get this information when you simply call the sf object\n\ntz_admin1\n\n\n\n\n\n\n\n\n\n\nThe snapshot above would likely be similar to what you would see in your console. You’ll notice that it would display in your environment as a table, but the difference between this and any other table is that it includes additional information in the metadata indicating its spatial features. In this case it is important to read in and check the metadata for the type of spatial information you have loaded.\n\n\nPoint data\nNow that we have in some polygon information, let’s try load a different type of vector data - points. Often times in public health space we would receive data that might represent points on a map (e.g. cases at a health facility, bed nets used in a village). This style of data would include GPS coordinates of longitude and latitude to help us know geographically where they are located.\nWe are going to pull some prepared MIS data into R. For alternatives I would suggest checking out the malaria prevalence data using the malariaAtlas package in R.\n\ntz_pr &lt;- read_csv(\"data/pfpr-mis-tanzania-clean.csv\")\n\nRows: 436 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (9): country, country_id, continent, site_name, rural_urban, method, r...\ndbl  (15): id, site_id, latitude, longitude, month_start, year_start, month_...\nlgl   (2): pcr_type, source_id3\ndate  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Make the gps data into a point simple feature\ntz_pr_points &lt;- st_as_sf(tz_pr, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\nYou’ll notice that when you brought in the data it does not display the spatial information and came in as a table, so then we may want to convert these into spatial information using the st_as_sf() function. The data we’ve pulled is all the public available data from the DHS program for Tanzania, we’ve pre-processed this specifically for prevalence information only. It includes the latitude and longitude information. When converting the table to a simple feature it must have complete information (i.e. no missing coordinates), we would recommend if you get data from elsewhere make sure to check there is no missingness. Note that we set the projection for the spatial object using the crs command; crs 4326 is the standard WGS 84 CRS.\n\n\n\nPlotting the shapefiles in ggplot2\nNow that we have some shapefiles let’s try make a plot of them both. We can use the geom called geom_sf() with the ggplot funtions to make a plot. This was explored in the previous hackathon on data visualization\n\ntz_region &lt;- ggplot(tz_admin1)+\n  geom_sf()+\n  theme_bw()+\n  labs(title = \"Tanzania Regions\")\n\nWe now have a plot of the map of Tanzania and its regions. Like other ggplots we can continue to add points to this map.\n\nggplot()+\n  geom_sf(tz_admin1, mapping = aes(geometry = geometry))+\n  geom_point(tz_pr, mapping = aes(x = longitude, y = latitude, color = pf_pr))+\n  scale_color_distiller(palette = \"Spectral\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nNote that when you move the data into the geom_sf() it must include a mapping= aes() argument. If you don’t have any specific information you want to display and just want the map, you can say you would like it to map to the geometry .\n\n\n\n\n\n\nChallenge 1: plot the spatial points\n\n\n\n\nTry make the same map as above but using the tz_pr_points spatial dataset instead, what geom would you use?\nCan you change the shape, transparency and color of the points?\nCan you make the size vary by examined?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot()+\n  geom_sf(tz_admin1, mapping = aes(geometry = geometry))+\n  #notice we added size inside the aes(), shape and alpha control icon and transparency respectively\n  geom_sf(tz_pr_points, mapping = aes(color = pf_pr, size = examined), shape = 15, alpha = 0.7)+\n  #we added the scale_size() to control the size of points\n  scale_size(range = c(0.5, 4))+\n  #there are many palettes available, try see which one works for you\n  scale_color_distiller(palette = \"RdBu\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nJoining data to shapefiles\nWe often want to join data to shapefiles to enable the creation of maps and to analyse spatial data. You can use the join functions in tidysverse to join sf data to tables. Just note that the sf data must go first to automatically be recognised as sf. Else you would need to reset it using st_as_sf() .\nWe’re gonna bring in population data by Region to join to the regional shapefile. However, before we can join the data, the names for the regions don’t match. In order for a correct join the key column must be exactly the same in both datasets. So we must make a key column called name_1 to match the shapefile. We’ll be using the tricks we learnt previous hackathons like the function str_to_sentence() from the stringr package of tidyverse.\n\ntz_pop_adm1 = read_csv(\"data/tza_admpop_adm1_2020_v2.csv\") %&gt;% \n  #Change the characters from upper to title style\n  mutate(name_1 = str_to_title(ADM1_EN)) %&gt;% \n  #Some names completely don't match so manually change them\n  mutate(name_1 = case_when(name_1 == \"Dar Es Salaam\" ~ \"Dar-es-salaam\",\n                            name_1 == \"Pemba North\" ~ \"Kaskazini Pemba\",\n                            name_1 == \"Pemba South\" ~ \"Kusini Pemba\",\n                            name_1 == \"Zanzibar North\" ~ \"Kaskazini Unguja\",\n                            name_1 == \"Zanzibar Central/South\" ~ \"Kusini Unguja\",\n                            name_1 == \"Zanzibar Urban/West\" ~ \"Mjini Magharibi\",\n                            name_1 == \"Coast\" ~ \"Pwani\",\n                            TRUE ~ as.character(name_1) #be sure to include this or it will turn name_1 NA\n                            ))\n\n#check if names in the columns match\ntable(tz_admin1$name_1 %in% tz_pop_adm1$name_1)\n\n\nFALSE  TRUE \n    5    31 \n\n\n\n\n\n\n\n\nTip\n\n\n\nYou’ll notice 5 regions haven’t matched the population table. If you open the shapefile you’ll find the column type_1 and notice that the shapefile includes polygon shapes for water bodies. If you’d like to exlcude these you can run a filter and make a tz_region_only shapefile by running:\ntz_region_only &lt;- filter(tz_admin1, type_1 == \"Region\")\n\n\nNow we can join the data to the current shapefile. Don’t worry that they currently don’t match! we have a plan ;)\n\ntz_pop_admin1 &lt;- tz_admin1 %&gt;% left_join(tz_pop_adm1, by = \"name_1\") \n\nggplot(tz_pop_admin1)+\n  geom_sf(mapping = aes(fill = T_TL))+\n  #use na.value to make the lakes appear as lightblue\n  scale_fill_viridis_c(option = \"B\", na.value = \"lightblue\", trans = 'sqrt')+\n  theme_bw()+\n  labs(fill = \"Total Population\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2: plot binned information\n\n\n\n\nCan you make the plot above using binned categories for population?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(tz_pop_admin1)+\n  geom_sf(mapping = aes(fill = T_TL))+\n  #use na.value to make the lakes appear as lightblue\n  scale_fill_viridis_b(option = \"B\", na.value = \"lightblue\", trans = 'sqrt', breaks = c(0,100000,1000000, 2000000, 3000000, 4000000, 8000000))+\n  theme_bw()+\n  labs(fill = \"Total Population\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting polygon names for point data\ncoming back to our prevalence data, often times for decision making we might want data to be summarised to regional forms. Currently the point prevalence doesn’t have any information about which region they belong to. So we can using the st_join . By default this spatial join will look for where the geometries of the spatial data’s intersect using st_interesect . It also by default uses a left join which is why the points come first.\n\nsf_use_s2(FALSE)\ntz_pr_point_region &lt;- st_join(tz_pr_points, tz_admin1)\n\nfrom this we can try and calculate the mean prevalence in each region\n\ntz_region_map &lt;- tz_pr_point_region %&gt;% \n  ungroup() %&gt;% #run this to remove any previous groupings that occured\n  group_by(name_1) %&gt;% \n  summarise(mean_pr = mean(pf_pr, na.rm=TRUE)) %&gt;% \n  st_drop_geometry() %&gt;% \n   #we put a \".\" to indicate where the data we've been working with goes for left join\n  left_join(tz_admin1, .) %&gt;% \n  ggplot()+\n  geom_sf(mapping = aes(fill = mean_pr))+\n  scale_fill_distiller(palette = \"Spectral\", na.value = \"lightblue\")+\n  labs(fill = \"Mean PfPR 0-5 years\", title = \"Tanzania Regions\", subtitle = \"MIS 2017\")+\n  theme_bw()\n\ntz_region_map\n\n\n\n\n\n\n\n\n\n\nBuffers\nSometimes you might want to create a buffer around the spatial information you have. Let’s try this on the point data. we’ll make a 20km buffer around each spatial point. We can use the st_buffer() to create this.\n\ntz_pf_buffer_20km &lt;- st_buffer(tz_pr_points, dist = 0.2) #20km is approx 0.2 decimal degree close to the equator\n\nWarning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\nendCapStyle, : st_buffer does not correctly buffer longitude/latitude data\n\n\ndist is assumed to be in decimal degrees (arc_degrees).\n\ntz_region+\n  geom_sf(tz_pf_buffer_20km, mapping = aes(geometry = geometry))+\n  geom_point(tz_pr, mapping = aes(x = longitude, y = latitude, color = pf_pr), size = 0.5)+\n  scale_color_distiller(palette = \"Spectral\")+\n  labs(color = \"PfPR 0-5 years\", subtitle = \"MIS 2017\")\n\n\n\n\n\n\n\n\nBuffers are especially useful when you combine them with rasters and want to extract a summarised value of pixels for a small area.\nNote that you’ll have gotten a warning that buffers don’t work correctly with longitude/latitude data. Remember the coordinate reference systems - these are important! the unit for the CRS we use WGS84 is decimal degrees and assumes an ellipsoidal (round) world still. However, the maps we create are flat. For best results you should think about changing the coordinate reference system to UTM (Universal Transverse Mercator) that assumed a flat plan and uses units of meters.\n\n\nShapefile projections\nLet’s look at our current CRS. You can view the CRS using the command st_crs()\n\nst_crs(tz_admin1)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nst_crs(tz_pr_points)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nIf these projections say don’t match or aren’t in the correct coordinate system for our analysis we can change the projection using the command st_transform().\n\n# Change the projection to UTM zone 35S\ntz_admin1_utm &lt;- st_transform(tz_admin1, 32735)\ntz_pr_points_utm &lt;- st_transform(tz_pr_points, 32735)\n\n\n\n\n\n\n\nWhere to find the right CRS?\n\n\n\nThe code needed for the crs is normally called ESPG short for European Petroleum Survey Group that maintains the standard database of codes and can be found on https://epsg.io/\nsimply type your country and it’ll offer suggested CRS to use\n\n\n\n\n\n\n\n\nChallenge 3: plot projected shapefiles\n\n\n\n\nCalculate the buffer at 20km, hint: remember the projection units\nCan you make a plot the projected shapefiles?\nDo you see any difference between the maps? If not, why?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntz_pf_buffer_20km_utm &lt;- st_buffer(tz_pr_points_utm, dist = 20000) #20km is approx 0.2 decimal degree close to the equator\n\n\nggplot(tz_admin1_utm)+\n  geom_sf()+\n  geom_sf(tz_pf_buffer_20km_utm, mapping = aes(geometry=geometry))+\n  geom_sf(tz_pr_points_utm, mapping = aes(color = pf_pr), size = 0.5)+\n  scale_color_distiller(palette = \"Spectral\")+\n  labs(color = \"PfPR 0-5 years\", subtitle = \"MIS 2017\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking publication style maps\nfor publishing maps typically you’ll find that maps require a bit more information, such as north compass, scale bars and appropriate legends. We can use ggspatial package for that.\n\npublicaton_pr_map &lt;- tz_region_map+\n  annotation_north_arrow(\n    location = 'tr', #put top right\n    height = unit(0.5, \"cm\"), \n    width = unit(0.5, \"cm\")\n  )+\n  annotation_scale(\n    location = 'bl', #bottom left\n  )+\n  theme_void()\n\npublicaton_pr_map\n\n\n\n\n\n\n\n\nWe might want to add labels of the regions on as well, we can do this using ggreppel\n\npublicaton_pr_map+\n  geom_sf_text(mapping = aes(label = name_1), size = 1.5)\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\n\n\nWriting out plots and shapefiles\nOnce we’ve completed out image we might want to save it. We can use ggsave() to do so. Note that if you don’t specific the plot object it will save the last image you created in the plot window\n\nggsave(filename = \"tanzania_pr_map_2017.png\")\n\nIf you wish to save one of the shapefiles we’ve created say the population information we can use st_write()\n\nst_write(tz_pop_admin1, \"data/shapefiles/tz_population_admin1.shp\")\n\nYou will need to save the file as a .shp file, it will create the other metadata information. Note that if you make any changes and want to save over again you will need to add the overwrite = TRUE argument into st_write()"
  },
  {
    "objectID": "posts/mapping-r/index.html#interactive-maps-using-tmap",
    "href": "posts/mapping-r/index.html#interactive-maps-using-tmap",
    "title": "Live Session 3: Introduction to Mapping in R",
    "section": "Interactive maps using tmap",
    "text": "Interactive maps using tmap\nSo far we’ve explored the use of the sf package with ggplot as well as some additions like ggspatial. Another useful package that can help build interactive maps in tmap. The tmappackage is a powerful tool for creating thematic maps in R. It provides an intuitive and flexible way to visualize spatial data, similar to how ggplot2 works for general data visualization.\nLike ggplot, the package tmap also building maps using layers. You start with tm_shape() to define the data, then add layers with various tm_*() functions.\nLet’s try recreate our first map of tanzania regions:\n\ntm_shape(tz_admin1) +\n  tm_polygons()\n\n\n\n\n\n\n\n\nSimilarly if we wanted to showcase data in each polygon we could use this for the population data we summarised:\n\ntm_shape(tz_pop_admin1) +\n  tm_polygons(\"T_TL\", palette = \"viridis\", title = \"Population\",\n               style = 'pretty', n = 4,\n              colorNA = 'lightblue', textNA = \"lakes\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"pretty\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n', 'palette' (rename to 'values'),\n  'colorNA' (rename to 'value.na'), 'textNA' (rename to 'label.na') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\nFor small multiples, specify a 'tm_scale_' for each multiple, and put them in a\nlist: 'fill'.scale = list(&lt;scale1&gt;, &lt;scale2&gt;, ...)'\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n\n\n\n\n\n\n\n\n\nwe might want to further customise things like putting the legend outside and add labels to the regions:\n\ntm_shape(tz_pop_admin1) +\n  tm_polygons(\"T_TL\", palette = \"viridis\", title = \"Population\",\n               style = 'pretty', n = 4,\n              colorNA = 'lightblue', textNA = \"lakes\")+\n  tm_text(text = \"name_1\", size = 0.5)+\n  tm_layout(legend.outside = TRUE)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"pretty\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n', 'palette' (rename to 'values'),\n  'colorNA' (rename to 'value.na'), 'textNA' (rename to 'label.na') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\nFor small multiples, specify a 'tm_scale_' for each multiple, and put them in a\nlist: 'fill'.scale = list(&lt;scale1&gt;, &lt;scale2&gt;, ...)'\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n\n\n\n\n\n\n\n\n\nbut the best part of tmap is that it can make your maps interactive. To switch to interactive mode:\n\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\n tm_shape(tz_pop_admin1) +\n  tm_polygons(\"T_TL\", \n              id=\"name_1\", #added for the labels in interactive to show region\n              palette = \"viridis\", title = \"Population\",\n               style = 'pretty', n = 4,\n              colorNA = 'lightblue', textNA = \"lakes\")+\n  #tm_text(text = \"name_1\", size = 0.5)+\n  tm_layout(legend.outside = TRUE)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"pretty\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n', 'palette' (rename to 'values'),\n  'colorNA' (rename to 'value.na'), 'textNA' (rename to 'label.na') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\nFor small multiples, specify a 'tm_scale_' for each multiple, and put them in a\nlist: 'fill'.scale = list(&lt;scale1&gt;, &lt;scale2&gt;, ...)'[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4: Interactive map of prevalence by region\n\n\n\n\nCan you make an interactive map of the prevalence by region?\nWhat is the important information you might need to display?\nCan you try make this at admin2 level?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntz_pr_point_region %&gt;% \n  ungroup() %&gt;% #run this to remove any previous groupings that occured\n  group_by(name_1) %&gt;% \n  summarise(pf_pos = sum(pf_pos, na.rm=TRUE),\n            examined = sum(examined, na.rm=TRUE),\n            mean_pr = pf_pos/examined*100) %&gt;% \n  st_drop_geometry() %&gt;% \n   #we put a \".\" to indicate where the data we've been working with goes for left join\n  left_join(tz_admin1, .) %&gt;% \n  tm_shape()+\n  tm_polygons(\"mean_pr\", \n              id=\"name_1\", #added for the labels in interactive to show region\n              palette = \"-RdYlGn\", #add negative sign to reverse the palette\n              style = \"pretty\", \n              title = \"Malaria Prevalence 0-5 years\", \n              colorNA = 'lightblue', textNA = \"lakes\")\n\n\n\n\n\n#at admin2 (council)\n#read data and join points to admin2\ntz_admin2 &lt;- st_read(\"data/shapefiles/TZ_admin2.shp\")\n\nReading layer `TZ_admin2' from data source \n  `C:\\Users\\jmillar\\OneDrive - PATH\\Documents\\github_new\\ammnet-hackathon\\posts\\mapping-r\\data\\shapefiles\\TZ_admin2.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 200 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29.3414 ymin: -11.7612 xmax: 40.4432 ymax: -0.9844\nGeodetic CRS:  WGS 84\n\ntz_pr_point_council &lt;- st_join(tz_pr_points, tz_admin2)\n\n#summarise data to admin2 for plotting\ntz_pr_point_council %&gt;% \n  ungroup() %&gt;% #run this to remove any previous groupings that occured\n  group_by(name_2) %&gt;% \n  summarise(mean_pr = mean(pf_pr, na.rm=TRUE)*100) %&gt;% #calculate prevalence as percent\n  st_drop_geometry() %&gt;% \n   #we put a \".\" to indicate where the data we've been working with goes for left join\n  left_join(tz_admin2, .) %&gt;% \n  tm_shape()+\n  tm_polygons(\"mean_pr\", \n              id=\"name_2\", #added for the labels in interactive to show region\n              #you can pick colours to make your own palette\n              palette = c(\"green4\", \"darkseagreen\", \"yellow2\",\"red3\"),  \n              title = \"Malaria Prevalence 0-5 years\", \n              n=5, breaks = c(0,1,5,30,100), \n              labels = c(\"0 - 1\",\"1- 5\",\"5-30\", \"&gt;30\"), \n              style = 'fixed',\n              colorNA = 'lightblue', textNA = \"lakes\")"
  },
  {
    "objectID": "posts/mapping-r/index.html#additional-resources",
    "href": "posts/mapping-r/index.html#additional-resources",
    "title": "Live Session 3: Introduction to Mapping in R",
    "section": "Additional Resources",
    "text": "Additional Resources\nWe’re hoping this hackathon tutorial has been helpful in getting you started with shapefiles. As you embark on your map making journey i’m sure many of you will have lots of questions about how to do more advanced things. We’ve put together a bunch of more detailed resources that this has both pulled from and the instructors have used in their own work. We hope this will be useful to you too!\nIf you find any other resources also in different languages we are always looking to share knowledge so please post them on the AMMnet slack for others to get to know more about :)\nHere are some favourites:\n\nData Carprentry Geospatial Lesson: A good starting lesson, mainly focuses on ecology and raster data\nMalaria Atlas Training: majority of material has come from this source, it includes some more tutorials on QGIS software too\nSf package tutorials: Includes a cool cheatsheet for getting started\nEarth Lab tutorials: overall great resource for everything spatial\nThe tmap book: a great more detailed book for using tmap, including using tmap for shiny!\nColorbrewer for maps: many folks might want to explore how to manipulate colors on maps, this is a great tool for that!"
  }
]