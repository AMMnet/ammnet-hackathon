[
  {
    "objectID": "posts/mapping-r/index.html",
    "href": "posts/mapping-r/index.html",
    "title": "Live Session 3: Introduction to Mapping in R",
    "section": "",
    "text": "Take the survey!\n\n\n\nIf you are present for the live session on Thursday March 6th 2025, please click here to take the survey."
  },
  {
    "objectID": "posts/mapping-r/index.html#overview",
    "href": "posts/mapping-r/index.html#overview",
    "title": "Live Session 3: Introduction to Mapping in R",
    "section": "Overview",
    "text": "Overview\nThis live session aims to introduce participants to mapping in R, with a focus on working with vector data. We will start with a brief introduction to key GIS concepts before diving into hands-on mapping techniques using R. Participants will learn how to read, manipulate, and visualize spatial data using the sf, tidyverse, and tmap packages. By the end of the session, attendees will be able to load vector data, perform basic spatial operations, and create effective maps in R.\n\nObjectives of tutorial\n\nIntroduction to GIS concepts\n\nTypes of Spatial Data – Overview of vector (point, line, polygon) and raster data.\nCoordinate reference systems\n\nVector data\n\nIntroduction to sp and sf packages\nImporting shapefiles into R (Spatial points and polygons)\nJoining data to shapefiles\nMaking buffes\nWriting shapefiles out in R\n\nCreating publication quality maps\n\nUsing ggplot2\nUsing tmap\n\nInteractive maps using tmap"
  },
  {
    "objectID": "posts/mapping-r/index.html#gis-concepts",
    "href": "posts/mapping-r/index.html#gis-concepts",
    "title": "Live Session 3: Introduction to Mapping in R",
    "section": "GIS concepts",
    "text": "GIS concepts\n\nBasic Definitions\nSpatial data is a term we use to describe any data which includes information on the locations and shapes of geographic features and the relationships between them, usually stored as coordinates and topology. Spatial data can be used to describe, for example:\n\nLocations of points of interest (e.g. water bodies, households, health facilities)\nDiscrete events and their locations (e.g. malaria cases, crime spots, floods)\nContinuous surfaces (e.g. elevation, temperature, rainfall)\nAreas with counts or rates aggregated from individual-level data (e.g. disease prevalence rates, population census, etc.)\n\nSpatial data can often be categorised into vector data or raster data.\nVector data is a representation of the world using points, lines, and polygons. This data is created by digitizing the base data, and it stores information in x, y coordinates. Vector data is one of two primary types of spatial data in geographic information systems (GIS) – the other being raster data. Example: sea, lake, land parcels, countries and their administrative regions, etc.\n\n\n\n\n\n\n\n\n\nsource:Colin Williams (NEON); captured from Earth Data Science course\n\nPoints: Each individual point is defined by a single x, y coordinate. There can be many points in a vector point file. Examples of point data include: Health facility locations, household clusters.\nLines: Lines are composed of many (at least 2) vertices, or points, that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each “bend” in the road or stream represents a vertex that has defined x, y location.\nPolygons: A polygon consists of 3 or more vertices that are connected and “closed”. Thus the outlines of plot boundaries, lakes, oceans, and states or countries are often represented by polygons. Occasionally, a polygon can have a hole in the middle of it (like a doughnut), this is something to be aware of but not an issue you will deal with in this tutorial.\n\n\n\n\n\n\nTip\n\n\n\nOccasionally, a polygon can have a hole in the middle of it (like a doughnut), or might gave gaps between each other, this is something to be aware of but not an issue you will deal with in this tutorial. As a tip, we would first recommend reaching out to the owner of the shapefile to clean prior to fixing it yourself.\n\n\n\nRaster data consists of a matrix of cells (or pixels) organized into rows and columns (or a grid) where each cell contains a value representing information, such as temperature. Rasters are digital aerial photographs, imagery from satellites, digital pictures, or even scanned maps.\n\n\n\n\n\n\n\n\n\n(source: National Ecological Observatory Network (NEON))\nBoth types of data structures can either hold discrete or continuous information and can be layered upon each other and interact. An example of discrete data would be data defining the spatial extent of water bodies, built up areas, forests, or locations of health facilities. In contrast, continuous spatial data would be used to record quantities, such as elevation, temperature, population, etc.\n\n\n\n\n\n\n\n\n\nsource:https://link.springer.com/chapter/10.1007/978-3-030-01680-7_1\nAnalysis of spatial data supports explanation of certain phenomena and solving problems which are influenced by components which vary in space, or both space and time. Two important properties of geospatial data are, for example, that:\n\nObservations in geographic space tend to be dependent on other geographic factors. For example, variations in disease rates within a country may be due to variations in population density, age, socioeconomic factors, disease vector densities, temperature or a combination of these and other factors.\nObservations that are geographically close to each other tend to be more alike than those which are further apart.\nIts important to have layers of spatial data all be on the same coordinate reference system.\n\n\n\nCoordinate Reference System\n\n\n\n\n\n\n\n\n\nsource: http://ayresriverblog.com\nLet’s address the elephant in the room and agree that the Earth is round! ;)\nWhen dealing with spatial data we often are dealing with topology which allows us to describe the information geographically. A coordinate reference system (CRS) refers to the way in which this spatial data that represent the earth’s surface (which is round / 3 dimensional) are flattened so that you can “Draw” them on a 2-dimensional surface (e.g. your computer screens or a piece of paper). There are many ways which we can ‘flatten’ the earth, each using a different (sometimes) mathematical approach to performing the flattening resulting in different coordinate system grids: most commonly the Geographic Coordinate System (GCS) and Projected Coordinate System (PCS). These approaches to flattening the data are specifically designed to optimize the accuracy of the data in terms of length and area and normally are stored in the CRS part of the shapefile in R. This is especially important when dealing with countries that are further away from the equator. Here is a fun little video to showcase how coordinate reference systems can really change/distort the reality of the actual geography!\n\n\n\n\nGeographic Coordinate System\nA geographic coordinate system is a reference system used to pin point locations onto the surface of the Earth. The locations in this system are defined by two values: latitude and longitude. These values are measured as angular distance from two planes which cross the center of the Earth - the plane defined by the Equator (for measuring the latitude coordinate), and the plane defined by the Prime Meridian (for measuring the longitude coordinate).\nA Geographic Coordinate System (GCS) is defined by an ellipsoid, geoid and datum.\nThe use of the ellipsoid in GCS comes from the fact that the Earth is not a perfect sphere. In particular, the Earth’s equatorial axis is around 21 km longer than the prime meridian axis. The geoid represents the heterogeneity of the Earth’s surface resulting from the variations in the gravitational pull. The datum represents the choice of alignment of ellipsoid (or sphere) and the geoid, to complete the ensemble of the GCS.\nIt is important to know which GCS is associated with a given spatial file, because changing the choice of the GCS (i.e. the choice of ellipsoid, geoid or datum) can change the values of the coordinates measured for the same locations. Therefore, if our GCS is not properly defined, it could lead to misplacement of point locations on the map.\n\n\nProjected Coordinate System\nThe Projected Coordinate System (PCS) is used to represent the Earth’s coordinates on a flat (map) surface. The PCS is represented by a grid, with locations defined by the Cartesian coordinates (with x and y axis). In order to transform coordinates from the GCS to PCS, mathematical transformations need to be applied, hereby referred to as projections.\n\n\n\n\n\n\n\n\n\nsource: CA Furuti"
  },
  {
    "objectID": "posts/mapping-r/index.html#vector-data-in-r-using-sf",
    "href": "posts/mapping-r/index.html#vector-data-in-r-using-sf",
    "title": "Live Session 3: Introduction to Mapping in R",
    "section": "Vector data in R using sf",
    "text": "Vector data in R using sf\nVector data are composed of discrete geometric locations (x,y values) known as vertices that define the “shape” of the spatial object. The organization of the vertices determines the type of vector that you are working with: point, line or polygon. Typically vector data is stored in shapefiles and within R can be called in using several different packages, most common being sp and sf. For the purpose of this material we will focus of teaching you the package sf as it is intended to succeed and replace R packages sp, rgeos and the vector parts of rgdal packages. It also connects nicely to tidyverse learnt in previous hackathons :)\nwe’ll start off by loading in some key packages we’ll be using during this tutorial section\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(ggspatial)\nlibrary(ggrepel)\nlibrary(tidyverse)\nlibrary(malariaAtlas)\n\n\n\n\n\n\n\nTransition from sp to sf packages\n\n\n\nThe sp package (spatial) provides classes and methods for spatial (vector) data; the classes document where the spatial location information resides, for 2D or 3D data. Utility functions are provided, e.g. for plotting data as maps, spatial selection, as well as methods for retrieving coordinates, for subsetting, print, summary, etc.\nThe sf package (simple features = points, lines, polygons and their respective ‘multi’ versions) is the new kid on the block with further functions to work with simple features, a standardized way to encode spatial vector data. It binds to the packages ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations.\nFor the time being, it is best to know and use both the sp and the sf packages, as discussed in this post. However, we focus on the sf package. for the following reasons:\n\nsf ensures fast reading and writing of data\nsf provides enhanced plotting performance\nsf objects can be treated as data frames in most operations\nsf functions can be combined using %&gt;% operator and works well with the tidyverse collection of R packages.\nsf function names are relatively consistent and intuitive (all begin with st_) However, in some cases we need to transform sf objects to sp objects or vice versa. In that case, a simple transformation to the desired class is necessary:\n\nTo sp object &lt;- as(object, Class = \"Spatial\")\nTo sf object_sf = st_as_sf(object_sp, \"sf\")\nA word of advice: be flexible in the usage of sf and sp. Sometimes it may be hard to explain why functions work for one data type and do not for the other. But since transformation is quite easy, time is better spent on analyzing your data than on wondering why operations do not work.\n\n\n\nShapefiles: Points, Lines, and Polygons\nGeospatial data in vector format are often stored in a shapefile format. Because the structure of points, lines, and polygons are different, each individual shapefile can only contain one vector type (all points, all lines or all polygons). You will not find a mixture of point, line and polygon objects in a single shapefile.\nObjects stored in a shapefile often have a set of associated attributes that describe the data. For example, a line shapefile that contains the locations of streams, might contain the associated stream name, stream “order” and other information about each stream line object.\n\n\n\n\n\n\n\n\n\nShapefiles often have many file types associated to it. Each of these files continains valuable information. The most important file is the .shp file which is the main file that stores the feature geometries. .shx is an index file to connect the feature geometry and .dbf is a dBASE file that stores all the attribute information (like a table of information). These three files are required for plotting vector data. Often times you might also get additional useful files such as the .prj which stores the coordination system information.\n\n\nImporting shapefiles into R\nShapefiles can be called in to R using the function st_read(). Similarly to read_csv() we include a filepath to a shapefile. In this instance we would load the part of the shapefile that ends with .shp\n\nPolygons\n\ntz_admin1 &lt;- st_read(\"data/shapefiles/TZ_admin1.shp\")\n\nReading layer `TZ_admin1' from data source \n  `C:\\Users\\jmillar\\OneDrive - PATH\\Documents\\github_new\\ammnet-hackathon\\posts\\mapping-r\\data\\shapefiles\\TZ_admin1.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 36 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29.3414 ymin: -11.7612 xmax: 40.4432 ymax: -0.9844\nGeodetic CRS:  WGS 84\n\n\nYou’ll notice that when you load the shapefile in, there will be a set of information explaining the features of the shapefile. The first sentence shows that you have loaded a ESRI shapefile, it contains 36 features (which are polygons in this case) and 17 columns of information stored as a data tabll. It mentions also there is a spatial extent (called bounding box) and the coordinate reference system (CRS).\nYou can also get this information when you simply call the sf object\n\ntz_admin1\n\n\n\n\n\n\n\n\n\n\nThe snapshot above would likely be similar to what you would see in your console. You’ll notice that it would display in your environment as a table, but the difference between this and any other table is that it includes additional information in the metadata indicating its spatial features. In this case it is important to read in and check the metadata for the type of spatial information you have loaded.\n\n\nPoint data\nNow that we have in some polygon information, let’s try load a different type of vector data - points. Often times in public health space we would receive data that might represent points on a map (e.g. cases at a health facility, bed nets used in a village). This style of data would include GPS coordinates of longitude and latitude to help us know geographically where they are located.\nWe are going to pull some prepared MIS data into R. For alternatives I would suggest checking out the malaria prevalence data using the malariaAtlas package in R.\n\ntz_pr &lt;- read_csv(\"data/pfpr-mis-tanzania-clean.csv\")\n\nRows: 436 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (9): country, country_id, continent, site_name, rural_urban, method, r...\ndbl  (15): id, site_id, latitude, longitude, month_start, year_start, month_...\nlgl   (2): pcr_type, source_id3\ndate  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Make the gps data into a point simple feature\ntz_pr_points &lt;- st_as_sf(tz_pr, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\nYou’ll notice that when you brought in the data it does not display the spatial information and came in as a table, so then we may want to convert these into spatial information using the st_as_sf() function. The data we’ve pulled is all the public available data from the DHS program for Tanzania, we’ve pre-processed this specifically for prevalence information only. It includes the latitude and longitude information. When converting the table to a simple feature it must have complete information (i.e. no missing coordinates), we would recommend if you get data from elsewhere make sure to check there is no missingness. Note that we set the projection for the spatial object using the crs command; crs 4326 is the standard WGS 84 CRS.\n\n\n\nPlotting the shapefiles in ggplot2\nNow that we have some shapefiles let’s try make a plot of them both. We can use the geom called geom_sf() with the ggplot funtions to make a plot. This was explored in the previous hackathon on data visualization\n\ntz_region &lt;- ggplot(tz_admin1)+\n  geom_sf()+\n  theme_bw()+\n  labs(title = \"Tanzania Regions\")\n\nWe now have a plot of the map of Tanzania and its regions. Like other ggplots we can continue to add points to this map.\n\nggplot()+\n  geom_sf(tz_admin1, mapping = aes(geometry = geometry))+\n  geom_point(tz_pr, mapping = aes(x = longitude, y = latitude, color = pf_pr))+\n  scale_color_distiller(palette = \"Spectral\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nNote that when you move the data into the geom_sf() it must include a mapping= aes() argument. If you don’t have any specific information you want to display and just want the map, you can say you would like it to map to the geometry .\n\n\n\n\n\n\nChallenge 1: plot the spatial points\n\n\n\n\nTry make the same map as above but using the tz_pr_points spatial dataset instead, what geom would you use?\nCan you change the shape, transparency and color of the points?\nCan you make the size vary by examined?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot()+\n  geom_sf(tz_admin1, mapping = aes(geometry = geometry))+\n  #notice we added size inside the aes(), shape and alpha control icon and transparency respectively\n  geom_sf(tz_pr_points, mapping = aes(color = pf_pr, size = examined), shape = 15, alpha = 0.7)+\n  #we added the scale_size() to control the size of points\n  scale_size(range = c(0.5, 4))+\n  #there are many palettes available, try see which one works for you\n  scale_color_distiller(palette = \"RdBu\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nJoining data to shapefiles\nWe often want to join data to shapefiles to enable the creation of maps and to analyse spatial data. You can use the join functions in tidysverse to join sf data to tables. Just note that the sf data must go first to automatically be recognised as sf. Else you would need to reset it using st_as_sf() .\nWe’re gonna bring in population data by Region to join to the regional shapefile. However, before we can join the data, the names for the regions don’t match. In order for a correct join the key column must be exactly the same in both datasets. So we must make a key column called name_1 to match the shapefile. We’ll be using the tricks we learnt previous hackathons like the function str_to_sentence() from the stringr package of tidyverse.\n\ntz_pop_adm1 = read_csv(\"data/tza_admpop_adm1_2020_v2.csv\") %&gt;% \n  #Change the characters from upper to title style\n  mutate(name_1 = str_to_title(ADM1_EN)) %&gt;% \n  #Some names completely don't match so manually change them\n  mutate(name_1 = case_when(name_1 == \"Dar Es Salaam\" ~ \"Dar-es-salaam\",\n                            name_1 == \"Pemba North\" ~ \"Kaskazini Pemba\",\n                            name_1 == \"Pemba South\" ~ \"Kusini Pemba\",\n                            name_1 == \"Zanzibar North\" ~ \"Kaskazini Unguja\",\n                            name_1 == \"Zanzibar Central/South\" ~ \"Kusini Unguja\",\n                            name_1 == \"Zanzibar Urban/West\" ~ \"Mjini Magharibi\",\n                            name_1 == \"Coast\" ~ \"Pwani\",\n                            TRUE ~ as.character(name_1) #be sure to include this or it will turn name_1 NA\n                            ))\n\n#check if names in the columns match\ntable(tz_admin1$name_1 %in% tz_pop_adm1$name_1)\n\n\nFALSE  TRUE \n    5    31 \n\n\n\n\n\n\n\n\nTip\n\n\n\nYou’ll notice 5 regions haven’t matched the population table. If you open the shapefile you’ll find the column type_1 and notice that the shapefile includes polygon shapes for water bodies. If you’d like to exlcude these you can run a filter and make a tz_region_only shapefile by running:\ntz_region_only &lt;- filter(tz_admin1, type_1 == \"Region\")\n\n\nNow we can join the data to the current shapefile. Don’t worry that they currently don’t match! we have a plan ;)\n\ntz_pop_admin1 &lt;- tz_admin1 %&gt;% left_join(tz_pop_adm1, by = \"name_1\") \n\nggplot(tz_pop_admin1)+\n  geom_sf(mapping = aes(fill = T_TL))+\n  #use na.value to make the lakes appear as lightblue\n  scale_fill_viridis_c(option = \"B\", na.value = \"lightblue\", trans = 'sqrt')+\n  theme_bw()+\n  labs(fill = \"Total Population\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2: plot binned information\n\n\n\n\nCan you make the plot above using binned categories for population?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(tz_pop_admin1)+\n  geom_sf(mapping = aes(fill = T_TL))+\n  #use na.value to make the lakes appear as lightblue\n  scale_fill_viridis_b(option = \"B\", na.value = \"lightblue\", trans = 'sqrt', breaks = c(0,100000,1000000, 2000000, 3000000, 4000000, 8000000))+\n  theme_bw()+\n  labs(fill = \"Total Population\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting polygon names for point data\ncoming back to our prevalence data, often times for decision making we might want data to be summarised to regional forms. Currently the point prevalence doesn’t have any information about which region they belong to. So we can using the st_join . By default this spatial join will look for where the geometries of the spatial data’s intersect using st_interesect . It also by default uses a left join which is why the points come first.\n\nsf_use_s2(FALSE)\ntz_pr_point_region &lt;- st_join(tz_pr_points, tz_admin1)\n\nfrom this we can try and calculate the mean prevalence in each region\n\ntz_region_map &lt;- tz_pr_point_region %&gt;% \n  ungroup() %&gt;% #run this to remove any previous groupings that occured\n  group_by(name_1) %&gt;% \n  summarise(mean_pr = mean(pf_pr, na.rm=TRUE)) %&gt;% \n  st_drop_geometry() %&gt;% \n   #we put a \".\" to indicate where the data we've been working with goes for left join\n  left_join(tz_admin1, .) %&gt;% \n  ggplot()+\n  geom_sf(mapping = aes(fill = mean_pr))+\n  scale_fill_distiller(palette = \"Spectral\", na.value = \"lightblue\")+\n  labs(fill = \"Mean PfPR 0-5 years\", title = \"Tanzania Regions\", subtitle = \"MIS 2017\")+\n  theme_bw()\n\ntz_region_map\n\n\n\n\n\n\n\n\n\n\nBuffers\nSometimes you might want to create a buffer around the spatial information you have. Let’s try this on the point data. we’ll make a 20km buffer around each spatial point. We can use the st_buffer() to create this.\n\ntz_pf_buffer_20km &lt;- st_buffer(tz_pr_points, dist = 0.2) #20km is approx 0.2 decimal degree close to the equator\n\nWarning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle =\nendCapStyle, : st_buffer does not correctly buffer longitude/latitude data\n\n\ndist is assumed to be in decimal degrees (arc_degrees).\n\ntz_region+\n  geom_sf(tz_pf_buffer_20km, mapping = aes(geometry = geometry))+\n  geom_point(tz_pr, mapping = aes(x = longitude, y = latitude, color = pf_pr), size = 0.5)+\n  scale_color_distiller(palette = \"Spectral\")+\n  labs(color = \"PfPR 0-5 years\", subtitle = \"MIS 2017\")\n\n\n\n\n\n\n\n\nBuffers are especially useful when you combine them with rasters and want to extract a summarised value of pixels for a small area.\nNote that you’ll have gotten a warning that buffers don’t work correctly with longitude/latitude data. Remember the coordinate reference systems - these are important! the unit for the CRS we use WGS84 is decimal degrees and assumes an ellipsoidal (round) world still. However, the maps we create are flat. For best results you should think about changing the coordinate reference system to UTM (Universal Transverse Mercator) that assumed a flat plan and uses units of meters.\n\n\nShapefile projections\nLet’s look at our current CRS. You can view the CRS using the command st_crs()\n\nst_crs(tz_admin1)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nst_crs(tz_pr_points)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nIf these projections say don’t match or aren’t in the correct coordinate system for our analysis we can change the projection using the command st_transform().\n\n# Change the projection to UTM zone 35S\ntz_admin1_utm &lt;- st_transform(tz_admin1, 32735)\ntz_pr_points_utm &lt;- st_transform(tz_pr_points, 32735)\n\n\n\n\n\n\n\nWhere to find the right CRS?\n\n\n\nThe code needed for the crs is normally called ESPG short for European Petroleum Survey Group that maintains the standard database of codes and can be found on https://epsg.io/\nsimply type your country and it’ll offer suggested CRS to use\n\n\n\n\n\n\n\n\nChallenge 3: plot projected shapefiles\n\n\n\n\nCalculate the buffer at 20km, hint: remember the projection units\nCan you make a plot the projected shapefiles?\nDo you see any difference between the maps? If not, why?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntz_pf_buffer_20km_utm &lt;- st_buffer(tz_pr_points_utm, dist = 20000) #20km is approx 0.2 decimal degree close to the equator\n\n\nggplot(tz_admin1_utm)+\n  geom_sf()+\n  geom_sf(tz_pf_buffer_20km_utm, mapping = aes(geometry=geometry))+\n  geom_sf(tz_pr_points_utm, mapping = aes(color = pf_pr), size = 0.5)+\n  scale_color_distiller(palette = \"Spectral\")+\n  labs(color = \"PfPR 0-5 years\", subtitle = \"MIS 2017\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking publication style maps\nfor publishing maps typically you’ll find that maps require a bit more information, such as north compass, scale bars and appropriate legends. We can use ggspatial package for that.\n\npublicaton_pr_map &lt;- tz_region_map+\n  annotation_north_arrow(\n    location = 'tr', #put top right\n    height = unit(0.5, \"cm\"), \n    width = unit(0.5, \"cm\")\n  )+\n  annotation_scale(\n    location = 'bl', #bottom left\n  )+\n  theme_void()\n\npublicaton_pr_map\n\n\n\n\n\n\n\n\nWe might want to add labels of the regions on as well, we can do this using ggreppel\n\npublicaton_pr_map+\n  geom_sf_text(mapping = aes(label = name_1), size = 1.5)\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\n\n\nWriting out plots and shapefiles\nOnce we’ve completed out image we might want to save it. We can use ggsave() to do so. Note that if you don’t specific the plot object it will save the last image you created in the plot window\n\nggsave(filename = \"tanzania_pr_map_2017.png\")\n\nIf you wish to save one of the shapefiles we’ve created say the population information we can use st_write()\n\nst_write(tz_pop_admin1, \"data/shapefiles/tz_population_admin1.shp\")\n\nYou will need to save the file as a .shp file, it will create the other metadata information. Note that if you make any changes and want to save over again you will need to add the overwrite = TRUE argument into st_write()"
  },
  {
    "objectID": "posts/mapping-r/index.html#interactive-maps-using-tmap",
    "href": "posts/mapping-r/index.html#interactive-maps-using-tmap",
    "title": "Live Session 3: Introduction to Mapping in R",
    "section": "Interactive maps using tmap",
    "text": "Interactive maps using tmap\nSo far we’ve explored the use of the sf package with ggplot as well as some additions like ggspatial. Another useful package that can help build interactive maps in tmap. The tmappackage is a powerful tool for creating thematic maps in R. It provides an intuitive and flexible way to visualize spatial data, similar to how ggplot2 works for general data visualization.\nLike ggplot, the package tmap also building maps using layers. You start with tm_shape() to define the data, then add layers with various tm_*() functions.\nLet’s try recreate our first map of tanzania regions:\n\ntm_shape(tz_admin1) +\n  tm_polygons()\n\n\n\n\n\n\n\n\nSimilarly if we wanted to showcase data in each polygon we could use this for the population data we summarised:\n\ntm_shape(tz_pop_admin1) +\n  tm_polygons(\"T_TL\", palette = \"viridis\", title = \"Population\",\n               style = 'pretty', n = 4,\n              colorNA = 'lightblue', textNA = \"lakes\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"pretty\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n', 'palette' (rename to 'values'),\n  'colorNA' (rename to 'value.na'), 'textNA' (rename to 'label.na') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\nFor small multiples, specify a 'tm_scale_' for each multiple, and put them in a\nlist: 'fill'.scale = list(&lt;scale1&gt;, &lt;scale2&gt;, ...)'\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n\n\n\n\n\n\n\n\n\nwe might want to further customise things like putting the legend outside and add labels to the regions:\n\ntm_shape(tz_pop_admin1) +\n  tm_polygons(\"T_TL\", palette = \"viridis\", title = \"Population\",\n               style = 'pretty', n = 4,\n              colorNA = 'lightblue', textNA = \"lakes\")+\n  tm_text(text = \"name_1\", size = 0.5)+\n  tm_layout(legend.outside = TRUE)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"pretty\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n', 'palette' (rename to 'values'),\n  'colorNA' (rename to 'value.na'), 'textNA' (rename to 'label.na') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\nFor small multiples, specify a 'tm_scale_' for each multiple, and put them in a\nlist: 'fill'.scale = list(&lt;scale1&gt;, &lt;scale2&gt;, ...)'\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n\n\n\n\n\n\n\n\n\nbut the best part of tmap is that it can make your maps interactive. To switch to interactive mode:\n\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\n tm_shape(tz_pop_admin1) +\n  tm_polygons(\"T_TL\", \n              id=\"name_1\", #added for the labels in interactive to show region\n              palette = \"viridis\", title = \"Population\",\n               style = 'pretty', n = 4,\n              colorNA = 'lightblue', textNA = \"lakes\")+\n  #tm_text(text = \"name_1\", size = 0.5)+\n  tm_layout(legend.outside = TRUE)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"pretty\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n', 'palette' (rename to 'values'),\n  'colorNA' (rename to 'value.na'), 'textNA' (rename to 'label.na') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\nFor small multiples, specify a 'tm_scale_' for each multiple, and put them in a\nlist: 'fill'.scale = list(&lt;scale1&gt;, &lt;scale2&gt;, ...)'[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4: Interactive map of prevalence by region\n\n\n\n\nCan you make an interactive map of the prevalence by region?\nWhat is the important information you might need to display?\nCan you try make this at admin2 level?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntz_pr_point_region %&gt;% \n  ungroup() %&gt;% #run this to remove any previous groupings that occured\n  group_by(name_1) %&gt;% \n  summarise(pf_pos = sum(pf_pos, na.rm=TRUE),\n            examined = sum(examined, na.rm=TRUE),\n            mean_pr = pf_pos/examined*100) %&gt;% \n  st_drop_geometry() %&gt;% \n   #we put a \".\" to indicate where the data we've been working with goes for left join\n  left_join(tz_admin1, .) %&gt;% \n  tm_shape()+\n  tm_polygons(\"mean_pr\", \n              id=\"name_1\", #added for the labels in interactive to show region\n              palette = \"-RdYlGn\", #add negative sign to reverse the palette\n              style = \"pretty\", \n              title = \"Malaria Prevalence 0-5 years\", \n              colorNA = 'lightblue', textNA = \"lakes\")\n\n\n\n\n\n#at admin2 (council)\n#read data and join points to admin2\ntz_admin2 &lt;- st_read(\"data/shapefiles/TZ_admin2.shp\")\n\nReading layer `TZ_admin2' from data source \n  `C:\\Users\\jmillar\\OneDrive - PATH\\Documents\\github_new\\ammnet-hackathon\\posts\\mapping-r\\data\\shapefiles\\TZ_admin2.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 200 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29.3414 ymin: -11.7612 xmax: 40.4432 ymax: -0.9844\nGeodetic CRS:  WGS 84\n\ntz_pr_point_council &lt;- st_join(tz_pr_points, tz_admin2)\n\n#summarise data to admin2 for plotting\ntz_pr_point_council %&gt;% \n  ungroup() %&gt;% #run this to remove any previous groupings that occured\n  group_by(name_2) %&gt;% \n  summarise(mean_pr = mean(pf_pr, na.rm=TRUE)*100) %&gt;% #calculate prevalence as percent\n  st_drop_geometry() %&gt;% \n   #we put a \".\" to indicate where the data we've been working with goes for left join\n  left_join(tz_admin2, .) %&gt;% \n  tm_shape()+\n  tm_polygons(\"mean_pr\", \n              id=\"name_2\", #added for the labels in interactive to show region\n              #you can pick colours to make your own palette\n              palette = c(\"green4\", \"darkseagreen\", \"yellow2\",\"red3\"),  \n              title = \"Malaria Prevalence 0-5 years\", \n              n=5, breaks = c(0,1,5,30,100), \n              labels = c(\"0 - 1\",\"1- 5\",\"5-30\", \"&gt;30\"), \n              style = 'fixed',\n              colorNA = 'lightblue', textNA = \"lakes\")"
  },
  {
    "objectID": "posts/mapping-r/index.html#additional-resources",
    "href": "posts/mapping-r/index.html#additional-resources",
    "title": "Live Session 3: Introduction to Mapping in R",
    "section": "Additional Resources",
    "text": "Additional Resources\nWe’re hoping this hackathon tutorial has been helpful in getting you started with shapefiles. As you embark on your map making journey i’m sure many of you will have lots of questions about how to do more advanced things. We’ve put together a bunch of more detailed resources that this has both pulled from and the instructors have used in their own work. We hope this will be useful to you too!\nIf you find any other resources also in different languages we are always looking to share knowledge so please post them on the AMMnet slack for others to get to know more about :)\nHere are some favourites:\n\nData Carprentry Geospatial Lesson: A good starting lesson, mainly focuses on ecology and raster data\nMalaria Atlas Training: majority of material has come from this source, it includes some more tutorials on QGIS software too\nSf package tutorials: Includes a cool cheatsheet for getting started\nEarth Lab tutorials: overall great resource for everything spatial\nThe tmap book: a great more detailed book for using tmap, including using tmap for shiny!\nColorbrewer for maps: many folks might want to explore how to manipulate colors on maps, this is a great tool for that!"
  },
  {
    "objectID": "posts/data-vis/index.html",
    "href": "posts/data-vis/index.html",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "",
    "text": "This tutorial introduces you to data visualization in R. We will learn how to develop an understanding of our data before visualization, making quick exploratory visualizations using base R functions, and creating various plots using the ggplot2 package. You’ll learn how to customize and enhance your visualizations for clear data communication. By the end, you’ll have the skills to create plots to effectively present your data insights."
  },
  {
    "objectID": "posts/data-vis/index.html#getting-started",
    "href": "posts/data-vis/index.html#getting-started",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "Getting Started",
    "text": "Getting Started\nBefore you begin, you might want to create a new project in RStudio. This can be done by clicking on the “New Project” button in the upper right corner of the RStudio window. You can then name the project and choose a directory to save it in.\nNext, we will load the tidyverse package. This package provides a set of useful functions for data manipulation and visualization. We will use the ggplot2 package to create plots in the later section of this tutorial.\n\n# load packages\nlibrary(tidyverse)\n\nNext, let’s download the two example datasets we will use in this tutorial. These are avialable in the AMMnet Hackathon GitHub repository.\nI suggest creating a data folder inside your R project, then we can download the two example datasets so that they are saved to your computer.\n\n# Create a data folder\ndir.create(\"data\")\n\n# Download example data\nurl &lt;- \"https://raw.githubusercontent.com/AMMnet/AMMnet-Hackathon/main/01_data-vis/data/\"\n\ndownload.file(paste0(url, \"mockdata_cases.csv\"), destfile = \"data/mockdata_cases.csv\")\ndownload.file(paste0(url, \"mosq_mock.csv\"), destfile = \"data/mosq_mock.csv\")\n\n# Load example data\nmalaria_data   &lt;- read_csv(\"data/mockdata_cases.csv\")\nmosquito_data  &lt;- read_csv(\"data/mosq_mock.csv\")\n\nThe two datasets we will use are mockdata_cases.csv and mosq_mock.csv, which are mock example datasets that should be similar to malaria case surviellance and mosquito field collection data, respectively. In the following sections we will use the mockdata_cases.csv to introduce concepts of data visualization in R. The mosq_mock.csv dataset is used in the challenge sections."
  },
  {
    "objectID": "posts/data-vis/index.html#characterizing-our-data",
    "href": "posts/data-vis/index.html#characterizing-our-data",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "Characterizing our data",
    "text": "Characterizing our data\nBefore we start visualizing our data, we need to understand the characteristics of our data. The goal is to get an idea of the data structure and to understand the relationships between variables.\nHere are some functions that can help us understand the structure of our data:\n\n# Explore the structure and summary of the datasets\ndim(malaria_data)  \nhead(malaria_data)\nsummary(malaria_data)\n\nWe should also explore individual columns/variables\n\nmalaria_data$location          # values for a single column\nunique(malaria_data$location)  # unique values for a single column\ntable(malaria_data$location)   # frequencies for a single column\ntable(malaria_data$location, malaria_data$ages)  # frequencies for multiple columns\n\nFinally, we should check for missing values in each column, as these can affect our visualizations.\n\nsum(is.na(malaria_data))\n\n[1] 0\n\n\n\n\n\n\n\n\nChallenge 1: Explore the structure and summary of the mosquito_data dataset\n\n\n\n\nWhat are the dimensions of the dataset?\nWhat are the column names?\nWhat are the column types?\nWhat are some key variables or relationships that we can explore?"
  },
  {
    "objectID": "posts/data-vis/index.html#exploratory-visualizations-using-base-r-functions",
    "href": "posts/data-vis/index.html#exploratory-visualizations-using-base-r-functions",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "Exploratory Visualizations Using Base R Functions",
    "text": "Exploratory Visualizations Using Base R Functions\nFirst, we will look at some exploratory data visualization techniques using base R functions. The purpose of these plots is to help us understand the relationships between variables and characteristics of our data. They are useful for quickly exploring the data and understanding the relationships, but they are not are not great for sharing in scientific publications/presentations.\n\nSingle variable comparison\nFor one variable comparison, we can use hist() function to create a histogram.\n\nhist(malaria_data$prev)\n\n\n\n\n\n\n\nhist(malaria_data$prev, \n    breaks = 10, \n    main = \"Distribution of Malaria Prevalence\",\n    xlab = \"Malaria Prevalence\",\n    ylab = \"Frequency\",\n    col = \"lightblue\",\n    border = \"black\")\n\n\n\n\n\n\n\n\nAnnother useful function for single variable comparisons is barplot(). In this case, we will use the table() function to count the number of observations in each category, then use barplot() to create a barplot.\n\nbarplot(table(malaria_data$ages))\n\n\n\n\n\n\n\nbarplot(table(malaria_data$location))\n\n\n\n\n\n\n\nbarplot(table(malaria_data$year))\n\n\n\n\n\n\n\n\n\n\nMultiple variables\nFor multiple variables, we can use plot() function to create a scatterplot. In this case, we will use the S operator to pull out an individual column from the dataset. Then we will use plot() to create a scatterplot. The first argument in plot() is the x variable, and the second argument is the y variable.\n\nplot(malaria_data$total, malaria_data$positive)\n\n\n\n\n\n\n\nplot(malaria_data$month, malaria_data$prev)\n\n\n\n\n\n\n\n\nWe can also create boxplots by using boxplot() function. In this function we use the ~ operator, which tells R to use the values on the lefthand side of the ~ as the x variable and the righthand side of the ~ as the y variable. I think of ~ as “in terms of”, and for boxplots this means that your numerical variable will be on the x axis and the categorical variable will be on the y axis.\n\nboxplot(malaria_data$prev ~ malaria_data$month) \n\n\n\n\n\n\n\nboxplot(malaria_data$prev ~ malaria_data$location) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2: Explore the structure and summary of the mosquito_data dataset\n\n\n\n\nAre their any interesting patterns in individual variables/columns?\nAre there any relationships between variables/columns?"
  },
  {
    "objectID": "posts/data-vis/index.html#data-visualization-with-ggplot2",
    "href": "posts/data-vis/index.html#data-visualization-with-ggplot2",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "Data Visualization with ggplot2",
    "text": "Data Visualization with ggplot2\nBase R functions like hist() and barplot() are great for quickly exploring our data, but we may want to use more powerful visualization techniques when preparing outputs for scientific reports, presentations, and publications.\nThe ggplot2 package is a popular visualization package for R. It provides an easy-to-use interface for creating data visualizations. The ggplot2 package is based on the “grammar of graphics” and is a powerful way to create complex visualizations that are useful for creating scientific and publication-quality figures.\nThe “grammar of graphics” used in ggplot2 is a set of rules that are used to develop data visualizations using a layering approach. Layers are added using the ‘+’ operator.\n\nComponents of a ggplot\nThere are three main components of a ggplot: 1. The data: the dataset we want to visualize 2. The aesthetics: the visual properties from the data used in the plot 3. The geometries: the visual representations of the data (e.g., points, lines, bars)\n\nThe data\nAll ggplot2 plots require a data frame as input. Just running this line will produce a blank plot because we have stated which elements from the data we want to visualize or how we want to visualize them.\n\nggplot(data = malaria_data) \n\n\n\n\n\n\n\n\n\n\nThe aesthetics\nNext, we need to specify the visual properties of the plot that are determined by the data. The aesthetics are specified using the aes() function. The output should now produce a blank plot but with determined visual properties (e.g., axes labels).\n\nggplot(data = malaria_data, aes(x = total, y = positive)) \n\n\n\n\n\n\n\n\n\n\nThe geometries\nFinally, we need to specify the visual representation of the data. The geometries are specified using the geom_* function. There are many different types of geometries that can be used in ggplot2. We will use geom_point() in this example and we will append it to the previous plot using the + operator. The output should now produce a plot with the specified visual representation of the data.\n\nggplot(data = malaria_data, aes(x = total, y = positive)) + geom_point()\n\n\n\n\n\n\n\n\nHere are some examples of different geom functions:\n\nggplot(data = malaria_data, aes(x = prev)) +\n  geom_histogram(bins = 20)  # the \"bins\" argument specifies the number of bars\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = year)) +\n  geom_bar(fill = \"tomato\")  # the \"fill\" argument specifies the color of the bars\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = location, y = prev)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2)  # geom_jitter adds jittered points to the plot, and \n\n\n\n\n\n\n\n                            # the \"alpha\" argument specifies the transparency\n\nggplot(data = malaria_data, aes(x = location, y = prev)) +\n  geom_violin() +          # Violin plot are similar to boxplots, but illustrate \n  geom_jitter(alpha = 0.2) # the distribution of the data\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = total, y = positive)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")  # The smooth geom add a smoothed line to the plot, \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n                              # using the \"lm\" or other methods\n\n\n\n\nExtending the aesthetics\nAdditional visual properties, such as color, size, and shape, can be defined from our input data using the aes() function. Here is an example of adding color to a previous plot using the color aesthetic.\n\nggplot(data = malaria_data, aes(x = total, y = positive, color = location)) +\n  geom_point()\n\n\n\n\n\n\n\n\nNote that this is different then defining a color directly within the geom_point(), which would only apply a single color to all points.\n\nggplot(data = malaria_data, aes(x = total, y = positive)) +\n  geom_point(color = \"tomato\")\n\n\n\n\n\n\n\n\nWhen using the aes() function, the visual properties will be determined by a variable in the dataset. This allows us to visualize relationships between multiple variables at the same time.\n\nggplot(data = malaria_data, aes(x = prev, fill = ages)) +\n  geom_histogram(color = \"black\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2)\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = total, y = positive, color = location), alpha = 0.5) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = xcoord, y = ycoord, color = location)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3: Create ggplot2 visualizations of the ‘mosquito_data’ dataset\n\n\n\n\nAre their any interesting patterns in individual variables/columns?\nHow can we use the aes() function to view multiple variables in a single plot?\nAre there any additional geometries that may be useful for visualizing this dataset?"
  },
  {
    "objectID": "posts/data-vis/index.html#customizing-ggplot-graphics-for-presentation-and-communication",
    "href": "posts/data-vis/index.html#customizing-ggplot-graphics-for-presentation-and-communication",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "Customizing ggplot Graphics for Presentation and Communication",
    "text": "Customizing ggplot Graphics for Presentation and Communication\nIn this section, we will using additional features of ggplot2 to customize and develop high-quality plots that can used in scientific publications and presentations.\n\nThemes\nThere are many different themes that can be used in ggplot2. The “theme” function is used to specify the theme of the plot. There are many preset theme functions, and further custom themes can be created using the generic theme() function.\nTypically you will want to set the theme at the end of your plot.\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  theme_classic()\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  theme_bw()\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = ages)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nLabels\nLabels can be added to various components of a plot using the labs() function.\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = ages)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  labs(title = \"Malaria prevalence by location and age group\",\n       subtitle = \"Data from 2018 - 2020\",\n       x = \"Location\",\n       y = \"Prevalence\",\n       fill = \"Age group\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n### Custom color palettes\nThere are many different color palettes that can be used in ggplot2. The “scale_color” function is used to specify the color of the plot. There are many preset color palettes, and further custom color palettes can be created using the generic scale_color() function.\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nWe can also set our own colors.\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  scale_fill_manual(values = c(\"#C6E0FF\", \"#136F63\", \"#E0CA3C\", \"#F34213\", \"#3E2F5B\"))\n\n\n\n\n\n\n\n\nThe examples above show how to use colors for categorical variables, but we can also use custom color palettes for continuous variables.\n\nggplot(data = malaria_data, aes(x = total, y = positive, color = prev)) +\n  geom_point() +\n  scale_color_gradient(low = \"blue\", high = \"red\")\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = total, y = positive, color = prev)) +\n  geom_point() +\n  # use viridis package to create custom color palettes\n  scale_color_viridis_c(option = \"magma\")  \n\n\n\n\n\n\n\n\n\n\nFacets\nFacets are a powerful feature of ggplot2 that allow us to create multiple plots based on a single variable. This “small multiple” approach is another effective way to visualize relationships between mutliple variables.\nFacets also make use of the ~ operator.\n\nggplot(data = malaria_data, aes(x = total, y = positive, color = prev)) +\n  geom_point() +\n  scale_color_viridis_c(option = \"magma\") +\n  facet_wrap(~ location)\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  facet_wrap(~ ages) +\n  coord_flip() +  # flips the x and y axes\n  scale_fill_manual(\n    values = c(\"#C6E0FF\", \"#136F63\", \"#E0CA3C\", \"#F34213\", \"#3E2F5B\")) +\n  labs(title = \"Malaria prevalence by location and age group\",\n       subtitle = \"Data from 2018 - 2020\",\n       x = \"Location\",\n       y = \"Prevalence\",\n       fill = \"Age group\") +\ntheme_classic()\n\n\n\n\n\n\n\nggplot(data = malaria_data, aes(x = prev, fill = ages)) +\n  geom_histogram(bins = 10) +\n  scale_fill_viridis_d() +\n  facet_grid(year ~ .)\n\n\n\n\n\n\n\n\n\n\nExporting plots\nWe can export plots to a variety of formats using the ggsave() function. We can specify which plot to export by saving in an object and then calling the object in the ggsave() function, otherwise ggsave() will save the current/last plot. The width and height of the output image using the width and height can be set using the width and height arguments, and the resolution of the image using the dpi argument.\nThe file type can be set using the format argument, or by using a specific file extension. I recommend using informative names for the output file so that it is easily identifiable.\n\nggplot(data = malaria_data, aes(x = location, y = prev, fill = location)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2) +\n  facet_wrap(~ ages) +\n  coord_flip() +  # flips the x and y axes\n  scale_fill_manual(values = c(\"#C6E0FF\", \"#136F63\", \"#E0CA3C\", \"#F34213\", \"#3E2F5B\")) +\n  labs(title = \"Malaria prevalence by location and age group\",\n       subtitle = \"Data from 2018 - 2020\",\n       x = \"Location\",\n       y = \"Prevalence\",\n       fill = \"Age group\") +\ntheme_classic()\n\nggsave(\"malaria-prevalence-age-boxplot.png\", width = 10, height = 6, dpi = 300)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4: Develop customized ggplot figures for the ‘mosquito_data’ dataset\n\n\n\n\nTest customs themes on your previous plots, consider looking for new packages with more themes\nApply custom color palettes to your plots, explore additional color palettes and packages\nUse facets to visualize relationships between multiple variables"
  },
  {
    "objectID": "posts/data-vis/index.html#final-challenges",
    "href": "posts/data-vis/index.html#final-challenges",
    "title": "Live Session 1: Introduction to Data Visualization in R",
    "section": "Final Challenges",
    "text": "Final Challenges\nCHALLENGE 1: Create a figure showing how the Anopheles gambiae total counts vary each day and by location.\nCHALLENGE 2: Create a figure showing the hourly Anopheles gambiae total counts each hour."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A new virtual hackathon series for continual learning was developed by a collaborative effort between the Best Practices and Learning Committees.\nAims:\n\nTo create a continual support space that anyone can access\nTo learn how to code and how to code better\nTo upskill people from a distance, ultimately improving all our abilities in:\n\ndata handling, presentation and analysis\nexperimental design\nquantitative understanding\ncoding and problem solving\ntransmission modelling\n\nTo collaborate better and more equitably\n\nStructure:\n\nA theme is selected by the committee\nExperts sign up to support specific themed areas\nA participant submits a coding related problem for consideration within a specific theme\nExperts select 2 – 3 problems per session on the chosen theme, and prepare some solutions to present\n\nExperts will work with the Hackathon Committee for a half a day to consider the problems submitted and draft solutions. Once solutions are ready for a hackathon audience, they will present these during a 2-hour training session. Once the 2-hour training session concludes, experts will summarize the session, adapt solutions, and upload them to the shared space. A recording of the training will also be provided.\nIf you are an expert in the following topic areas, we need your help!\n\nData handling\nVisualisation \nDescriptive statistics\nPower calculations\nRegression statistics\nMechanistic modelling\nBayesian statistics\n\nWhy volunteer as an expert? The best way to learn is to teach. You’ll also have an opportunity to network and gain professional visibility within the AMMnet community.\nTo volunteer as an expert, please complete the form at the link here."
  },
  {
    "objectID": "about.html#a-virtual-hackathon-series-for-continual-learning",
    "href": "about.html#a-virtual-hackathon-series-for-continual-learning",
    "title": "About",
    "section": "",
    "text": "A new virtual hackathon series for continual learning was developed by a collaborative effort between the Best Practices and Learning Committees.\nAims:\n\nTo create a continual support space that anyone can access\nTo learn how to code and how to code better\nTo upskill people from a distance, ultimately improving all our abilities in:\n\ndata handling, presentation and analysis\nexperimental design\nquantitative understanding\ncoding and problem solving\ntransmission modelling\n\nTo collaborate better and more equitably\n\nStructure:\n\nA theme is selected by the committee\nExperts sign up to support specific themed areas\nA participant submits a coding related problem for consideration within a specific theme\nExperts select 2 – 3 problems per session on the chosen theme, and prepare some solutions to present\n\nExperts will work with the Hackathon Committee for a half a day to consider the problems submitted and draft solutions. Once solutions are ready for a hackathon audience, they will present these during a 2-hour training session. Once the 2-hour training session concludes, experts will summarize the session, adapt solutions, and upload them to the shared space. A recording of the training will also be provided.\nIf you are an expert in the following topic areas, we need your help!\n\nData handling\nVisualisation \nDescriptive statistics\nPower calculations\nRegression statistics\nMechanistic modelling\nBayesian statistics\n\nWhy volunteer as an expert? The best way to learn is to teach. You’ll also have an opportunity to network and gain professional visibility within the AMMnet community.\nTo volunteer as an expert, please complete the form at the link here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to AMMNet Hackathon Blog!",
    "section": "",
    "text": "Live Session 3: Introduction to Mapping in R\n\n\n\n\n\n\nR\n\n\nSpatial data\n\n\nData cleaning\n\n\nData visualization\n\n\nGIS\n\n\nLive session\n\n\n\n\n\n\n\n\n\nFeb 20, 2025\n\n\nPunam Amratia, Jailos Lubinda, Adam Saddler, Paulina Dzianach, Justin Millar, Naomie Tedto\n\n\n\n\n\n\n\n\n\n\n\n\nLive Session 2: Introduction to Data Wrangling in R\n\n\n\n\n\n\nR\n\n\nData cleaning\n\n\nData validation\n\n\nLive session\n\n\n\n\n\n\n\n\n\nOct 14, 2024\n\n\nLazaro Mwandigha, Christian Selinger, Ellie Sherrard-Smith, Justin Millar\n\n\n\n\n\n\n\n\n\n\n\n\nLive Session 1: Introduction to Data Visualization in R\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\nggplot2\n\n\nLive session\n\n\n\n\n\n\n\n\n\nJul 19, 2024\n\n\nJustin Millar, Ellie Sherrard-Smith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/data-wrangle/index.html",
    "href": "posts/data-wrangle/index.html",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "",
    "text": "Take the survey!\n\n\n\nIf you are present for the live session on Monday October 21st, please click here to take the survey."
  },
  {
    "objectID": "posts/data-wrangle/index.html#introduction",
    "href": "posts/data-wrangle/index.html#introduction",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "Introduction",
    "text": "Introduction\n\nWhat do we mean with data wrangling?\nThe Cambridge dictionary gives several meanings to the verb wrangle:\n\nto argue with someone about something, especially for a long time\nto take care of, control, or move animals, especially large animals such as cows or horses (mainly American English)\nto move a person or thing somewhere, usually with difficulty or using force\nto take care of or deal with something, usually when this is difficult\n\n\n\nData Wrangling\nBy data wrangling, we mean here the process of checking and correcting quality and integrity of data relevant to malaria modeling, prior to any further analysis. This is also known as data validation.\nData validation involves checking various aspects of your dataset, such as missing values, data types, outliers, and adherence to specific rules or constraints.\nValidating our data helps maintain its quality and integrity, ensuring that any analyses or decisions made based on the data are robust and reliable.\n\n\nWhy Validate Data?\nEnsure Data Integrity: Validating data helps identify and rectify errors, ensuring the integrity of the dataset.\nImprove Analysis Accuracy: Clean and validated data leads to more accurate analysis and modeling results.\nCompliance and Standards: Data validation ensures that the data conforms to predefined rules, standards, or regulatory requirements.\nError Prevention: Early detection of errors can prevent downstream issues and save time in troubleshooting."
  },
  {
    "objectID": "posts/data-wrangle/index.html#getting-started",
    "href": "posts/data-wrangle/index.html#getting-started",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "Getting Started",
    "text": "Getting Started\nBefore you begin, you might want to create a new project in RStudio. This can be done by clicking on the “New Project” button in the upper right corner of the RStudio window. You can then name the project and choose a directory to save it in.\nNext, we will load the tidyverse package. This package provides a set of useful functions for data manipulation and visualization. We will use the ggplot2 package to create plots in the later section of this tutorial.\n\n# load packages\nlibrary(tidyverse)\n\nNext, let’s download the two example datasets we will use in this tutorial. These are available in the AMMnet Hackathon GitHub repository.\nI suggest creating a data folder inside your R project, then we can download the two example datasets so that they are saved to your computer.\n\n# Create a data folder\ndir.create(\"data\")\n\n# Download example data\nurl &lt;- \"https://raw.githubusercontent.com/AMMnet/AMMnet-Hackathon/main/02_data-wrangle/data/\"\n\ndownload.file(paste0(url, \"mockdata_cases1.csv\"), destfile = \"data/mockdata_cases1.csv\")\ndownload.file(paste0(url, \"mosq_mock1.csv\"), destfile = \"data/mosq_mock1.csv\")\n\n# Load example data\ndata_cases   &lt;- read_csv(\"data/mockdata_cases1.csv\")\nmosq_data  &lt;- read_csv(\"data/mosq_mock1.csv\")\n\nThe two datasets we will use are mockdata_cases1.csv and mosq_mock1.csv, which are mock example datasets that should be similar to malaria case surveillance and mosquito field collection data, respectively. In the following sections we will use the mockdata_cases1.csv and mosq_mock1.csv to introduce concepts of data cleaning and characterization in R."
  },
  {
    "objectID": "posts/data-wrangle/index.html#check-the-data-for-potential-errors",
    "href": "posts/data-wrangle/index.html#check-the-data-for-potential-errors",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "1. Check the data for potential errors",
    "text": "1. Check the data for potential errors\n\nPrevalence is a fraction defined in [0,1]\nNote: Prevalence of 0 or 1 while not statistically erroneous, need checking for accuracy.\nWhat observations have errors?\n\n# Erroneous values for prevalence\ndata_cases%&gt;%\n   dplyr::filter(prev &lt;= 0 | prev &gt;= 1)\n\n# A tibble: 3 × 10\n  location month  year ages  total positive xcoord ycoord    prev time_order_loc\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1 mordor       4  2018 15_a…    91       23  -20.0   30.5 25.3                 4\n2 neverwh…     2  2019 15_a…    22       -1  -20.8   29.6 -0.0455             14\n3 neverwh…     3  2018 unde…    25        0  -19.8   30.2  0                   3\n\n\nComment: We have two rows with nonsensical prev data 25.3and -0.455, and one row with zero prev at a given month.\n\n\nDefensive programming\nNote: The use of “::” enables us to call a function from a specific R package I have had instances where if “stats” base R package was called first, the filter function if not specified with the R package fails.\n\n# Erroneous values for prevalence\ndata_cases%&gt;%\n    stats::filter(prev &lt; 0 | prev &gt; 1) \n\n\n\nWe correct the two prevalence by re-calculating\nGood practice to leave the original data intact (advantage of R over Stata)\n\n# Update erroneous values for prevalence\ndata_prev &lt;- data_cases%&gt;%\n                       dplyr::mutate(prev_updated=positive/total)\n\nWe have a case erroneously reported with a negative value.\nWhat are your options?\n\nNever delete data\nQuery and have data management team make the necessary investigations and make a correction\n\n\ndata_prev%&gt;%\n    dplyr::filter(prev_updated &lt;= 0 | prev_updated &gt;= 1)\n\n# A tibble: 2 × 11\n  location month  year ages  total positive xcoord ycoord    prev time_order_loc\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1 neverwh…     2  2019 15_a…    22       -1  -20.8   29.6 -0.0455             14\n2 neverwh…     3  2018 unde…    25        0  -19.8   30.2  0                   3\n# ℹ 1 more variable: prev_updated &lt;dbl&gt;\n\n\nFor now (in order to proceed with this demo), we drop the problematic observation.\nWhy is this not working?\n\n# Filter erroneous values for prevalence, wrong way\ndata_use &lt;- data_prev%&gt;%\n              dplyr::filter (prev_updated &gt;= 0 | prev_updated &lt;= 1)\n\nWhy is this working?\n\n# Filter erroneous values for prevalence\ndata_use &lt;- data_prev%&gt;%\n             dplyr::filter (prev_updated &gt;= 0 )%&gt;%\n              dplyr::filter (prev_updated &lt;= 1)\n\ndata_use%&gt;%\n       dplyr::filter(prev_updated &lt;= 0 | prev_updated &gt;= 1)\n\n# A tibble: 1 × 11\n  location   month  year ages  total positive xcoord ycoord  prev time_order_loc\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1 neverwhere     3  2018 unde…    25        0  -19.8   30.2     0              3\n# ℹ 1 more variable: prev_updated &lt;dbl&gt;\n\n\n\n\nSchemas\nTo prevent nonsensical data appearing in your data, you should define a schema that comes along with your recorded data. A schema is a document that states rules for data types and values or ranges to be expected in a particular column of your data frame.\nE.g. for prevalence, we know that this should be a real number between zero and one.\nThe R package validate can be used to create a schema for your data frame:\n\n# Filter erroneous values for prevalence\nlibrary(validate)\nschema &lt;- validate::validator(prev &gt;= 0,\n                   prev &lt;= 1,\n                   positive &gt;= 0)\n\nout   &lt;- validate::confront(data_cases, schema)\nsummary(out)\n\n  name items passes fails nNA error warning             expression\n1   V1   514    513     1   0 FALSE   FALSE     prev - 0 &gt;= -1e-08\n2   V2   514    513     1   0 FALSE   FALSE      prev - 1 &lt;= 1e-08\n3   V3   514    513     1   0 FALSE   FALSE positive - 0 &gt;= -1e-08\n\n\nUsing the schema for the columns prev and positive, we could have readily detected the three problematic entries. For more details, you can have a look into the vignette of the validate package.\nNote: Next time when you receive data from your collaborators, you might want to ask them for the associated schema file (e.g. YAML format). Good luck!"
  },
  {
    "objectID": "posts/data-wrangle/index.html#look-at-summary-statistics",
    "href": "posts/data-wrangle/index.html#look-at-summary-statistics",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "2. Look at summary statistics",
    "text": "2. Look at summary statistics\n\nSummary stats by location (across all time points)\n\n# Summary statistics \n\ndata_use%&gt;%\n   dplyr::group_by(location)%&gt;%\n     dplyr::summarise(nobs=n(),\n                      mean_prev=mean(prev_updated),\n                      min_prev=min(prev_updated),\n                      max_prev=max(prev_updated))\n\n# A tibble: 5 × 5\n  location    nobs mean_prev min_prev max_prev\n  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mordor       105     0.314   0.158     0.488\n2 narnia       104     0.326   0.08      0.488\n3 neverwhere    95     0.301   0         0.486\n4 oz           104     0.255   0.0714    0.459\n5 wonderland   105     0.382   0.194     0.535\n\n\n\n\nSummary stats by location and year (across all time points)\nTable getting longer. Might be too cumbersome to add checks by month and age group Note: point of query - why just had 3 measurements in 2020?\n\n# Summary statistics by location\ndata_use%&gt;%\n  dplyr::group_by(location, year)%&gt;%\n  dplyr::summarise(nobs=n(),\n                   mean_prev=mean(prev_updated),\n                   min_prev=min(prev_updated),\n                   max_prev=max(prev_updated))\n\n# A tibble: 15 × 6\n# Groups:   location [5]\n   location    year  nobs mean_prev min_prev max_prev\n   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 mordor      2018    36     0.318   0.206     0.473\n 2 mordor      2019    36     0.313   0.170     0.451\n 3 mordor      2020    33     0.312   0.158     0.488\n 4 narnia      2018    36     0.340   0.138     0.449\n 5 narnia      2019    36     0.361   0.216     0.488\n 6 narnia      2020    32     0.270   0.08      0.483\n 7 neverwhere  2018    36     0.304   0         0.45 \n 8 neverwhere  2019    56     0.298   0.0370    0.486\n 9 neverwhere  2020     3     0.307   0.04      0.473\n10 oz          2018    35     0.252   0.0714    0.459\n11 oz          2019    36     0.254   0.0861    0.446\n12 oz          2020    33     0.260   0.112     0.405\n13 wonderland  2018    36     0.365   0.255     0.454\n14 wonderland  2019    36     0.388   0.194     0.535\n15 wonderland  2020    33     0.393   0.276     0.476\n\n\n\n\n\n\n\n\nChallenge 1: Explore the data_prev and data_use datasets\n\n\n\n\nCreate a table showing the number of data entries per age group and location for each of them!\nWhich age group and location have observations removed?\n\n\n\nSlightly more advanced. Use of lists (not scope of the course but there is a point here).\n\n# Summary statistics by location\ndata_use_list &lt;- data_use%&gt;%\n                  dplyr::group_split(location)\n\nOr use the purrr library:\n\n# Summary statistics by location, map summary function\nlibrary(purrr)\n\ndata_use_age_summary &lt;- purrr::map(.x=seq(length(data_use_list)),\n                                   .f=function(x){\n                                     data_use_list[[x]]%&gt;%\n                                       dplyr::group_by(location,year,ages)%&gt;%\n                                       dplyr::summarise(nobs=n(),\n                                                        mean_prev=mean(prev_updated),\n                                                        min_prev=min(prev_updated),\n                                                        max_prev=max(prev_updated)) \n                                     \n                                   })\n\n\n\nNow let’s focus on the first list object (mordor)\nWe know pregnant mothers, children &lt;5 are most vulnerable.\nOutput (ages) isn’t ordered as we would want (chronologically).\n\n# Summary statistics by location\n\ndata_mordor &lt;- data_use_age_summary[[1]]\n\ndata_mordor\n\n# A tibble: 9 × 7\n# Groups:   location, year [3]\n  location  year ages      nobs mean_prev min_prev max_prev\n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mordor    2018 15_above    12     0.270    0.206    0.369\n2 mordor    2018 5_to_14     12     0.335    0.219    0.427\n3 mordor    2018 under_5     12     0.348    0.259    0.473\n4 mordor    2019 15_above    12     0.266    0.170    0.377\n5 mordor    2019 5_to_14     12     0.278    0.176    0.390\n6 mordor    2019 under_5     12     0.394    0.315    0.451\n7 mordor    2020 15_above    11     0.255    0.158    0.333\n8 mordor    2020 5_to_14     11     0.352    0.258    0.488\n9 mordor    2020 under_5     11     0.330    0.190    0.422\n\n\n\n\nHow to proceed?\n\n# Summary statistics with age groups\nage_order &lt;- c(\"under_5\",\"5_to_14\",\"15_above\")\n\ndata_use_ordered &lt;- data_use\n\ndata_use_ordered$age_group &lt;- factor(data_use$ages, levels =age_order)\n\ndata_mordor_reordered &lt;- data_use_ordered%&gt;%\n                           dplyr::group_by(location, year,age_group)%&gt;%\n                            dplyr::summarise(nobs=n(),\n                                             mean_prev=mean(prev_updated),\n                                             min_prev=min(prev_updated),\n                                             max_prev=max(prev_updated))%&gt;%\n                                 dplyr::filter(location==\"mordor\")\n\nLet’s compare the two\n\n# Compare for Mordor\n\ndata_mordor\n\n# A tibble: 9 × 7\n# Groups:   location, year [3]\n  location  year ages      nobs mean_prev min_prev max_prev\n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mordor    2018 15_above    12     0.270    0.206    0.369\n2 mordor    2018 5_to_14     12     0.335    0.219    0.427\n3 mordor    2018 under_5     12     0.348    0.259    0.473\n4 mordor    2019 15_above    12     0.266    0.170    0.377\n5 mordor    2019 5_to_14     12     0.278    0.176    0.390\n6 mordor    2019 under_5     12     0.394    0.315    0.451\n7 mordor    2020 15_above    11     0.255    0.158    0.333\n8 mordor    2020 5_to_14     11     0.352    0.258    0.488\n9 mordor    2020 under_5     11     0.330    0.190    0.422\n\ndata_mordor_reordered\n\n# A tibble: 9 × 7\n# Groups:   location, year [3]\n  location  year age_group  nobs mean_prev min_prev max_prev\n  &lt;chr&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mordor    2018 under_5      12     0.348    0.259    0.473\n2 mordor    2018 5_to_14      12     0.335    0.219    0.427\n3 mordor    2018 15_above     12     0.270    0.206    0.369\n4 mordor    2019 under_5      12     0.394    0.315    0.451\n5 mordor    2019 5_to_14      12     0.278    0.176    0.390\n6 mordor    2019 15_above     12     0.266    0.170    0.377\n7 mordor    2020 under_5      11     0.330    0.190    0.422\n8 mordor    2020 5_to_14      11     0.352    0.258    0.488\n9 mordor    2020 15_above     11     0.255    0.158    0.333"
  },
  {
    "objectID": "posts/data-wrangle/index.html#use-of-graphs",
    "href": "posts/data-wrangle/index.html#use-of-graphs",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "3. Use of graphs",
    "text": "3. Use of graphs\n\nWe need to assess the evolution of prevalence for all regions by month\n\n#Plotting evolution over time\nevolution_plot &lt;- ggplot2::ggplot(data=data_use_ordered,\n                                  mapping=aes(x=month,\n                                              y=prev_updated,\n                                              group=location,\n                                              colour=location))+\n                        ggplot2::geom_line(lwd=1.1)+\n                           ggplot2::facet_wrap(~year)+ \n                            ggplot2::theme_bw()+\n                             ggplot2::xlab(\"Month of the Year\")+\n                               ggplot2::ylab(\"Prevalence\")+\n                                ggplot2::scale_x_discrete(limits=factor(1:12),\n                                                          labels=c(\"J\",\"F\",\"M\",\n                                                                   \"A\",\"M\",\"J\",\n                                                                   \"J\",\"A\",\"S\",\n                                                                   \"O\",\"N\",\"D\"))+\n                                   ggplot2::scale_y_continuous(breaks=seq(from=0,\n                                                                          to=0.7,\n                                                                          by=0.1))\n\nevolution_plot\n\n\n\n\n\n\n\n\nObservation: Prevalence graph with vertical lines per month and year, means we have several subgroups for prevalence data, we plot facets for levels of age_group\n\n#Plotting evolution over time, fix 1\nevolution_plot_ages &lt;- ggplot2::ggplot(data=data_use_ordered,\n                                  mapping=aes(x=month,\n                                              y=prev_updated,\n                                              group=location,\n                                              colour=location))+\n  ggplot2::geom_line(lwd=1.1)+\n  ggplot2::facet_wrap(age_group~year)+ \n  ggplot2::theme_bw()+\n  ggplot2::xlab(\"Month of the Year\")+\n  ggplot2::ylab(\"Prevalence\")+\n  ggplot2::scale_x_discrete(limits=factor(1:12),\n                            labels=c(\"J\",\"F\",\"M\",\n                                     \"A\",\"M\",\"J\",\n                                     \"J\",\"A\",\"S\",\n                                     \"O\",\"N\",\"D\"))+\n  ggplot2::scale_y_continuous(breaks=seq(from=0,\n                                         to=0.7,\n                                         by=0.1))\n\nevolution_plot_ages\n\n\n\n\n\n\n\n\nObservation: Some improvements, but we still have vertical lines, maybe we have other group variables. Let’s only look at those rows that have more than one entry per location, month, year, age_group\n\n#Plotting evolution over time, fix 2\n\ndata_use_ordered%&gt;%\n  group_by(location,month,year,age_group)%&gt;%\n  tally()%&gt;%\n  filter(n&gt;1)%&gt;%\n  left_join(data_use_ordered)\n\n# A tibble: 48 × 13\n# Groups:   location, month, year [8]\n   location   month  year age_group     n ages     total positive xcoord ycoord\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 neverwhere     6  2019 under_5       2 under_5     24        4  -20.6   30.7\n 2 neverwhere     6  2019 under_5       2 under_5     26        1  -20.5   30.7\n 3 neverwhere     6  2019 5_to_14       2 5_to_14     27        5  -19.7   30.0\n 4 neverwhere     6  2019 5_to_14       2 5_to_14     27        8  -19.3   30.2\n 5 neverwhere     6  2019 15_above      2 15_above    70       31  -19.4   29.4\n 6 neverwhere     6  2019 15_above      2 15_above    74       27  -19.2   29.2\n 7 neverwhere     7  2019 under_5       2 under_5     25        5  -20.0   29.1\n 8 neverwhere     7  2019 under_5       2 under_5     26        4  -20.7   28.6\n 9 neverwhere     7  2019 5_to_14       2 5_to_14     27        7  -18.8   29.3\n10 neverwhere     7  2019 5_to_14       2 5_to_14     23        6  -20.4   29.8\n# ℹ 38 more rows\n# ℹ 3 more variables: prev &lt;dbl&gt;, time_order_loc &lt;dbl&gt;, prev_updated &lt;dbl&gt;\n\n\nObservation: OK, we see that within one location there are several prevalence data points, they differ by the xcoord and ycoord. In order to plot by location, we could average across xcoord and ycoord witin each location; maybe those are duplicated recordings, since xcoord and ycoord are very close?\n\n#Plotting evolution over time, fix 3\n\ndata_use_ordered%&gt;%\n  group_by(location,month,year,age_group)%&gt;%\n  summarize(prev_updated_mean=mean(prev_updated),\n            prev_updated_min=min(prev_updated),\n            prev_updated_max=max(prev_updated))%&gt;%\n  ggplot2::ggplot(mapping=aes(x=month,\n                              y=prev_updated_mean,\n                              file=location,\n                              group=location,\n                              colour=location))+\n  ggplot2::geom_line(lwd=1.1)+\n  ggplot2::facet_wrap(age_group~year)+ \n  ggplot2::theme_bw()+\n  ggplot2::xlab(\"Month of the Year\")+\n  ggplot2::ylab(\"Prevalence\")+\n  ggplot2::scale_x_discrete(limits=factor(1:12),\n                            labels=c(\"J\",\"F\",\"M\",\n                                     \"A\",\"M\",\"J\",\n                                     \"J\",\"A\",\"S\",\n                                     \"O\",\"N\",\"D\"))+\n  ggplot2::scale_y_continuous(breaks=seq(from=0,\n                                         to=0.7,\n                                         by=0.1))\n\n\n\n\n\n\n\n\nObservation: Prevalence widely variable throughout they year across the locations on average, wonderland affected by high prevalence while oz has the lowest prevalence"
  },
  {
    "objectID": "posts/data-wrangle/index.html#the-mosquito-data-set",
    "href": "posts/data-wrangle/index.html#the-mosquito-data-set",
    "title": "Live Session 2: Introduction to Data Wrangling in R",
    "section": "The mosquito data set",
    "text": "The mosquito data set\nLet’s take a look at the mosq_datadataset.\nWe check the sanity of this data set by displaying a table of recorded values per column:\n\nmosq_data %&gt;%\n  map( function(x) table(x) )\n\n$session\nx\n 1  2 \n52 52 \n\n$Village\nx\nnaernia  narnia \n      2     102 \n\n$Compound.ID\nx\n 1  2  3  4 \n26 26 26 26 \n\n$Method\nx\nALC HLC \n  1 103 \n\n$Location\nx\n Indoor Outdoor \n     52      52 \n\n$hour\nx\n01h-02h 02h-03h 03h-04h 04h-05h 05h-06h 06h-07h 07h-08h 19h-20h 20h-21h 21h-22h \n      8       8       8       8       8       8       8       8       8       8 \n22h-23h 23h-24h 24h-01h \n      8       8       8 \n\n$ag.Male\nx\n 0  3  4  5  6  7 14 16 20 22 27 35 \n93  1  1  1  1  1  1  1  1  1  1  1 \n\n$Ag.unfed\nx\n 0  1  2  3  4  5  6  7  8 10 20 \n57 13  7  8  4  4  2  4  2  1  2 \n\n$Ag.halffed\nx\n 0  3  4  5  8  9 \n92  3  3  3  1  2 \n\n$Ag.fed\nx\n 0  1  3  5 \n88  7  3  6 \n\n$Ag.grsgr\nx\n 0  1  2  3  4  6  8 12 17 20 23 27 35 37 \n70 13  6  1  2  1  3  2  1  1  1  1  1  1 \n\n$tot.gamb\nx\n 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 23 \n40 10 10  9  8  6  3  2  1  1  5  2  1  2  2  2 \n\n$Culex.male\nx\n  0 \n104 \n\n$Culex.female\nx\n 0  1  2 \n94  9  1 \n\n$Mansonia.male\nx\n  0   1 \n103   1 \n\n$Mansonia.female\nx\n 0  1  2 \n90 11  3 \n\n$Aedes.male\nx\n  0 \n104 \n\n$Aedes.female\nx\n 0  1  2 \n98  5  1 \n\n\nLooks like we have some typos in the names for Method and Village.\n\n\n\n\n\n\nChallenge 2: Using schemas for the mosquito data set\n\n\n\n\nCreate a schema that provides rules for the strings (i.e. words) to be expected in the columns Method and Village.\nUse the syntax from here\n\n\n\n\nschema &lt;- validate::validator(Method%in%c(\"HLC\"),\n                              Village%in%c(\"narnia\"))\n\nout   &lt;- validate::confront(mosq_data, schema)\nsummary(out)\n\n  name items passes fails nNA error warning                expression\n1   V1   104    103     1   0 FALSE   FALSE     Method %vin% c(\"HLC\")\n2   V2   104    102     2   0 FALSE   FALSE Village %vin% c(\"narnia\")\n\n\nThe columns Village and Method seem to have some data entry errors. We need to correct for that.\n\nmosq_data&lt;-mosq_data%&gt;%\n  mutate(Method=ifelse(Method==\"ALC\",\"HLC\",Method),\n         Village=ifelse(Village==\"naernia\",\"narnia\",Village))\n\nIt looks like the several columns concern Anopheles Gambiae population sizes. Let’s change the column names using rename from the tidyverse package.\n\nmosq_data%&gt;%\n  rename(\"AnophelesGambiae.male\"=\"ag.Male\",\n         \"AnophelesGambiae.unfed\"=\"Ag.unfed\",\n         \"AnophelesGambiae.halffed\"=\"Ag.halffed\",\n         \"AnophelesGambiae.fed\"=\"Ag.fed\",\n         \"AnophelesGambiae.gravid\"=\"Ag.grsgr\")-&gt;mosq_data\n\nSeems like the tot.gamb should count the the total number of Anopheles Gambiae populations. Let’s check:\n\nmosq_data%&gt;%\n  mutate(AnophelesGambiae_total=AnophelesGambiae.male+AnophelesGambiae.unfed+AnophelesGambiae.halffed+AnophelesGambiae.fed+AnophelesGambiae.gravid)-&gt;mosq_data\n\nmosq_data%&gt;%\n  filter(AnophelesGambiae_total!=tot.gamb)%&gt;%select(AnophelesGambiae_total,tot.gamb)\n\n# A tibble: 11 × 2\n   AnophelesGambiae_total tot.gamb\n                    &lt;dbl&gt;    &lt;dbl&gt;\n 1                     12        0\n 2                     16        2\n 3                      0        6\n 4                     24        8\n 5                     24        1\n 6                     74       12\n 7                     54        3\n 8                     70        1\n 9                     34        2\n10                     40        2\n11                     46        0\n\n\nOK, so 11 out of 104 rows have this discrepancy. Let’s keep rather Anopheles.total, since it was calculated from the data.\nSince the status of the Anopheles is mutually exclusive in the HLC data, we can draw a stacked bar chart, with the bar color defined by the status. To produce such a graph efficiently in ggplot2, we need to pivot the table.\nHere in particular we want to switch from a wide format to a long format table in order to obtain a column describing the status of the Anopheles mosquitoes. We will use in particular the names_separgument of the pivot_longer function to separate e.g. the column name AnophelesGambiae.male and use maleas level in a new column called status. The same goes for other column names.\nSetting the grouping variable to session, Village, Compound.ID, Method, Location, hour, AnophelesGambiae_total will help to keep those variables in the long format table.\n\nmosq_data%&gt;%\n  group_by(session,Village,Compound.ID,Method,Location,hour,AnophelesGambiae_total)%&gt;%\n  select(contains(\"AnophelesGambiae.\"))%&gt;%\n  pivot_longer(cols=contains(\"AnophelesGambiae.\"),names_sep=\"AnophelesGambiae.\",names_to=c(NA,\"status\"),values_to = \"AnophelesGambiae\")-&gt;mosq_data_gamb_wide\n\nmosq_data_gamb_wide%&gt;%\n  ggplot()+\n  geom_bar(aes(x=hour,y=AnophelesGambiae,fill=status),position=\"stack\",stat=\"identity\")+\n  scale_x_discrete(guide = guide_axis(angle = 60))\n\n\n\n\n\n\n\n\nObservation: We had several values for Compound.ID. The geom_bar geometry is automatically adding them up in the graph. We can use facet_wrapto see those strata:\n\nmosq_data_gamb_wide%&gt;%\n  ggplot()+\n  geom_bar(aes(x=hour,y=AnophelesGambiae,fill=status),position=\"stack\",stat=\"identity\")+\n  scale_x_discrete(guide = guide_axis(angle = 60))+\n  facet_wrap(~Compound.ID)\n\n\n\n\n\n\n\n\nOn we can also use our variable Anopheles_total and plot is as a line graph on top of the bar graph:\n\nmosq_data_gamb_wide%&gt;%\n  mutate(grouping=paste0(Compound.ID,Location,session))%&gt;%\n  ggplot()+\n  geom_bar(aes(x=hour,y=AnophelesGambiae,fill=status),position=\"stack\",stat=\"identity\")+\n  geom_line(aes(x=hour,y=AnophelesGambiae_total,group=grouping))+\n  scale_x_discrete(guide = guide_axis(angle = 60))+\n  facet_wrap(~Compound.ID+session+Location)"
  }
]